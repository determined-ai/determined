# End-to-end Object Detection with Transformers (Carion et al. 2020)
This example implements the Detection Transformers (DETR) model introduced by Carion et al. (2020) in their [recent paper](https://arxiv.org/abs/2005.12872).  
DETR drastically simplifies the object detection pipeline by doing away with complex postprocessing steps required by most other architectures to extract bounding boxes.  
In place of proposals, anchors, and window centers, DETR directly optimizes a set prediction loss to learn where to focus.  

This implementation largely uses the original implementation provided in [this repo](https://github.com/facebookresearch/detr).  With this Determined implementation, you can easily run DETR with distributed training and hyperparameter search by modifying a few flags in the [experiment config](distributed_gcs.yaml).  Note that the panoptic segmentation task is not supported in this example.

## Files
This example is structure as follows:
* [model_def.py](model_def.py): initializes the data, optimizer, and model and specifies the training and evaluation steps.
* [model.py](model.py): modifies the original [SetCriterion function](https://github.com/facebookresearch/detr/blob/master/models/detr.py#L83) to use horovod allreduce to sync number of bounding boxes.
* [data.py](data.py): creates a CocoDetection dataset to work with Google Cloud Storage. 

### Configuration Files
* [distributed_gcs.yaml](distributed_gcs.yaml): distributed training experiment to run with COCO dataset on Google Cloud Storage.
* [distributed_hdd.yaml](distributed_hdd.yaml): distributed training experiment to run with COCO dataset by downloading the data to disk.
* [const_fake.yaml](const_fake.yaml): single-GPU experiment to run with fake data.

## Data
This example requires the COCO 2017 Object Detection dataset.  There are three backends that you can select for accessing the data (see [data.py](data.py)): gcs, local, and fake.  The gcs backend will download images from a Google Cloud Storage bucket, local will download the COCO dataset directly from source to the harddrive and read images from disk, and fake will use a fake image without requiring COCO to be available anywhere.  Experiment config files are provided for each of the access patterns. 

## To Run
If you have the COCO 2017 dataset available in a Google Cloud Storage bucket, you can use [distributed_gcs.yaml](distributed_gcs.yaml) and change the `data_dir` field to your bucket name.  If you want to download the COCO dataset to disk, you can use [distributed_hdd.yaml](distributed_hdd.yaml) and change the `data_dir` field to the data directory.  Note that using a path on the bind mounted path will allow data to persist on the agent even after the container terminates.  Finally, you also have the option of running this example with fake data using the [const_fake.yaml](const_fake.yaml) config.  Submit the experiment to your Determined cluster by running
```
det experiment create <distributed_gcs.yaml> .
```
from the command line.

## Results
The training and validation curves generated by this experiment are shown below.  They closely match the [results for the 150 epoch learning rate schedule provided in the original repo](https://gist.github.com/szagoruyko/b4c3b2c3627294fc369b899987385a3f).

![train_curves](imgs/train_curves.png)
![val_curves](imgs/val_curves.png)
