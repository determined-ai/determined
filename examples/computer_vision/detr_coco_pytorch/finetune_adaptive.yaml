description: detr_coco_finetune_adaptive
hyperparameters:
    # We will tune learning rate and gradient clipping.
    lr: 
        type: double
        minval: 1e-5
        maxval: 1e-4
    lr_backbone: 0
    global_batch_size: 4
    weight_decay: 1e-4
    lr_drop: 4
    clip_max_norm: 
        type: double
        minval: 0.1
        maxval: 0.5

    # Set to true if you want to warmstart using pretrained weights.
    warmstart: true

    # Backbone
    backbone: resnet50
    dilation: false
    position_embedding: sine

    # Transformer
    enc_layers: 6
    dec_layers: 6
    dim_feedforward: 2048
    hidden_dim: 256
    dropout: 0.1
    nheads: 8
    num_queries: 100
    pre_norm: false

    # Loss
    aux_loss: true

    # Matcher
    set_cost_class: 1
    set_cost_bbox: 5
    set_cost_giou: 2

    # Loss Coefficients
    mask_loss_coef: 1
    dice_loss_coef: 1
    bbox_loss_coef: 5
    giou_loss_coef: 2
    eos_coef: 0.1

    # Dataset
    dataset_file: coco
    backend: gcs # specifiy the backend you want to use.  one of: gcs, aws, fake, local
    data_dir: determined-ai-coco-dataset # bucket name if using gcs or aws, otherwise directory to dataset
    cat_ids: # COCO category ids to finetune on.
        - 21 # Category ID for cows.
    num_classes: 1
    masks: false
    num_workers: 4

    device: cuda

bind_mounts:
    - host_path: /tmp
      container_path: /data
      read_only: false

min_validation_period:
    batches: 500
records_per_epoch: 1968
searcher:
  name: adaptive_asha
  metric: mAP
  smaller_is_better: false
  max_length:
      batches: 3200
  max_trials: 100
  mode: aggressive
  max_rungs: 4
  max_concurrent_trials: 36

resources:
    slots_per_trial: 2
    shm_size: 2000000000

entrypoint: model_def:DETRTrial
