name: language modeling adaptive
debug: false
environment:
  environment_variables:
    - NCCL_DEBUG=INFO
    # You may need to modify this to match your network configuration.
    - NCCL_SOCKET_IFNAME=ens,eth,ib
resources:
  slots_per_trial: 2
searcher:
  name: adaptive_asha
  max_length:
    batches: 100
  max_trials: 64
  max_rungs: 4
  divisor: 4
  metric: eval_loss
hyperparameters:
  training_arguments:
    learning_rate:
      type: log
      base: 10
      minval: -5
      maxval: -2
    adam_epsilon:
      type: log
      base: 10
      minval: -10
      maxval: -7
entrypoint: >-
  python -m determined.launch.torch_distributed
  python run_clm.py
  --model_name_or_path gpt2
  --dataset_name wikitext
  --dataset_config_name wikitext-2-raw-v1
  --do_train
  --do_eval
  --max_steps 100
  --logging_strategy steps
  --logging_steps 10
  --output_dir /tmp/test-clm
  --eval_steps 10
  --evaluation_strategy steps
  --save_total_limit 3
  --seed 1337
  --save_strategy steps
  --save_steps 20
  --per_device_train_batch_size 8
  --per_device_eval_batch_size 8
  --trust_remote_code false
max_restarts: 0
