name: torchvision_models
workspace: DS_AT_PROTOTYPING
project: native_dsat_tests
description: resnet152 baseline
max_restarts: 0
environment:
  image:
    gpu: determinedai/environments:cuda-11.3-pytorch-1.10-deepspeed-0.8.3-gpu-0.21.2
resources:
  slots_per_trial: 2
  # max_slots: 8
  shm_size: 17179869184 # 16 GiB. Was hitting "Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)." on Grenoble.
  # resource_pool: young # Grenoble specific; remove later.
searcher:
  name: single
  metric: throughput
  max_length: 10 # Just setting this to something larger than the default exit step.
  smaller_is_better: False
hyperparameters:
  model_name: resnet152
  # NOTE: dsat code expects usual DS config dict to appear as in the below.
  ds_config:
    train_micro_batch_size_per_gpu: 128
    gradient_accumulation_steps: 1
    optimizer:
      TYPE: Adam
      params:
        lr: 1e-3
    fp16:
      enabled: True
    autotuning:
      enabled: True
      fast: False
      tuner_type: random
      tuner_num_trials: 50
      num_tuning_micro_batch_sizes: 3
      tuner_early_stopping: 5
      metric: throughput
entrypoint: python3 deepspeed_single_node_launcher.py --autotuning run -- main.py --deepspeed_config ds_config.json
# entrypoint: python3 deepspeed_single_node_launcher.py -- main.py --deepspeed_config ds_config.json
# Using --autotuning run is a terrible hack: it sets off a short training run after autotuning so that
# the report_and_save_native_autotuning_results function in main.py can be reached. In the autotuning runs which
# proceed the final training run, exit() is called before reaching this function.
