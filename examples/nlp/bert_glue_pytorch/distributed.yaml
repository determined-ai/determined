name: bert_glue_pytorch_distributed 
hyperparameters:
  global_batch_size: 192 # per gpu batch size of 24
  learning_rate: 2.0e-5
  lr_scheduler_epoch_freq: 1
  model_type: 'bert'
  adam_epsilon: 1.0e-8
  weight_decay: 0
  num_warmup_steps: 0
  num_training_steps: 459
  max_seq_length: 128
searcher:
  name: single
  metric: acc
  max_length:
    batches: 50
  smaller_is_better: false
resources:
    slots_per_trial: 8
data:
  task: 'MRPC'
  model_name_or_path: "bert-base-uncased"
  output_mode: "classification"
  path_to_mrpc: ''
  download_data: True
entrypoint: model_def:BertPyTorch
