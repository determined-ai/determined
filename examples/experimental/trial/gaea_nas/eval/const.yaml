# NOTE: this config uses randomly generated data on single GPU.
# Please use distributed.yaml if you want to run on the
# the actual imagenet data.
description: gaea_eval 

data:
  # Change bucket_name to GCP bucket with imagenet dataset
  # Data folder structure assumed to be imagenet/train and 
  # imagenet/validation for the two data splits.
  #
  # If bucket_name is null, we will run with randomly 
  # generated data.
  bucket_name: null
  # We recommend num_workers_train to be set to 16
  # when running with slots_per_trial=8
  num_workers_train: 2
  num_workers_val: 2
  # If streaming is true, we will send request to bucket
  # every time an image is requested.  If false, we will
  # save data to disk and load that the next time the
  # image is requested.  We recommend streaming=true
  # to avoid having to mount directories to docker container 
  # and guarantee good performance regardless of disk speed.
  streaming: true
  # This folder is only used if streaming is false.
  # This should probably match the container_path
  # in a provided bind mount.
  data_download_dir: null

# Uncomment this if you want to mount a host directory to the 
# docker container.
#bind_mounts:
#  - host_path: /tmp
#    container_path: /mnt/data
#    read_only: false

min_validation_period:
  batches: 200

hyperparameters:
  num_classes: 1000
  drop_path_prob: 0.2
  drop_prob: 0.2
  label_smoothing_rate: 0.1
  ema_decay: 0.999
  clip_gradients_l2_norm: 5
  learning_rate: 0.5
  momentum: 0.9
  weight_decay: 3e-5
  # Choices include linear, efficientnet, and cosine
  lr_scheduler: linear
  lr_epochs: 300
  warmup_epochs: 5
  # These HPs only used for efficientnet scheduler
  lr_gamma: 0.97
  lr_decay_every: 2

  init_channels: 48
  layers: 14
  # Set global_batch_size to 1024
  # if not using randomly generated data.
  global_batch_size: 64
  auxiliary: false
  auxiliary_weight: 0.4
  randaugment: true
  cutout: true
  cutout_length: 16
  do_SE: true
  activation: swish 

resources:
  # We recommend 8 slots with V100
  # and at least 16 slots with K80 GPUs.
  slots_per_trial: 2
  shm_size: 30000000000

searcher:
  name: single
  metric: top1_accuracy 
  max_length:
    batches: 30000 # 1.2 million images in training set of imagenet
  smaller_is_better: false 

optimizations:
  aggregation_frequency:  1

entrypoint: model_def:GAEAEvalTrial
