description: gaea_eval 

# Recommended settings:
#   - Nvidia Tesla V100: set batch size to 1024 and slots_per_trial to 8
#   - Nvidia Tesla K80: 
#       - set batch size to 2048 and slots_per_trial to 24 and increase learning_rate to 1.

data:
  # Change bucket_name to GCP bucket with imagenet dataset
  # Data folder structure assumed to be imagenet/train and 
  # imagenet/validation for the two data splits.
  #
  # If bucket_name is null, we will run with randomly 
  # generated data.
  bucket_name: determined-ai-datasets 
  # We recommend num_workers_train to be set to 16
  # when running with slots_per_trial=8
  num_workers_train: 16
  num_workers_val: 2
  # If streaming is true, we will send request to bucket
  # every time an image is requested.  If false, we will
  # save data to disk and load that the next time the
  # image is requested.  We recommend streaming=true
  # to avoid having to mount directories to docker container 
  # and guarantee good performance regardless of disk speed.
  streaming: true
  # This folder is only used if streaming is false.
  # This should probably match the container_path
  # in a provided bind mount.
  data_download_dir: null

# Uncomment this if you want to mount a host directory to the 
# docker container.
#bind_mounts:
#  - host_path: /tmp
#    container_path: /mnt/data
#    read_only: false

min_validation_period:
  epochs: 1 

hyperparameters:
  num_classes: 1000
  learning_rate: 0.5
  momentum: 0.9
  weight_decay: 3e-5
  drop_path_prob: 0.0
  drop_prob: 0.0
  label_smoothing_rate: 0.1
  ema_decay: 0.999
  clip_gradients_l2_norm: 5
  # Choices include linear, efficientnet, and cosine
  lr_scheduler: linear
  lr_epochs: 250 
  warmup_epochs: 5

  # These HPs only used for efficientnet scheduler
  lr_gamma: 0.96
  lr_decay_every: 3

  # Batch size may be too big for GPU memory depending
  # on your GPU and number of slots_per_trial.
  global_batch_size: 1024 
  init_channels: 48
  layers: 14
  auxiliary: true 
  auxiliary_weight: 0.4
  randaugment: false 
  cutout: true
  cutout_length: 16
  do_SE: false 
  activation: relu 

resources:
  slots_per_trial: 8
  shm_size: 30000000000

checkpoint_storage:
  save_experiment_best: 5
  save_trial_best: 2
  save_trial_latest: 1

records_per_epoch: 1281167

searcher:
  name: single
  metric: top1_accuracy 
  max_length:
    epochs: 250
  smaller_is_better: false 

optimizations:
  aggregation_frequency:  1

entrypoint: model_def:GAEAEvalTrial
