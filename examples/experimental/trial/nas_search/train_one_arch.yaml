description: NAS_ASHA
hyperparameters:
  clip_gradients_l2_norm: .25
  global_batch_size: 64
  bptt: 35
  learning_rate: 20
  dropout: 0.75
  dropouth: 0.25
  dropoutx: 0.75
  dropouti: 0.2
  nonmono: 5
  log_interval: 50
  alpha: 0
  beta: .001
  max_seq_length_delta: 20
  unrolled: True
  emsize: 850
  nhid: 850
  nhidlast: 850
  dropoute: 0
  wdecay: 8e-07
  seq_len: 35
  init_op: 'tanh'
  eval_batch_size: 10
  depth: 1
  step_every_epoch: False # set to True if you would like Determined to handle the LR scheduler
  step_every_batch: False # If step_every_batch and step_every_epoch are set to false, the scheduler will mimic the original repo
  arch_to_use: 'ASHA'
  eval_same_arch: True
  num_archs_to_eval: 0 # only used when eval_same_arch is False
min_validation_period: 4
entrypoint: model_def:NASModel
reproducibility: # used to reproduce the same results of the arch
  experiment_seed: 300
max_restarts: 0
searcher:
  name: single
  metric: loss
  max_steps: 434 # is about 100 epochs = ~ 400 batches is 1 epoch for batch size 64
  smaller_is_better: true
data:
  out_file: 'data/architecture_'
# Uncomment if using predownloaded data
# bind_mounts:
#   - host_path: "/path/to/ptb/data"
#     container_path: 'data' # Mounts the host_path to data/
