# After fine-tuning for 150 steps, model should achieve F1 = 88.52 per https://github.com/huggingface/transformers/tree/master/examples/question-answering
description: Bert_SQuAD_PyTorch_distributed
hyperparameters:
    global_batch_size: 96  # per slot batch size = 12
    learning_rate: 3e-5
    lr_scheduler_epoch_freq: 1
    model_type: 'bert'
    adam_epsilon: 1e-8
    weight_decay: 0
    num_warmup_steps: 0
    max_seq_length: 384
    doc_stride: 128
    max_query_length: 64
    n_best_size: 20
    max_answer_length: 30
    null_score_diff_threshold: 0.0
    max_grad_norm: 1.0
    num_training_steps: 15000 # This is the number of optimizer steps. Set it
                              # to max_steps * batches_per_step (assuming we
                              # step every batch in the LR scheduler definition)
resources:
    slots_per_trial: 8
searcher:
    name: single
    metric: f1
    max_steps: 150 # There are ~90k examples in the training set, and we run for
                   # 2 epochs as in the huggingface repo. We get 150 by:
                   # 90k / global_batch_size / (100 batches per step) * 2 epochs
    smaller_is_better: false
min_validation_period: 10 # Validation is fairly costly (~10 minutes per), but
                          # relative to overall runtime it's not that high, so
                          # validate often to give more frequent feedback.
data:
    pretrained_model_name: "bert-base-uncased"
    download_data: False
    task: "SQuAD1.1"
entrypoint: model_def:BertSQuADPyTorch
