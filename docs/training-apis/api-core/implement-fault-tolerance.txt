.. _core-implement-fault-tolerance:

###########################
 Implement Fault Tolerance
###########################

Do you have training that take hours, days, or even weeks to complete?

Don't let power outages, network outages, or even slow memory leaks cause you to have to restart.

When you use the Core API, can checkpoint your progress as often as you like. Determined will
restart your job with the last checkpoint you reported, and all you have to do for fault tolerance
is load that checkpoint and go!

.. code::

       model, loss_fn, opt = build_model(hparams["learning_rate"])

       batches_trained = 0
       epochs_complete = 0

   +   # read state from the last checkpoint
   +   if context.latest_checkpoint is not None:
   +       with context.latest_checkpoint as path:
   +           state = torch.load(f"{path]/my_checkpoint")
   +       model.load_state(state["model"])
   +       batches_trained = state["batches_trained"]
   +       epochs_trained = state["epochs_trained"]

       for op in context.searcher.ops():
           while epochs_trained < op.length:
               # train model
               ...

               # report training metrics to the master
               ...

               # track how much we have trained
               epochs_complete += 1

               # update progress in the webui
               ...

   +           # checkpoint every epoch for high fault-tolerance
   +           # also track how far we have trained
   +           state = {
   +               "model": model.state_dict(),
   +               "epoch": epoch,
   +               "batches_trained": batches_trained,
   +               "epochs_complete": epochs_complete,
   +           }
   +           with context.checkpoint.save_path() as path:
   +               torch.save(state, f"{path]/my_checkpoint")

           # evaluate model
           ...

           # report validation metrics to the master
           ...

           # report evaluation of the searcher_metric for this op
           ...

   -   with context.checkpoint.save_path() as path:
   -       torch.save(model.state_dict(), f"{path]/my_checkpoint")
