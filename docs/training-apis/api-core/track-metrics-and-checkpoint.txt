.. _core-track-metrics-and-checkpoint:

##############################
 Track Metrics and Checkpoint
##############################

Because the WebUI does not provide useful information about training metrics or checkpoints and the
``bind_mounts`` configuration setting can be problematic, you might want to use the following Core
API methods:

+--------------------------------------------------+-------------------------------------------+
| Method                                           | Function                                  |
+==================================================+===========================================+
| ``det.core.init()``                              | Initializes a Core API context            |
+--------------------------------------------------+-------------------------------------------+
| ``context.training.report_training_metrics()``   | Reports training metrics                  |
+--------------------------------------------------+-------------------------------------------+
| ``context.training.report_validation_metrics()`` | Reports validation metrics                |
+--------------------------------------------------+-------------------------------------------+
| ``context.checkpoint.save_path()``               | Reports checkpoints to the Checkpoint API |
+--------------------------------------------------+-------------------------------------------+

Using these methods, you can:

-  track and visualize metrics in the WebUI
-  track checkpoints

***************************
 ``train.py`` File Updates
***************************

This example training file demonstrates how to report all metrics and checkpoints.

.. code::

       import torch
       from model import build_model, train_model, eval_model
       from data import build_data

       BATCH_SIZE=32
       LR=0.00001
       EPOCHS=50

   -   def main():
   +   def main(context):
           model, loss_fn, opt = build_model(LR)
           train_data, eval_data = build_dataset(BATCH_SIZE)

   +       batches_trained = 0

           for epoch in range(config.EPOCHS):
               train_loss = model.train_model(model, loss_fn, opt, train_data)
               print(f"after epoch {epoch}, train_loss={train_loss}")

   +           batches_trained += len(train_data)
   +           context.training.report_training_metrics(
   +               batches_trained=batches_trained,
   +               metrics={"loss": train_loss},
   +           )

               eval_loss = model.eval_model(model, loss_fn, train_data)
               print(f"after epoch {epoch}, eval_loss={eval_loss}")

   +           context.training.report_validation_metrics(
   +               batches_trained=batches_trained,
   +               metrics={"loss": eval_loss},
   +           )

   -       path="checkpoint_dir/my_checkpoint"
   -       torch.save(model.state_dict(), path)
   +       with context.checkpoint.save_path() as path:
   +           torch.save(model.state_dict(), f"{path]/my_checkpoint")

       if __name__ == "__main__":
   -       main()
   +       import determined as det
   +       with det.core.init() as context:
   +           main(context)

Some configurations.

Initialize a context object for accessing the Core API.

Keep track of how many batches were trained.

Train model.

Report training metrics to the master.

Evaluate model.

Report validation metrics to the master.

Checkpoint model.

*********************************
 ``my_config.yaml`` File Updates
*********************************

.. code::

       entrypoint_script: python3 train.py

       searcher:
           # we're not doing hp search yet so just use the 'single' searcher
           name: "single"
           # we have to configure the searcher for the master even though
           # our training script is going to totally ignore it.
           max_length: 1
   -
   -   bind_mounts:
   -     - host_path: /my/path/to/checkpoints
   -       container_path: ./checkpoint_dir

By using the ``context.checkpoint.save_path()`` method, you can now rely on the cluster checkpoint
configuration instead of bind mounts.
