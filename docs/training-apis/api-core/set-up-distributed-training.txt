.. _core-set-up-distributed-training:

#############################
 Set up Distributed Training
#############################

Determined exposes a launch layer to reduce the difficulty of setting up all of your machines and
getting them to communicate. By letting Determined coordinate all of the compute nodes in a
multi-node distributed training job, you need only to start training after everything is ready.

****************
 Setup Sequence
****************

#. Schedule a multi-GPU training job with a custom ``launch_layer`` setting in the experiment
   configuration. The ``launch_layer`` setting value should be an executable script with arguments.
#. Determined starts all containers on their assigned GPUs.
#. Determined gathers all of the IP addresses and assigned GPUs for every compute node in the
   training job. This is the "rendezvous info" for the job.
#. Determined calls your ``launch_layer`` on every compute node, with the "rendezvous info" exposed
   as both a python API and as environment variables.
#. Your launch layer starts the distributed training job on the assigned compute nodes, as
   applicable.

For common distributed training solutions, such as Horovod, ``torch.distributed``, and PyTorch
Lightning, you can use a pre-made launch layer.

***********
 Example 1
***********

Launch a Detectron2 model based on ``torch.distributed`` using Determined built-in
``torch.distributed`` support:

.. code:: yaml

   launch_layer: python3 -m determined.launch.torch_distributed
   entrypoint_script: python3 train_detectron.py

***********
 Example 2
***********

Train a model that directly uses Horovod:

.. code:: yaml

   launch_layer: python3 -m determined.launch.horovod
   entrypoint_script: python3 horovod_model.py

***********
 Next Step
***********

Decide on an API for writing a launch layer from scratch, including examples.
