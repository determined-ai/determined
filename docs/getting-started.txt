#################################
 Quickstart for Model Developers
#################################

This quickstart uses the MNIST dataset to walk you through the basic steps to install Determined,
run training experiments, and visualize the results in your browser. It uses three examples to
demonstrate the scalability and enhanced functionality gained through configuration settings:

-  Training on a local, single CPU or GPU.
-  Running a distributed training job on multiple GPUs.
-  Using hyperparameter tuning.

An experiment is a training job that consists of one or more variations, or trials, of the same
model. Using the Determined API, you automatically get metric frequency output, plots, and
checkpointing for every experiment without writing extra code. Model information, configuration
settings, output logs, and training metrics are displayed in the WebUI.

In the distributed configuration examples, a Determined cluster comprises a master and one or more
agents, with the master providing centralized management of agent resources.

Each of these quickstart examples uses the same model code and example dataset, differing only in
their configuration settings. For a list of all experiment configuration settings and more detailed
information about each, see :doc:`/training-apis/experiment-config`.

After gaining basic familiarity with Determined tools and operations, use Determined in your domain
by replacing the example files with your model data and code.

***************
 Prerequisites
***************

Software
========

-  Determined agent and master nodes must be configured with Ubuntu 16.04 or higher, CentOS 7, or
   macOS 10.13 or higher.

-  Agent nodes must have Docker installed.

-  To run jobs with GPUs, Nvidia drivers, version 384.81 or higher, must be installed on each agent.
   Nvidia drivers can be installed as part of a CUDA installation but the rest of the CUDA toolkit
   is not required.

Hardware
========

-  At least 4-CPU cores are recommended for the master node, Intel Broadwell or later, and 8GB RAM,
   and 200GB of free disk space. The master node does not require GPUs.

-  at least 2-CPU cores are recommended for each agent node, Intel Broadwell or later, and 4GB RAM,
   and 50GB of free disk space. If you are using GPUs, Nvidia GPUs with compute capability 3.7 or
   greater are required, such as K80, P100, V100, A100, GTX 1080, GTX 1080 Ti, TITAN, and TITAN XP.

Docker
======

For the distributed examples that use an agent, install Docker to run containerized workloads. If
you do not already have Docker installed, follow the :doc:`Installing Docker
</sysadmin-deploy-on-prem/requirements>` instructions to install and run Docker on Linux or macOS.

Environment Variables
=====================

To run Determined on a remote master instance, set the remote IP address and port number in the
``DET_MASTER`` environment variable:

.. code:: bash

   export DET_MASTER=<ipAddress>:8080

*******************************
 Install and Deploy Determined
*******************************

To install the Determined library and start a cluster locally, enter:

.. code:: bash

   pip install determined
   det deploy local cluster-up

If your local machine does not have a supported Nvidia GPU, include the ``no-gpu`` option:

.. code:: bash

   pip install determined
   det deploy local cluster-up --no-gpu

You can also specify a remote master IP address in the command line:

.. code:: bash

   det -m http://<ipAddress>:8080 experiment create distributed.yaml .

******************************
 Quickstart Training Examples
******************************

Download and extract the files used in this quickstart to a local folder:

#. Download link: :download:`mnist_pytorch.tgz </examples/mnist_pytorch.tgz>`.

#. Extract the configuration and model files:

   .. code:: bash

      tar xzvf mnist_pytorch.tgz

You should see the following files in the ``mnist_pytorch`` folder:

.. code::

   adaptive.yaml
   const.yaml
   data.py
   distributed.yaml
   layers.py
   model_def.py
   README.md

Configuration Files
===================

Each of the YAML-formatted configuration files corresponds to one of the following example
experiments:

+------------------------+------------------------------------------------------+
| Configuration Filename | Example Experiment                                   |
+========================+======================================================+
| ``const.yaml``         | Train a single model on a single GPU/CPU, with       |
|                        | constant hyperparameter values.                      |
+------------------------+------------------------------------------------------+
| ``distributed.yaml``   | Train a single model using multiple, distributed     |
|                        | GPUs.                                                |
+------------------------+------------------------------------------------------+
| ``adaptive.yaml``      | Perform a hyperparameter search using the Determined |
|                        | adaptive hyperparameter tuning algorithm.            |
+------------------------+------------------------------------------------------+

Model and Pipeline Definition Files
===================================

Although the Python model and data pipeline definition files are not described in this quickstart,
you might want to review them to become familiar with how interact with the Determined API in your
code:

+------------------+------------------------------------------------------------------------+
| Filename         | Experiment Type                                                        |
+==================+========================================================================+
| ``data.py``      | Model data loading and preparation code.                               |
+------------------+------------------------------------------------------------------------+
| ``layers.py``    | Convolutional layers used by the model.                                |
+------------------+------------------------------------------------------------------------+
| ``model_def.py`` | Model definition and training/validation loops.                        |
+------------------+------------------------------------------------------------------------+

***********************************
 Run a Single CPU/GPU Training Job
***********************************

This exercise trains a single model for a fixed number of batches, using constant values for all
hyperparameters on a single slot.

#. In the mnist_pytorch directory, create an experiment specifying the ``const.yaml`` configuration
   file:

   .. code:: bash

      det experiment create const.yaml .

   The last argument dot notation (.) uploads all files in the current directory to the Determined
   cluster and makes them available in the working directory in which each trial is run. This
   directory is called the context directory.

   After the experiment has been submitted, you should see that the experiment is created:

   .. code:: console

      Preparing files (.../mnist_pytorch) to send to master... 8.6KB and 7 files
      Created experiment 1

   .. tip::

      To automatically stream log messages for the first trial in an experiment to ``stdout``,
      specifying the configuration file and context directory, enter:

      .. code:: bash

         det e create const.yaml . --follow

#. Enter the cluster address in the browser address bar to view experiment progress in the WebUI. If
   you installed locally using the ``det deploy local`` command, the URL is
   ``http://localhost:8080/``. Accept the default ``determined`` username and click **Sign In**. No
   password is required.

   .. image:: /assets/images/qs01b.png
      :width: 704px
      :align: center
      :alt: Dashboard

   The figure shows that two experiments have been run. Experiment **3** has **COMPLETED** and
   experiment **4** is still **ACTIVE**. Your number and status of experiments can differ depending
   on how many times you run the example.

#. While an experiment is in the ACTIVE, training state, click the experiment tile to see the
   **Metrics** graph update with your currently defined metrics:

   .. image:: /assets/images/qs04.png
      :width: 704px
      :align: center
      :alt: Metrics graph detail

   In this example, the graph displays the loss.

#. After the experiment completes, click the experiment tile to view the trial page:

   .. image:: /assets/images/qs03.png
      :width: 704px
      :align: center
      :alt: Trial page

With an understanding of Determined fundamentals, you are ready to scale to the distributed
environment shown in the next example.

********************************
 Run a Distributed Training Job
********************************

This exercise requires a Determined cluster with multiple GPUs. The model used in this example does
not fully demonstrate the benefits of distributed training but does show how to work with additional
hardware resources.

The ``distributed.yaml`` configuration file in this example is the same as the ``const.yaml`` file
in the previous example, except that a ``resources.slots_per_trial`` field is defined and set to
``8``, which is the number of your available GPU resources. The ``slots_per_trial`` value must be
divisible by the number of GPUs per machine. You can change the value to match your hardware
configuration.

#. Create and run the experiment:

   .. code:: bash

      det experiment create distributed.yaml .

#. Enter the cluster address in your browser address bar.

#. Accept the default ``determined`` username and click **Sign In** to view the WebUI dashboard. You
   do not need a password.

#. Click the **Experiment** tile to view the experimentâ€™s trial display.

The loss curve is similar to the single-GPU experiment in the previous exercise but the time to
complete the trial is reduced by about half.

*********************************
 Run a Hyperparameter Tuning Job
*********************************

This example demonstrates hyperparameter search using the ``adaptive.yaml`` configuration file,
which is similar to the ``const.yaml`` file in the first example but includes additional
``hyperparameters`` settings and ``searcher`` parameter changes.

Hyperparameter searches involve multiple trials or model variations per experiment. The parameter
settings tell the search algorithm the ranges to explore for each hyperparameter. This example model
uses a fixed batch size and searches on dropout size, filters, and learning rate. The searcher name
is set to ``adaptive_asha`` and ``max_trials`` is set to ``16``, indicating which search algorithm
to use and how many model configurations to explore.

#. Create and run the experiment:

   .. code:: bash

      det experiment create adaptive.yaml .

#. Enter your cluster address in the browser address bar.

#. Accept the default determined username with no password and click **Sign In** to view the
   dashboard.

#. This can take some time to complete and you can monitor the progress in the WebUI Dashboard.
   Click the **Experiment** tile to access the experiment trial display and notice that more trials
   have started:

   .. image:: /assets/images/qs05.png
      :width: 704px
      :align: center
      :alt: Trials graphic

   Determined runs the number of ``max_trials`` trials and automatically starts new trials as
   resources become available. On a typical laptop, 16 trials should take around 10 minutes to train
   with at least one trial performing at about 98 percent validation accuracy. The hyperparameter
   search halts poorly performing trials.

************
 Learn More
************

For instructions on installing Determined in different environments, see:

-  :doc:`/sysadmin-basics/index` for administrator setup tasks.
-  :doc:`/sysadmin-deploy-on-prem/index` for local setup and deployment tasks.
-  :doc:`/sysadmin-deploy-on-aws/index` for AWS deployment.
-  :doc:`/sysadmin-deploy-on-gcp/index` for GCP deployment.
-  :doc:`/sysadmin-deploy-on-k8s/index` for installation and running Determined on Kubernetes.

For faster, less structured ways to run a Determined cluster without writing a model, see:

-  :ref:`commands-and-shells`
-  :ref:`notebooks`

In the :doc:`/examples` section, you can find a list of example machine learning models that have
been ported to the Determined APIs. Each example includes a model definition and one or more
experiment configuration files, with instructions on how to run the example.

To learn more about the hyperparameter search algorithm, see the :doc:`Hyperparameter Tuning
</training-hyperparameter/index>` section.
