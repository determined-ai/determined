#################################
 Quickstart for Model Developers
#################################

This quickstart walks you through the basic steps to install and run Determined, train on the
provided model data, and visualize training experiment results in your browser.

Three model data and code examples use the MNIST dataset to demonstrate:

-  training on a local, single CPU with or without a GPU.
-  how to run a distributed training job on multiple GPUs to improve performance.
-  how to use hyperparameter tuning.

Each example uses the same model code and data, changing only a configuration parameter for
different functionality.

Follow these steps for a hands-on introduction to Determined. After gaining familiarity with
Determined tools and procedures, replace the provided model data and code with your model data and
code.

***************
 Prerequisites
***************

Software
========

-  Determined agent and master nodes must be configured with Ubuntu 16.04 or higher, CentOS 7, or
   macOS 10.13 or higher.

-  Agent nodes must have Docker installed.

-  To run jobs with GPUs, Nvidia drivers, version 384.81 or higher, must be installed on each agent.
   Nvidia drivers can be installed as part of a CUDA installation but the rest of the CUDA toolkit
   is not required.

Hardware
========

-  At least 4-CPU cores are recommended for the master node, Intel Broadwell or later, and 8GB RAM,
   and 200GB of free disk space. The master node does not require GPUs.

-  at least 2-CPU cores are recommended for each agent node, Intel Broadwell or later, and 4GB RAM,
   and 50GB of free disk space. If you are using GPUs, Nvidia GPUs with compute capability 3.7 or
   greater are required, such as K80, P100, V100, A100, GTX 1080, GTX 1080 Ti, TITAN, and TITAN XP.

Docker
======

For the examples that use an agent, install Docker installed to run containerized workloads. If you
do not already have Docker installed, follow the :doc:`Installing Docker
</sysadmin-deploy-on-prem/requirements>` instructions to install and run Docker on Linux or macOS.

Training Example
================

These example experiments use the PyTorch MNIST image classification model:

#. Download the image: :download:`mnist_pytorch.tgz </examples/mnist_pytorch.tgz>`.

#. Extract the configuration and model files and browse the directory:

   .. code:: bash

      tar xzvf mnist_pytorch.tgz

The ``mnist_pytorch`` directory contains the YAML configuration files used in these examples.

Configuration parameters specify:

-  dataset location
-  hyperparameters
-  number of batches of data to use in training
-  entry point location of the model class

Each example configuration file corresponds to one of the example experiments presented in this
quickstart:

+------------------------+------------------------------------------------------+
| Configuration Filename | Experiment Type                                      |
+========================+======================================================+
| ``adaptive.yaml``      | Perform a hyperparameter search using the Determined |
|                        | adaptive hyperparameter tuning algorithm.            |
+------------------------+------------------------------------------------------+
| ``const.yaml``         | Train a single model on a single GPU/CPU, with       |
|                        | constant hyperparameter values.                      |
+------------------------+------------------------------------------------------+
| ``distributed.yaml``   | Train a single model using multiple, distributed     |
|                        | GPUs.                                                |
+------------------------+------------------------------------------------------+

While the Python model and data pipeline definition files are not covered in this quickstart, you
might want to review them to gain an understanding of how the Determined API is used:

+------------------+------------------------------------------------------------------------+
| Filename         | Experiment Type                                                        |
+==================+========================================================================+
| ``data.py``      | Model data loading and preparation code.                               |
+------------------+------------------------------------------------------------------------+
| ``layers.py``    | Convolutional layers used by the model.                                |
+------------------+------------------------------------------------------------------------+
| ``model_def.py`` | Core, model code, which includes building and compiling the model.     |
+------------------+------------------------------------------------------------------------+

***************************************
 Install and Deploy Determined Locally
***************************************

To install the Determined library and start a cluster locally, enter:

.. code:: bash

   pip install determined det
   deploy local cluster-up

If your local machine does not have a supported Nvidia GPU, include the no-gpu option to start a
cluster:

.. code:: bash

   det deploy local cluster-up --no-gpu

**********************************
 Use a Remote Determined Instance
**********************************

You can also run these examples on a remote Determined master instance by setting the remote IP
address before running any of the commands:

.. code:: bash

   export DET_MASTER=<ipAddress>:8080

You can also include the master IP address in the command line:

.. code:: bash

   det -m http://<ipAddress>:8080 experiment create distributed.yaml

***********************************
 Run a Single CPU/GPU Training Job
***********************************

An experiment is a training job that consists of one or more variations, or trials, of the same
model. A trial page contains detailed information about the model, the configuration, output logs,
and training metrics. Typically, you have to code the metric frequency output, plots, and
checkpointing while managing the configuration for each model. By integrating the Determined API,
every experiment automatically provides these capabilities without writing extra code.

This exercise trains a single model for a fixed number of batches, using constant values for all
hyperparameters on a single slot.

#. In the mnist_pytorch directory, create an experiment using the const.yaml configuration file:

   .. code:: bash

      det experiment create const.yaml .

   The last argument dot notation (.) uploads all files in the current, context directory.

   After the experiment has been submitted, you should see that the experiment is created:

   .. code:: console

      Preparing files (.../mnist_pytorch) to send to master... 8.6KB and 7 files
      Created experiment 1

   Hint: To automatically stream log messages for the first trial in an experiment to STDOUT,
   specifying the configuration file and context directory, enter:

   .. code:: bash

      det e create const.yaml . --follow

#. Enter the cluster address in the browser to view the experiment status. If you installed locally
   using the ``det deploy local`` command, the URL should be http://localhost:8080/.

#. Accept the default determined username with no password and click **Sign In** to view the
   dashboard:

   .. image:: /assets/images/qs01b.png
      :width: 704px
      :align: center
      :alt: Dashboard

   In this example, you can see two experiments. Experiment 3 is in the COMPLETED state and
   experiment 4 is in the ACTIVE state.

   While an experiment is in the ACTIVE, training state, click an experiment tile to see the Metrics
   graph update with your currently defined metrics. In this example, the graph displays the loss
   and error rate per batch:

   .. image:: /assets/images/qs04.png
      :width: 704px
      :align: center
      :alt: Metrics graph detail

#. When the experiment completes, click an experiment tile to view the trial page:

   .. image:: /assets/images/qs03.png
      :width: 704px
      :align: center
      :alt: Trial page

   Because this experiment trains a single model with a fixed set of hyperparameters, there is only
   one trial so selecting the experiment tile takes you directly to the trial page. [TBD: what does
   this mean?]

You can create additional experiments repeating the ``det experiment create`` command as a way to
gain familiarity with other Determined WebUI functions.

********************************
 Run a Distributed Training Job
********************************

This exercise requires a Determined cluster with multiple GPUs.

Determined can coordinate multiple GPUs to train a single trial more quickly by using distributed
training. Distributed training is best applied to complex models. The model used in this example
does not fully demonstrate the benefits of distributed training but is still useful in showing how
to work with additional hardware resources.

Moving from single-GPU training to distributed training involves changing a single
:ref:`resources.slots_per_trial <exp-config-resources-slots-per-trial>` setting in the experiment
``distributed.yaml`` configuration file, which is similar to the ``const.yaml`` file used in the
previous example, except that a *resources* property is added with the ``resources.slots_per_trial``
field is set to ``8`` to represent eight GPUs. The default ``slots_per_trial`` setting is ``1``. If
``slots_per_trial`` is larger than the number of slots on an agent, set
``resources.slots_per_trial`` to a multiple of the number of GPUs in each machine in the cluster.
For example, if your cluster consists of 8-GPU machines, use a value that is a multiple of eight.
For a Determined cluster deployed in the cloud, the default value for each agent is eight GPUs.

#. Create the experiment using the distributed.yaml file:

   .. code:: bash

      det experiment create distributed.yaml .

#. Enter the cluster address in the browser.

#. Accept the default determined username with no password and click **Sign In** to view the
   dashboard.

#. Click the **Experiment** tile to view the experiment’s trial display.

The loss curve is similar to the single-GPU experiment in the previous exercise but the time to
complete the trial is reduced by about half.

*********************************
 Run a Hyperparameter Tuning Job
*********************************

Hyperparameter searches involve multiple trials or model variations, per experiment, to improve
model accuracy. Typically, hyperparameter tuning takes a significant amount of time and code.
Determined hyperparameter search efficiently executes a state-of-the-art early-stopping-based
algorithm with seamless parallelization.

This example demonstrates hyperparameter search using the ``adaptive.yaml`` configuration file. The
parameter settings tell the search algorithm the ranges to explore for each hyperparameter. This
example model uses a fixed batch size and searches on dropout size, filters, and learning rate. The
searcher name is set to ``adaptive_asha`` and ``max_trials`` is set to ``16``, indicating which
search algorithm to use and how many model configurations to explore.

#. Submit the experiment specifying the configuration file:

   .. code:: bash

      det experiment create adaptive.yaml .

#. Enter your cluster address in the browser.

#. Accept the default determined username with no password and click **Sign In** to view the
   dashboard.

#. This can take some time to complete and you can monitor the progress in the WebUI Dashboard.
   Click the **Experiment** tile to access the experiment trial display and notice that more trials
   have started:

   .. image:: /assets/images/qs05.png
      :width: 704px
      :align: center
      :alt: Trials graphic

   Determined runs the number of ``max_trials`` trials and automatically starts new trials as
   resources become available. The hyperparameter search halts poorly performing trials. On a
   typical laptop, 16 trials should take around 10 minutes to train with at least one trial
   performing at about 98 percent validation accuracy.

************
 Learn More
************

For instructions on installing Determined in different environments, see:

-  :doc:`/sysadmin-basics/index` for administrator setup tasks.
-  :doc:`/sysadmin-deploy-on-prem/index` for local setup and deployment tasks.
-  :doc:`/sysadmin-deploy-on-aws/index` for AWS deployment.
-  :doc:`/sysadmin-deploy-on-gcp/index` for GCP deployment.
-  :doc:`/sysadmin-deploy-on-k8s/index` for installation and running Determined on Kubernetes.

For faster, less structured ways to run a Determined cluster without writing a model, see:

-  :ref:`commands-and-shells`
-  :ref:`notebooks`

In the :doc:`/examples` section, you can find a list of example machine learning models that have
been ported to the Determined APIs. Each example includes a model definition and one or more
experiment configuration files, with instructions on how to run the example.

To learn more about the hyperparameter search algorithm, see the :doc:`Hyperparameter Tuning
</training-hyperparameter/index>` section.
