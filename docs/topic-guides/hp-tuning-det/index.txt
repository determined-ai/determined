.. _topic-guides_hp-tuning-det:

Hyperparameter Tuning With Determined
=====================================

Hyperparameter tuning is a common machine learning workflow that involves
appropriately configuring the data, model architecture, and learning algorithm
to yield an effective model. Hyperparameter tuning is a :ref:`challenging
problem<topic-guides_hp-tuning-basics-difficulty>` in deep learning given the
potentially large number of hyperparameters to consider. Determined provides
support for hyperparameter search as a first-class workflow that is tightly
integrated with Determined's job scheduler, which allows for efficient execution
of state-of-the-art early-stopping based approaches as well as seamless
parallelization of these methods.

Adaptive Search
---------------
Our default recommended search method is adaptive search, an early-stopping
based technique that speeds up traditional techniques like random search by
periodically abandoning low-performing hyperparameter configurations in a
principled fashion. Adaptive search builds on the two prototypical adaptive
downsampling approaches, `Successive Halving
<https://arxiv.org/pdf/1502.07943.pdf>`_ (SHA) and `Hyperband
<https://arxiv.org/abs/1603.06560>`_, while also enabling seamless
parallelization and a simpler user interface.

Determined offers two adaptive searchers that employ the same underlying
algorithm but differ in their level of configurability.

#. :ref:`Adaptive (Simple)<topic-guides_hp-tuning-det_adaptive-simple>` is
   easier to configure and provides sensible default settings for most
   situations. This searcher requires just two intuitive settings: the number of
   configurations to evaluate, and the maximum allowed resource budget per
   configuration.  We recommend starting with this searcher.
#. :ref:`Adaptive (Advanced) <topic-guides_hp-tuning-det_adaptive-advanced>`
   allows users to control the adaptive search behavior in a more fine-grained
   way.

Other Supported Methods
-----------------------
Determined also supports other common hyperparameter search algorithms:

#. :ref:`Single<topic-guides_hp-tuning-det_single>` is appropriate for manual
   hyperparameter tuning, as it trains a single hyperparameter configuration.
#. :ref:`Grid<topic-guides_hp-tuning-det_grid>` brute force evaluates
   all possible hyperparameter configurations and returns the best.
#. :ref:`Random<topic-guides_hp-tuning-det_random>` evaluates a set of
   hyperparameter configurations chosen at random and returns the best.
#. :ref:`Population-based training (PBT)<topic-guides_hp-tuning-det_pbt>` begins
   as random search but periodically replaces low-performing hyperparameter
   configurations with ones *near* the high-performing points in the
   hyperparameter space.

Handling Trial Errors and Early Stopping Requests
-------------------------------------------------
When a trial errors, we try to restart from the latest checkpoint unless we
have done so ``max_restarts`` times, which is configured in the experiment
configuration. Once we have reached ``max_restarts``, we set the trial as
errored. For search methods that adapt to validation metric values (Adaptive
and PBT search methods do this), we do not continue training the model even if
the search method would typically call for us to continue training.

When a trial requests to stop early via a framework callback or manually via
``context.set_stop_requested(True)``, we finish the current training or
validation step, and stop training like above.

Next Steps
------------------------

- :ref:`Experiment Configuration<experiment-configuration_searcher>`

.. toctree::
  :hidden:

  hp-single
  hp-grid
  hp-random
  hp-adaptive-simple
  hp-adaptive-advanced
  hp-pbt
