.. _topic-guides_hp-tuning-det:

#######################################
 Hyperparameter Tuning With Determined
#######################################

Hyperparameter tuning is a common machine learning workflow that
involves appropriately configuring the data, model architecture, and
learning algorithm to yield an effective model. Hyperparameter tuning is
a :ref:`challenging problem <topic-guides_hp-tuning-basics-difficulty>`
in deep learning given the potentially large number of hyperparameters
to consider. Determined provides support for hyperparameter search as a
first-class workflow that is tightly integrated with Determined's job
scheduler, which allows for efficient execution of state-of-the-art
early-stopping based approaches as well as seamless parallelization of
these methods.

*****************
 Adaptive Search
*****************

Our default recommended search method is `Adaptive (ASHA)
<https://arxiv.org/pdf/1810.05934.pdf>`_, a state-of-the-art
early-stopping based technique that speeds up traditional techniques
like random search by periodically abandoning low-performing
hyperparameter configurations in a principled fashion.

:ref:`Adaptive (ASHA) <topic-guides_hp-tuning-det_adaptive-asha>` offers
asynchronous search functionality more suitable for large-scale HP
search experiments in the distributed setting.

*************************
 Other Supported Methods
*************************

Determined also supports other common hyperparameter search algorithms:

#. :ref:`Single <topic-guides_hp-tuning-det_single>` is appropriate for
   manual hyperparameter tuning, as it trains a single hyperparameter
   configuration.

#. :ref:`Grid <topic-guides_hp-tuning-det_grid>` brute force evaluates
   all possible hyperparameter configurations and returns the best.

#. :ref:`Random <topic-guides_hp-tuning-det_random>` evaluates a set of
   hyperparameter configurations chosen at random and returns the best.

#. :ref:`Population-based training (PBT)
   <topic-guides_hp-tuning-det_pbt>` begins as random search but
   periodically replaces low-performing hyperparameter configurations
   with ones *near* the high-performing points in the hyperparameter
   space.

***************************************************
 Handling Trial Errors and Early Stopping Requests
***************************************************

When a trial encounters an error or fails unexpectedly, Determined will
restart it from the latest checkpoint unless we have done so
:ref:`max_restarts <max-restarts>` times, which is configured in the
experiment configuration. Once we have reached ``max_restarts``, any
further trials that fail will be marked as errored and will not be
restarted. For search methods that adapt to validation metric values
(:ref:`Adaptive (ASHA) <topic-guides_hp-tuning-det_adaptive-asha>`,
:ref:`Adaptive(Simple) <topic-guides_hp-tuning-det_adaptive-simple>`,
:ref:`Adaptive (Advanced)
<topic-guides_hp-tuning-det_adaptive-advanced>`, and
:ref:`Population-based training (PBT)
<topic-guides_hp-tuning-det_pbt>`), we do not continue training errored
trials, even if the search method would typically call for us to
continue training. This behavior is useful when some parts of the
hyperparameter space result in models that cannot be trained
successfully (e.g., the search explores a range of batch sizes and some
of those batch sizes cause GPU OOM errors). An experiment can complete
successfully as long as at least one of the trials within it completes
successfully.

Trial code can also request that training be stopped early, e.g., via a
framework callback such as `tf.keras.callbacks.EarlyStopping
<https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping>`__
or manually by calling
:meth:`determined.TrialContext.set_stop_requested`. When early stopping
is requested, Determined will finish the current training or validation
workload and checkpoint the trial. Trials that are stopped early are
considered to be "completed", whereas trials that fail are marked as
"errored".

************
 Next Steps
************

-  :ref:`Experiment Configuration <experiment-configuration_searcher>`

.. toctree::
   :hidden:

   hp-single
   hp-grid
   hp-random
   hp-adaptive-simple
   hp-adaptive-advanced
   hp-adaptive-asha
   hp-pbt
