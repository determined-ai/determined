.. _pytorch-mnist-tutorial:

PyTorch MNIST Tutorial
=============================

This tutorial walks through how to implement an existing PyTorch MNIST model in Determined. MNIST consists of handwritten digits, commonly used to test image classification models. This tutorial is based on the official `PyTorch MNIST example <https://github.com/PyTorch/examples/blob/master/mnist/main.py>`_.

*This tutorial requires the Determined CLI. For installation procedures:* :ref:`install-cli`.
We will also use ``torch``, ``torchvision``, and ``numpy`` libraries to build and train our model.

At the end of this tutorial, the reader should know:

#. Basic Concepts of ``PyTorchTrial``
#. Basic Understanding of ``TrialContext``
#. How to Implement a PyTorch Model with ``PyTorchTrial``
#. How to Write a Determined YAML Experiment Configuration
#. A General Procedure of a Determined Workflow

Note: As we walk through the tutorial, some code has been omitted for demonstration purposes. A complete code example is available at :download:`MNIST PyTorch </examples/mnist_pytorch.tgz>`.

Overview
--------
PyTorch is a deep learning research platform used to develop models. Although PyTorch provides excellent capabilities for training research prototypes, it can be challenging to convert these prototypes into production-grade applications. Using Determined for your PyTorch models maintains the PyTorch user experience while unlocking production-grade features such as state-of-the-art distributed training and hyperparameter tuning, experiment tracking, log management, metrics visualization, reproducibility, and dependency management, along with flexibility to share compute resources on the hardware of your choice. To unlock these features, users can leverage Determined's ``PyTorchTrial`` interface.

Traditionally in PyTorch, the user defines a model, a data loader, and optimizer then manages the optimizer, the backprop, cross-batch metric calculations, and other training steps. Determined's ``PyTorchTrial`` interface has the user define the model, data loader and optimizer, similar to PyTorch. However, Determined connects the pieces by handling the device management, training loop, and training steps, so you can focus on the task at hand---training better models.

To learn more about the benefits of Determined, read: :ref:`benefits-of-determined`.

This tutorial walks through building a Determined ``PyTorchTrial`` class and the necessary steps to run an experiment. The core file will be ``model_def.py`` and contains the MNIST ``PyTorchTrial`` class. Additionally, the Determined system expects two files to be provided:

#. an entrypoint (``__init__.py``)
    - This file is our entrypoint. It imports the user defined ``PyTorchTrial`` class.
#. an experiment configuration file (``*.yaml``)
    - This file contains information on the hyperparameters and other details for model and experiment configuration used internally and by user code. This can include, but is not limited to: global_batch_size, learning_rate, and layer size.

To learn more about yaml files, read: :ref:`topic-guides_yaml`. To learn more about the experiment configuration file, read: :ref:`experiment-configuration`.

The next sections will describe how to build a ``PyTorchTrial`` class.

Building a ``PyTorchTrial`` Class
---------------------------------
Determined's interface requires users to implement a class that inherits a Trial class based on the user's preferred framework. This tutorial uses the ``PyTorchTrial`` interface, which requires six specific functions to be overridden: ``build_training_data_loader``, ``build_validation_data_loader``, ``build_model``, ``optimizer``, ``train_batch`` and ``evaluate_batch``.

Each of these functions should contain code resembling traditional PyTorch. For example, the ``optimizer`` function should return a ``torch.optim`` object. By overriding the six functions, Determined will manage these common training objects eliminating the need to worry about when to properly calculate backprop, zero out gradients or other training steps.

The code snippet below demonstrates the skeleton of ``PyTorchTrial``. In the next few sections, we will implement each function.

.. code:: python

    from determined.pytorch import DataLoader, PyTorchTrial

    class MNISTTrial(PyTorchTrial):
        def __init__(self, context: det.TrialContext):
            # required: store the TrialContext for later use
            self.context = context

        def build_training_data_loader(self):
            # create the train data
            # return a Determined DataLoader
            return DataLoader()

        def build_validation_data_loader(self):
            # create the validation data
            # return a Determined DataLoader
            return DataLoader()

        def build_model(self):
            # define model
            return model

        def optimizer(self, model: nn.Module):
            # create optimizer
            return optimizer

        def train_batch(self, batch: TorchData, model: nn.Module, epoch_idx: int, batch_idx: int):
            # run the model for a batch
            # return at least the loss function
            return {"loss": loss}

        def evaluate_batch(self, batch: TorchData, model: nn.Module):
            # return metrics to measure evaluations
            return {"validation_error": error} ""

Initializing ``PyTorchTrial``
"""""""""""""""""""""""""""""
Determined's ``PyTorchTrial`` supports model development by having the machine learning developer focus on defining the main train objects. By inheriting the ``PyTorchTrial`` class, the user does not manually call each required function; instead, Determined will call each function at the appropriate time to manage resources and training steps. In other words, you really only state what optimizer and model to use as Determined handles the object calls. Since the ``PyTorchTrial`` interface expects these functions to be overridden, each function needs to have the correct parameters.

Also, Determined's ``PyTorchTrial.__init__`` is passed a single parameter: a :class:`TrialContext <determined.TrialContext>`. The ``TrialContext`` object, often stored as ``self.context``, contains a collection of runtime information for the particular trial, such as the hyperparameters in the experiment configuration file. Since it is highly encouraged to store all hyperparameters (i.e. batch size, learning rate) inside the experiment configuration file, the ``self.context`` object allows us to easily access the defined hyperparameters or data configurations.

The user must store the ``TrialContext`` for later use, as shown in the code snippet below.

.. code:: python

    def __init__(self, context: det.TrialContext):
        self.context = context

The code snippet below shows how to access the defined variables from the experiment configuration file.

.. code:: python

    self.context.get_hparam(<variable name>)
    self.context.get_data_config()[<variable name>]


Download and Prepare MNIST Dataset
----------------------------------

Downloading Data
""""""""""""""""

First, we create our data objects. For this tutorial, we will download MNIST directly. Determined launches a separate process for each each gpu during :ref:`multi-gpu-training`, so we add a unique ID to the download directory.

.. code:: python

    def download_data(self):
        download_directory = "/tmp/work_dir/{}/MNIST".format(self.get.get_rank())
        filepath = os.path.join(download_directory, basename)
        logging.info("Downloading {}".format(url))
        r = requests.get(url, stream=True)

        ...

        return os.path.dirname(download_directory)


Creating DataLoaders
""""""""""""""""""""
Traditionally in PyTorch, a `PyTorch DataLoader <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`_ defines data access to an underlying Dataset. A Determined DataLoader is an iterator handled by Determined that assigns the data to the corresponding resource. For ``build_training_data_loader`` and ``build_validation_data_loader``, we return a Determined DataLoader that represents the train and validation data respectively. Behind the scenes, the Determined DataLoader wraps a PyTorch DataLoader to allow Determined to load and assign the data to the correct resource (i.e. GPU) and automatically batch for ``train_batch``. From a user perspective, the Determined DataLoader has almost the same parameters and is treated very similar to traditional PyTorch DataLoader.

The code snippet below uses ``build_training_data_loader`` to load the PyTorch MNIST datasets. After loading the data, we pass the dataset into the Determined DataLoader and assign the batch size. The batch size is stored inside the experiment configuration file and accessed through the ``TrialContext`` or ``context`` object.

.. code:: python

    from determined.pytorch import DataLoader

    def build_training_data_loader(self):
        download_data_dir = download_data()
        train_data = datasets.MNIST(data_dir, train=True)
        batch_size = self.context.get_per_slot_batch_size()
        return DataLoader(train_data, batch_size=batch_size)


Building the Model
""""""""""""""""""
Determined calls ``build_model`` to define our model. In ``build_model``, the model object is defined and the object is returned. The original PyTorch example uses the Sequential API to create the model. The ``build_model`` function mimics the original model implementation:

.. code:: python

    from determined.pytorch import reset_parameters
    ...

    def build_model(self):
        model = nn.Sequential(
            nn.Conv2d(1, self.context.get_hparam("n_filters1"), kernel_size=5),
            nn.MaxPool2d(2),
            nn.ReLU(),
            nn.Conv2d(
                self.context.get_hparam("n_filters1"), self.context.get_hparam("n_filters2"), kernel_size=5,
            ),
            nn.MaxPool2d(2),
            nn.ReLU(),
            Flatten(),
            nn.Linear(16 * self.context.get_hparam("n_filters2"), 50),
            nn.ReLU(),
            nn.Dropout2d(self.context.get_hparam("dropout")),
            nn.Linear(50, 10),
            nn.LogSoftmax(),
        )

        reset_parameters(model)
        return model

Defining the Optimizer
""""""""""""""""""""""
Next, Determined calls the ``optimizer`` function. This function passes the generated model from ``build_model`` as a parameter and expects a ``torch.optim`` object to be returned. We do not need to call the optimizer during training because Determined will handle ``.step()`` and ``.zero_grad()``. In the code snippet below, the learning rate is stored in the experiment configuration file and accessed through the ``TrialContext`` object: ``self.context``.

.. code:: python

    def optimizer(self, model: nn.Module):
        return torch.optim.SGD(
            model.parameters(), lr=self.context.get_hparam("learning_rate"), momentum=0.9
        )

Defining the Training Loop
""""""""""""""""""""""""""
In PyTorch, the user defines the training loop based on the ``PyTorch.DataLoader`` to access a batch of data; however, Determined handles the loop for us. The ``train_batch`` and ``evaluate_batch`` functions provide the current batch and model. At this point, the correct resource has been assigned to the current batch and model.

The ``train_batch`` computes the forward pass and calculates the loss, similar to a traditional PyTorch training loop. The expected return values include a dictionary with the calculated loss and other user defined metrics. Since Determined handles the backprop call, we do not need to call loss.backwards().

In this case, we calculate the loss with ``torch.nn.functional.nll_loss`` and return the result under the key ``loss``. PyTorch's framework does not handle combining the metrics; however, Determined will automatically average all the metrics returned in the dictionary. In the example code below, we return the calculated error for this batch and Determined will average all the train_error keys together.

.. code:: python

    def error_rate(predictions: torch.Tensor, labels: torch.Tensor) -> float:
        matches = predictions.argmax(1) == labels.to(torch.long)
        return 1.0 - float(matches.sum()) / predictions.shape[0]

    ...

    def train_batch(self, batch: TorchData, model: nn.Module, epoch_idx: int, batch_idx: int):
        batch = cast(Tuple[torch.Tensor, torch.Tensor], batch)
        data, labels = batch

        output = model(data)
        loss = torch.nn.functional.nll_loss(output, labels)
        error = error_rate(output, labels)

        return {"loss": loss, "train_error": error}


For evaluation, Determined loops through our evaluation DataLoader defined from ``build_validation_data_loader``, similar to ``train_batch``. We compute the forward pass with the trained model and calculate the validation metrics. Determined will average the results.

.. code:: python

    def evaluate_batch(self, batch: TorchData, model: nn.Module):
        batch = cast(Tuple[torch.Tensor, torch.Tensor], batch)
        data, labels = batch

        output = model(data)
        error = error_rate(output, labels)

        return {"validation_error": error}


Training the Model
------------------

Lastly, we create an experiment configuration file that contains all the experiment information for Determined. We set the hyperparameters, along with the name or description of the experiment. In Determined, we define length of training based on the number of steps. One step consists of 100 batches by default. For this tutorial, we set ``max_steps`` to 2, so 200 batches will run.

More information on the experiment configuration file can be found: :ref:`experiment-configuration`.

.. code:: yaml

    description: mnist_pytorch_const
    hyperparameters:
        learning_rate: 0.001
        dropout: 0.5
        global_batch_size: 64
        n_filters1: 40
        n_filters2: 40
    searcher:
        name: single
        metric: validation_error
        max_steps: 20
        smaller_is_better: true
    entrypoint: model_def:MNistTrial

Running an Experiment
---------------------
When a user wants to run their model, they submit an experiment. **Experiment** is a collection of one or more trials. **Trial** is a training task with a dataset, a deep learning model, and a defined set of hyperparameters. An experiment corresponds to a set of trials that are exploring a user-defined hyperparameter space. For example, during a learning rate hyperparameter search, an experiment can consists of three trials with ``.001``, ``.01``, and ``.1`` learning rates.

More information on Determined's Terminologies can be found on the topic guide: :ref:`terminologies-concepts`

This tutorial will run an experiment with one trial because all the hyperparameter are specifically defined. To start the experiment, we run:

.. code::

    det experiment create const.yaml .

Here, the first argument (``const.yaml``) specifies the experiment configuration file and the second argument (``.``) the location of the directory that contains our model definition files. An optional flag ``-m`` allows the user to state the master's IP address inline. More information on the CLI can be found at the reference page: :ref:`cli`.

Once the experiment is started, you will see a notification:

.. code::

    Preparing files (../mnist_pytorch) to send to master... 2.5KB and 4 files
    Created experiment xxx
    Activated experiment xxx

Evaluating the Model
--------------------

Model evaluation is done automatically for you by Determined.
To access information on both training and validation performance,
simply go to the WebUI by entering the address of your ``DETERMINED_MASTER``
in your web browser.

Once you are on the Determined landing page, you can find your experiment
either via the experiment ID (xxx) or via its description.

This code is also available at
:download:`MNIST PyTorch </examples/mnist_pytorch.tgz>`.

Next Steps
----------
- :ref:`tf-cifar-tutorial`
- :ref:`experiment-configuration`
- :ref:`command-configuration`
- :ref:`topic-guides_yaml`
- :ref:`benefits-of-determined`
- :ref:`terminologies-concepts`
