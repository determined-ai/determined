##############################
 determined.pytorch.lightning
##############################

.. _lightning-adapter:

***************************************************
 ``determined.pytorch.lightning.LightningAdapter``
***************************************************

Pytorch Lightning Adapter, defined here as ``LightningAdapter``,
provides a quick way to train your Pytorch Lightning models with all the
Determined features, such as mid-epoch preemption, easy distributed
training, simple job submission to the Determined cluster, and so on.

LightningAdapter is built on top of our :ref:`PyTorchTrial
<pytorch-trial>` API, which has a built-in training loop that integrates
with the Determined features. However, it only supports `LightningModule
<https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html>`_
(v1.2.0). To migrate your code from the `Trainer
<https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html>`_,
please read more about :ref:`PyTorchTrial <pytorch-trial>` and
:ref:`experiment-configuration`.

Porting your ``PyTorchLightning`` code is often pretty simple: 1. Bring
in your ``LightningModule`` and ``LightningDataModule`` and initialize
them 2. Create a new trial based on ``LightningAdapter`` and initialize
it. 3. Define the dataloaders.

Here is an example:

.. code:: python

   from determined.pytorch import PyTorchTrialContext, DataLoader
   from determined.pytorch.lightning import LightningAdapter

   # bring in your LightningModule and optionally LightningDataModule
   from mnist import LightningMNISTClassifier, MNISTDataModule


   class MNISTTrial(LightningAdapter):
       def __init__(self, context: PyTorchTrialContext) -> None:
           # instantiate your LightningModule with hyperparameter from the Determined
           # config file or from the searcher for automatic hyperparameter tuning.
           lm = LightningMNISTClassifier(lr=context.get_hparam("learning_rate"))

           # instantiate your LightningDataModule and make it distributed training ready.
           data_dir = f"/tmp/data-rank{context.distributed.get_rank()}"
           self.dm = MNISTDataModule(context.get_data_config()["url"], data_dir)

           # initialize LightningAdapter.
           super().__init__(context, lightning_module=lm)
           self.dm.prepare_data()

       def build_training_data_loader(self) -> DataLoader:
           self.dm.setup()
           dl = self.dm.train_dataloader()
           return DataLoader(
               dl.dataset, batch_size=dl.batch_size, num_workers=dl.num_workers
           )

       def build_validation_data_loader(self) -> DataLoader:
           self.dm.setup()
           dl = self.dm.val_dataloader()
           return DataLoader(
               dl.dataset, batch_size=dl.batch_size, num_workers=dl.num_workers
           )

.. autoclass:: determined.pytorch.lightning.LightningAdapter
   :members:
   :exclude-members: trial_controller_class, trial_context_class, evaluate_full_dataset, train_batch, evaluate_batch, setup_optimizers_schedulers
   :inherited-members:
   :member-order: bysource
   :special-members: __init__

In this approach, the ``LightningModule`` is not paired with the PyTorch
Lightning ``Trainer`` so that there are some methods and hooks that are
not supported. Read about those here:

-  No separate test-set definition in Determined: ``test_step``,
   ``test_step_end``, ``test_epoch_end``, ``on_test_batch_start``,
   ``on_test_batch_end``, ``on_test_epoch_start``,
   ``on_test_epoch_end``, ``test_dataloader``.

-  No fit or pre-train stage: ``setup``, ``teardown``, ``on_fit_start``,
   ``on_fit_end``, ``on_pretrain_routine_start``,
   ``on_pretrain_routine_end``.

-  Additionally, no: ``training_step_end`` & ``validation_step_end``,
   ``hiddens`` parameter in ``training_step`` and ``tbptt_split_batch``,
   ``transfer_batch_to_device``, ``get_progress_bar_dict``,
   ``on_train_epoch_end``, ``manual_backward``, ``backward``,
   ``optimizer_step``, ``optimizer_zero_grad``

-  Support coming soon: `precision`, `on_load_checkpoint`,
   `on_save_checkpoint`

In addition, we also patched some ``LightningModule`` methods to make
porting your code easier:

-  `log` and `log_dict` are patched to always log ship their values to
   Tensorboard. In the current version only the first two arguments in
   ``log``: ``key`` and ``value``, and the first argument in
   ``log_dict`` are supported.

.. note::

   Make sure to return the metric you defined as ``searcher.metric`` in
   your experiment's :ref:`configuration <experiment-configuration>`
   from your ``validation_step``.

.. note::

   Determined will automatically log the metrics you return from
   ``training_step`` and ``validation_step`` to Tensorboard.

Data Loading
============

Loading your dataset when using the :ref:`LightningAdapter
<lightning-adapter>` works the same way as it does with :ref:`PyTorch
Trial <pytorch-trial>`.

If you already have a ``LightningDataModule`` you can bring it in and
use it to implement ``build_training_data_loader`` and
``build_validation_data_loader`` methods easily. For more information
read PyTorchTrial's section on Data Loading.

***********
 Debugging
***********

Please see :ref:`model-debug`.

**********
 Examples
**********

-  :download:`gan_mnist_pl.tgz </examples/gan_mnist_pl.tgz>`
-  :download:`mnist_pl.tgz </examples/mnist_pl.tgz>`
