determined.pytorch
==================

.. _pytorch-trial:

``determined.pytorch.PyTorchTrial``
-----------------------------------

.. autoclass:: determined.pytorch.PyTorchTrial
    :members:
    :exclude-members: trial_controller_class, trial_context_class, build_model, optimizer, create_lr_scheduler
    :inherited-members:
    :member-order: bysource
    :special-members: __init__

.. autoclass:: determined.pytorch.LRScheduler
    :members:
    :special-members: __init__

.. autoclass:: determined.pytorch.Reducer
    :members:

.. _pytorch-data-loading:

Data Loading
~~~~~~~~~~~~

Loading data into ``PyTorchTrial`` models is done by defining two functions,
``build_training_data_loader()`` and ``build_validation_data_loader()``.
These functions should each return an instance of
``determined.pytorch.DataLoader``.  ``determined.pytorch.DataLoader`` behaves
the same as ``torch.utils.data.DataLoader`` and is a drop-in replacement.

Each ``DataLoader`` is allowed to return batches with arbitrary
structures of the following types, which will be fed directly to the
``train_batch`` and ``evaluate_batch`` functions:

-  ``np.ndarray``

   .. code:: python

      np.array([[0, 0], [0, 0]])

-  ``torch.Tensor``

   .. code:: python

      torch.Tensor([[0, 0], [0, 0]])

-  tuple of ``np.ndarray``\ s or ``torch.Tensor``\ s

   .. code:: python

      (torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]]))

-  list of ``np.ndarray``\ s or ``torch.Tensor``\ s

   .. code:: python

      [torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]])]

-  dictionary mapping strings to ``np.ndarray``\ s or
   ``torch.Tensor``\ s

   .. code:: python

      {"data": torch.Tensor([[0, 0], [0, 0]]), "label": torch.Tensor([[1, 1], [1, 1]])}

-  combination of the above

   .. code:: python

      {
          "data": [
              {"sub_data1": torch.Tensor([[0, 0], [0, 0]])},
              {"sub_data2": torch.Tensor([0, 0])},
          ],
          "label": (torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]])),
      }

Trial Context
~~~~~~~~~~~~~

:class:`determined.pytorch.PyTorchTrialContext` subclasses :class:`determined.TrialContext`.
It provides useful methods for writing ``Trial`` subclasses.

.. autoclass:: determined.pytorch.PyTorchTrialContext
    :members:
    :exclude-members: get_model, get_optimizer, get_lr_scheduler


Gradient Clipping
^^^^^^^^^^^^^^^^^

Users need to pass a gradient clipping function to
:meth:`determined.pytorch.PyTorchTrialContext.step_optimizer`.


.. _pytorch-callbacks:

Callbacks
~~~~~~~~~

To execute arbitrary Python code during the lifecycle of a
``PyTorchTrial``, implement the callback interface:

.. autoclass:: determined.pytorch.PyTorchCallback
    :members:

``ReduceLROnPlateau``
^^^^^^^^^^^^^^^^^^^^^

To use the `torch.optim.lr_scheduler.ReduceLROnPlateau
<https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau>`_
class with ``PyTorchTrial``, implement the following callback:

.. code::

    class ReduceLROnPlateauEveryValidationStep(PyTorchCallback):
        def __init__(self, context):
            self.reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(
                context.get_optimizer(), "min", verbose=True
            )  # customize arguments as desired here

        def on_validation_end(self, metrics):
            self.reduce_lr.step(metrics["validation_error"])

        def state_dict(self):
            return self.reduce_lr.state_dict()

        def load_state_dict(self, state_dict):
            self.reduce_lr.load_state_dict(state_dict)

Then, implement the ``build_callbacks`` function in ``PyTorchTrial``:

.. code::

    def build_callbacks(self):
        return {"reduce_lr": ReduceLROnPlateauEveryValidationStep(self.context)}

.. _migration-guide-flexible-primitives:

Migration from deprecated interface
-----------------------------------

The current PyTorch interface is designed to be flexible and to support multiple
models, optimizers, and LR schedulers. The ability to run forward and backward
passes in an arbitrary order affords users much greater flexibility compared to
the `deprecated approach
<https://docs.determined.ai/0.12.12/reference/api/pytorch.html>`__ used in
Determined 0.12.12 and earlier.

To migrate from the previous PyTorch API, please change the following places in
your code:

#. Wrap models, optimizers, and LR schedulers in the :meth:`__init__` method with the
   ``wrap_model``, ``wrap_optimizer``, and ``wrap_lr_scheduler`` methods that are provided by
   :class:`PyTorchTrialContext <determined.pytorch.PyTorchTrialContext>`. At the same time,
   remove the implementation of :meth:`build_model`, :meth:`optimizer`,
   :meth:`create_lr_scheduler`.

#. If using automatic mixed precision (AMP), configure Apex AMP in the ``__init__`` method
   with the :meth:`context.configure_apex_amp
   <determined.pytorch.PyTorchTrialContext>` method.  At the same time, remove
   the experiment configuration field ``optimizations.mixed_precision``.

#. Run backward passes on losses and step optimizers in the :meth:`train_batch` method with
   the ``backward`` and ``step_optimizer`` methods provided by
   :class:`PyTorchTrialContext <determined.pytorch.PyTorchTrialContext>`. Clip gradients by passing a function
   to the ``clip_grads`` argument of ``step_optimizer``
   while removing the ``PyTorchCallback`` counterpart in the :meth:`build_callbacks` method.

Examples
--------

-  :download:`cifar10_cnn_pytorch.tgz </examples/cifar10_cnn_pytorch.tgz>`
-  :download:`mnist_pytorch.tgz </examples/mnist_pytorch.tgz>`
