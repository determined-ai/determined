####################
 determined.pytorch
####################

.. _pytorch-trial:

*************************************
 ``determined.pytorch.PyTorchTrial``
*************************************

.. autoclass:: determined.pytorch.PyTorchTrial
   :members:
   :exclude-members: trial_controller_class, trial_context_class, build_model, optimizer, create_lr_scheduler
   :inherited-members:
   :member-order: bysource
   :special-members: __init__

.. autoclass:: determined.pytorch.LRScheduler
   :members:
   :special-members: __init__

.. autoclass:: determined.pytorch.Reducer
   :members:

.. autoclass:: determined.tensorboard.metric_writers.pytorch.TorchWriter

.. _pytorch-data-loading:

Data Loading
============

Loading data into ``PyTorchTrial`` models is done by defining two
functions, ``build_training_data_loader()`` and
``build_validation_data_loader()``. These functions should each return
an instance of ``determined.pytorch.DataLoader``.
``determined.pytorch.DataLoader`` behaves the same as
``torch.utils.data.DataLoader`` and is a drop-in replacement.

Each ``DataLoader`` is allowed to return batches with arbitrary
structures of the following types, which will be fed directly to the
``train_batch`` and ``evaluate_batch`` functions:

-  ``np.ndarray``

   .. code:: python

      np.array([[0, 0], [0, 0]])

-  ``torch.Tensor``

   .. code:: python

      torch.Tensor([[0, 0], [0, 0]])

-  tuple of ``np.ndarray``\ s or ``torch.Tensor``\ s

   .. code:: python

      (torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]]))

-  list of ``np.ndarray``\ s or ``torch.Tensor``\ s

   .. code:: python

      [torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]])]

-  dictionary mapping strings to ``np.ndarray``\ s or
   ``torch.Tensor``\ s

   .. code:: python

      {"data": torch.Tensor([[0, 0], [0, 0]]), "label": torch.Tensor([[1, 1], [1, 1]])}

-  combination of the above

   .. code:: python

      {
          "data": [
              {"sub_data1": torch.Tensor([[0, 0], [0, 0]])},
              {"sub_data2": torch.Tensor([0, 0])},
          ],
          "label": (torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]])),
      }

Trial Context
=============

:class:`determined.pytorch.PyTorchTrialContext` subclasses
:class:`determined.TrialContext`. It provides useful methods for writing
``Trial`` subclasses.

.. autoclass:: determined.pytorch.PyTorchTrialContext
   :members:
   :exclude-members: get_model, get_optimizer, get_lr_scheduler

.. autoclass:: determined.pytorch.PyTorchExperimentalContext
   :members:
   :exclude-members: reduce_metrics, reset_reducers

Gradient Clipping
-----------------

Users need to pass a gradient clipping function to
:meth:`determined.pytorch.PyTorchTrialContext.step_optimizer`.

.. _pytorch-callbacks:

Reducing Metrics
================

Determined supports proper reduction of arbitrary training and
validation metrics, even during distributed training, by allowing users
to define custom reducers. Custom reducers can be either a function or
an implementation of the :class:`determined.pytorch.MetricReducer`
interface.

See :meth:`determined.pytorch.PyTorchExperimentalContext.wrap_reducer()`
for more details.

.. autoclass:: determined.pytorch.MetricReducer
   :members: reset, per_slot_reduce, cross_slot_reduce
   :member-order: bysource

Callbacks
=========

To execute arbitrary Python code during the lifecycle of a
``PyTorchTrial``, implement the callback interface:

.. autoclass:: determined.pytorch.PyTorchCallback
   :members:

.. _migration-guide-flexible-primitives:

*************************************
 Migration from deprecated interface
*************************************

The current PyTorch interface is designed to be flexible and to support
multiple models, optimizers, and LR schedulers. The ability to run
forward and backward passes in an arbitrary order affords users much
greater flexibility compared to the `deprecated approach
<https://docs.determined.ai/0.12.12/reference/api/pytorch.html>`__ used
in Determined 0.12.12 and earlier.

To migrate from the previous PyTorch API, please change the following
places in your code:

#. Wrap models, optimizers, and LR schedulers in the :meth:`__init__`
   method with the ``wrap_model``, ``wrap_optimizer``, and
   ``wrap_lr_scheduler`` methods that are provided by
   :class:`PyTorchTrialContext
   <determined.pytorch.PyTorchTrialContext>`. At the same time, remove
   the implementation of :meth:`build_model`, :meth:`optimizer`,
   :meth:`create_lr_scheduler`.

#. If using automatic mixed precision (AMP), configure Apex AMP in the
   ``__init__`` method with the :meth:`context.configure_apex_amp
   <determined.pytorch.PyTorchTrialContext>` method. At the same time,
   remove the experiment configuration field
   ``optimizations.mixed_precision``.

#. Run backward passes on losses and step optimizers in the
   :meth:`train_batch` method with the ``backward`` and
   ``step_optimizer`` methods provided by :class:`PyTorchTrialContext
   <determined.pytorch.PyTorchTrialContext>`. Clip gradients by passing
   a function to the ``clip_grads`` argument of ``step_optimizer`` while
   removing the ``PyTorchCallback`` counterpart in the
   :meth:`build_callbacks` method.

**********
 Examples
**********

-  :download:`cifar10_pytorch.tgz </examples/cifar10_pytorch.tgz>`
-  :download:`mnist_pytorch.tgz </examples/mnist_pytorch.tgz>`
-  :download:`fasterrcnn_coco_pytorch.tgz
   </examples/fasterrcnn_coco_pytorch.tgz>`
