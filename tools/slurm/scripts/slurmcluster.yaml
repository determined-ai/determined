# This is a generic devcluster config with variables for the target
# Slurm test systems.   It is intended to be invoked via the
# tools/slurmcluster.sh script, which is customized to support
# per-user tunnel configuration, and per-system settings.

# This startup input will cause the harness to rebuild on startup.
startup_input: "p"

commands:
  p: make -C $OPT_PROJECT_ROOT/harness build        # rebuild Python
  w: make -C $OPT_PROJECT_ROOT/webui build          # rebuild Webui
  c: make -C $OPT_PROJECT_ROOT/docs build           # rebuild doCs

# Three stages: db, master, and agent.
stages:
  - db:
      port: 5432
      db_name: determined
      password: postgres
      container_name: determined_db
      image_name: "postgres:10.14"

      # data_dir is where the persistent files will be saved to.  If this key
      # is not present, the database will not persist at all.
      data_dir: ~/.slurmcluster/.postgres

  - master:
      pre:
        - sh: make -C $OPT_PROJECT_ROOT/proto build
        - sh: make -C $OPT_PROJECT_ROOT/master build
        - sh: make -C $OPT_PROJECT_ROOT/tools prep-root
      post:
        - logcheck:
            regex: accepting incoming connections on port
        - sh: >
            curl localhost:8080/api/v1/auth/login -X POST -d '{"username": "admin", "password": ""}' 2>/dev/null | jq -r .token > /tmp/slurmcluster-token
        - sh: >
            curl -H "Authorization: Bearer $(cat /tmp/slurmcluster-token)" -X PATCH -d '{"agent_user_group": {"agent_uid": $OPT_REMOTE_UID, "agent_gid": $OPT_REMOTE_GID, "agent_user": "$OPT_REMOTE_USER", "agent_group": "$OPT_REMOTE_GROUP"}}' localhost:8080/api/v1/users/1
        - sh: >
            curl -H "Authorization: Bearer $(cat /tmp/slurmcluster-token)" -X PATCH -d '{"agent_user_group": {"agent_uid": $OPT_REMOTE_UID, "agent_gid": $OPT_REMOTE_GID, "agent_user": "$OPT_REMOTE_USER", "agent_group": "$OPT_REMOTE_GROUP"}}' localhost:8080/api/v1/users/2
        - sh: >
            echo -e "Determined user list from master '$(hostname):8080'\n" && curl -H "Authorization: Bearer $(cat /tmp/slurmcluster-token)" -X GET localhost:8080/api/v1/users
      cmdline:
        - $OPT_PROJECT_ROOT/master/build/determined-master
        - --config-file
        - :config

      # config_file is just a master.yaml
      config_file:
        port: 8080
        db:
          host: localhost
          port: 5432
          password: postgres
          user: postgres
          name: determined
        checkpoint_storage:
          type: shared_fs
          host_path: /tmp
        log:
          level: debug
        cache:
          cache_dir: /tmp
        task_container_defaults: 
          bind_mounts:
            - host_path: /etc/hosts
              container_path: /etc/hosts
          # Workaround XDG_RUNTIME_DIR not provided by Slurm
          environment_variables:
            - ENROOT_RUNTIME_PATH=/srv/enroot
        resource_pools:

        resource_manager:
          master_host: localhost
          master_port: 8080
          host: localhost
          port: 8081
          protocol: http
          container_run_type: $OPT_CONTAINER_RUN_TYPE
          security:
             tls:
                skip_verify: true
          type: $OPT_WORKLOAD_MANAGER
          singularity_image_root: /srv/singularity
          # job_project_source: project
          # File containing the authorization token for communication with the launcher -- if blank then none.
          # This would typically be a full path where the determined master is running generated by
          # the `dev-keytool token` command.
          # If using devcluster relative to the directory from which it was invoked (typically determined-ee).
          auth_file: $OPT_AUTHFILE
          # When slurm is configured with SelectType=select/cons_tres, setting tres_supported: true
          # allows us to use it is schedule GPUs more easily. For systems without this plugin, set
          # this to false and add contraints to ensure select node have GPUs if desired.
          tres_supported: true
          # gres_supported: $OPT_GRESSUPPORTED
          # rendezvous_network_interface: $OPT_RENDEVOUSIFACE
          # default_aux_resource_pool: $OPT_DEFAULTAUXRESOURCEPOOL
          # default_compute_resource_pool: $OPT_DEFAULTCOMPUTERESOURCEPOOL
          # slot_type: $OPT_SLOTTYPE
          # Specify per-partition overrides for submitted tasks.
          # partition_overrides:
          #   defq:
          #     rendezvous_network_interface: eth0
          #     # Slot type for jobs submitted to the partition. Inferred from the capabilities of
          #     # the partition by default.
          #     slot_type: [cuda,cpu,rocm]
          #     task_container_defaults:
          #       dtrain_network_interface: ib0
          #       force_pull_image: true
        #
        # Launcher-provided resource pools (example).
        # resource_pools:
        #   - pool_name: defq_GPU_tesla
        #     description: Lands jobs on defq_GPU with tesla GPU selected
        #     task_container_defaults:
        #       slurm:
        #         gpu_type: tesla
        #     provider:
        #       type: hpc
        #       partition: defq_GPU
        # This is important: we have to use the symbolic links in the
        # tools/build directory to run properly.
        # The line below allows the resource manager to be removed using a sed command.
        # resource_manager_end
        root: $OPT_PROJECT_ROOT/tools/build

        observability:
          enable_prometheus: true
