# Use the latest 2.1 version of CircleCI pipeline process engine. See: https://circleci.com/docs/2.0/configuration-reference
version: 2.1

orbs:
  win: circleci/windows@5.0.0
  slack: circleci/slack@3.4.2
  kubernetes: circleci/kubernetes@0.11.0
  helm: circleci/helm@1.2.0
  gcloud: circleci/gcp-cli@2.1.0
  queue: eddiewebb/queue@volatile
  codecov: codecov/codecov@3.0.0

executors:
  python-37:
    docker:
      - image: python:3.7-slim-buster
  python-38:
    docker:
      - image: python:3.8-slim-buster
  python-39:
    docker:
      - image: python:3.9-slim-buster

parameters:
  det-version:
    type: string
    default: 0.19.3-dev0
  docker-image:
    type: string
    default: determinedai/cimg-base:latest
  machine-image:
    type: string
    default: ubuntu-2004:202201-02
  gpu-machine-image:
    type: string
    default: ubuntu-2004-cuda-11.2:202103-01
  # Some python, go, and react dependencies are cached by circleci via `save_cache`/`restore_cache`.
  # If the dependencies stay the same, but the circleci code that would produce them is changed,
  # it may be necessary to invalidate the cache by incrementing this value.
  # For example, if you change an env variable affecting a build of a python package with the same version,
  # the old build may be cached, and you may need to invalidate it.
  cache-buster:
    type: string
    default: v1dev12

release-and-rc-filters: &release-and-rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+/
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+/

rc-filters: &rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+/

release-filters: &release-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+/

upstream-feature-branch: &upstream-feature-branch
  branches:
    ignore:
      - /pull\/.*/
      - /release-.*/
      - master

any-upstream: &any-upstream
  branches:
    ignore:
      - /pull\/.*/

commands:
  fix-circle-working-directory:
    description: "Fix CIRCLE_WORKING_DIRECTORY"
    steps:
      - run: echo 'CIRCLE_WORKING_DIRECTORY="${CIRCLE_WORKING_DIRECTORY/#\~/$HOME}"' >> $BASH_ENV

  # circleci's checkout does not fetch submodules by default
  # https://circleci.com/docs/2.0/configuration-reference/#checkout
  checkout-with-sm:
    steps:
      - checkout
      - run: git submodule sync
      - run: git submodule update --init

  skip-if-docs-only:
    steps:
      - run: git remote add upstream https://github.com/determined-ai/determined
      - run: git fetch upstream
      - run:
          name: check for docs-only changes
          command: |
            MERGE_BASE="$(git merge-base upstream/master HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            if [ "$DIFF_DIRS" = "docs" ] ; then
              echo "docs-only change detected, halting job now"
              circleci-agent step halt
            else
              echo "detected non-docs-only changes: $DIFF_DIRS"
            fi

  skip-if-webui-only:
    steps:
      - run: git fetch upstream
      - run:
          name: check for webui-only changes  # must be run after skip-if-docs-only
          command: |
            MERGE_BASE="$(git merge-base upstream/master HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            if [ "$DIFF_DIRS" = "webui" ] ; then
              echo "webui-only change detected, halting job now"
              circleci-agent step halt
            else
              echo "detected non-webui-only changes: $DIFF_DIRS"
            fi

  skip-if-not-shared:
    steps:
      - run: git remote add upstream https://github.com/determined-ai/determined
      - run: git fetch upstream
      - run:
          name: check for shared changes
          command: |
            MERGE_BASE="$(git merge-base upstream/master HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE":webui/react/src HEAD:webui/react/src)"
            if [ "$DIFF_DIRS" = "shared" ] ; then
              echo "shared code changed, running downstream test"
            else
              echo "shared code not affected, halting job now"
              circleci-agent step halt
            fi

  set-slack-user-id:
    steps:
      - run:
          name: Set Slack variables
          command: |
            if ! [ -x "$(command -v jq)" ]; then
              apt update && apt install -y jq
            fi

            AUTHOR_EMAIL="$(git show -s --format='%ae' $CIRCLE_SHA1)"
            echo "export AUTHOR_EMAIL=\"${AUTHOR_EMAIL}\"" >> $BASH_ENV
            LOOKUP_RESPONSE=$(curl -s "https://slack.com/api/users.lookupByEmail?token=${SLACK_API_TOKEN}&email=${AUTHOR_EMAIL}")
            SUCCESS=$(echo "$LOOKUP_RESPONSE" | jq ".ok")
            if [[ "$SUCCESS" == "true" ]]; then
              SLACK_USER_ID=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.id")
              SLACK_NAME=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.name")
              echo "export SLACK_NAME=\"${SLACK_NAME}\"" >> $BASH_ENV
              echo "export SLACK_USER_ID=\"${SLACK_USER_ID}\"" >> $BASH_ENV
            else
              echo "Unable to find Slack user ID for  \"${AUTHOR_EMAIL}\"."
            fi

  pull-task-images:
    parameters:
      tf1:
        type: boolean
        default: false
      tf2:
        type: boolean
        default: false
    steps:
      - when:
          condition: <<parameters.tf1>>
          steps:
            - run: docker pull determinedai/environments:py-3.7-pytorch-1.7-tf-1.15-cpu-9119094
      - when:
          condition: <<parameters.tf2>>
          steps:
            - run: docker pull determinedai/environments:py-3.8-pytorch-1.10-tf-2.8-cpu-9119094

  login-docker:
    parameters:
      repository:
        type: string
        default: ""
      username:
        type: string
      password:
        type: string
    steps:
      - run: echo "<<parameters.password>>" | docker login <<parameters.repository>> -u "<<parameters.username>>" --password-stdin

  login-helm:
    steps:
      - run: helm repo add determined https://helm.ngc.nvidia.com/isv-ngc-partner/determined --username=$NGC_API_USERNAME --password=$NGC_API_KEY

  reinstall-go:
    steps:
      - run: sudo rm -rf /usr/local/go # Remove system go.
      - run: curl https://dl.google.com/go/go1.18.linux-amd64.tar.gz -o /tmp/go.linux-amd64.tar.gz
      - run: sudo tar -C /usr/local -xzf /tmp/go.linux-amd64.tar.gz

  install-protoc:
    steps:
      - run: curl -o /tmp/protoc.zip -L https://github.com/protocolbuffers/protobuf/releases/download/v3.17.1/protoc-3.17.1-linux-x86_64.zip
      - run: unzip /tmp/protoc.zip -d $HOME/.local

  setup-go-intg-deps:
    steps:
      - install-protoc # Install newer version of protoc into $HOME/.local/bin, since default is proto2.
      - run: PATH=$HOME/.local/bin:$PATH make -C proto get-deps
      - run: PATH=$HOME/.local/bin:$PATH make -C proto build
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - install-devcluster
      - start-devcluster:
          target-stage: elastic
          devcluster-config: elastic-base.devcluster.yaml

  go-get-deps:
    steps:
      - install-protoc
      - restore_cache:
          keys:
            - det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "master/go.sum" }}-{{ checksum  "agent/go.sum" }}-{{ checksum  "proto/go.sum" }}
      - run: make -C proto get-deps
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - save_cache:
          key: det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "master/go.sum" }}-{{ checksum  "agent/go.sum" }}-{{ checksum  "proto/go.sum" }}
          paths:
            - "/home/circleci/go/pkg/mod/"
  react-get-deps:
    steps:
      - attach_workspace:
          at: .
      - restore_cache:
          keys:
            - det-react-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "webui/react/package-lock.json" }}
      - run:
          name: Get React dependencies
          command: |
            if [ ! -d "webui/react/node_modules" ]; then
              make -C webui/react get-deps-npm
            fi
      - save_cache:
          key: det-react-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "webui/react/package-lock.json" }}
          paths:
            - "webui/react/node_modules"

  install-wheel:
    parameters:
      package-name:
        type: string
      package-location:
        type: string
    steps:
      - run:
          name: Install <<parameters.package-name>>
          working_directory: <<parameters.package-location>>
          command: |
            make build
            pip install --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>
            pip install --no-deps --force-reinstall --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>

  setup-python-venv:
    description: Set up and create Python venv.
    parameters:
      determined:
        type: boolean
        default: false
      model-hub:
        type: boolean
        default: false
      install-python:
        type: boolean
        default: true
      extras-requires:
        type: string
        default: ""
      extra-requirements-file:
        type: string
        default: ""
      executor:
        type: string
      python-version:
        type: string
        default: "3.7.11"
    steps:
      - when:
          condition: <<parameters.install-python>>
          steps:
            - run:
                name: prepare conda PATH
                command: |
                  echo 'export PATH=/opt/conda/bin:${PATH}' >> $BASH_ENV

            - run:
                name: prepare conda install_python.sh
                command: |
                  cat \<<'EOF' > /tmp/install_python.sh
                  #!/bin/bash
                  PYTHON_VERSION="${1}"

                  CONDA_DIR="/opt/conda"
                  CONDA_INSTALLER="Miniconda3-py39_4.10.3-Linux-x86_64.sh"
                  CONDA_MD5="8c69f65a4ae27fb41df0fe552b4a8a3b"
                  CONDA_URL="https://repo.anaconda.com/miniconda"

                  mkdir -p /etc/determined/conda.d
                  mkdir -p "${CONDA_DIR}"

                  cd /tmp
                  curl --retry 3 -fsSL -O "${CONDA_URL}/${CONDA_INSTALLER}"
                  echo "${CONDA_MD5}  ${CONDA_INSTALLER}" | md5sum -c -
                  bash "./${CONDA_INSTALLER}" -u -b -p "${CONDA_DIR}"
                  rm -f "./${CONDA_INSTALLER}"

                  ${CONDA_DIR}/bin/conda install python=${PYTHON_VERSION}
                  ${CONDA_DIR}/bin/conda update --prefix ${CONDA_DIR} --all -y
                  ${CONDA_DIR}/bin/conda clean --all -y
                  EOF

            - run:
                name: run conda install_python.sh
                command: |
                  sudo bash /tmp/install_python.sh <<parameters.python-version>>

            - run:
                name: Setup venv
                command: |
                  python3 -m venv /tmp/venv
                  echo 'export PATH=/tmp/venv/bin:$PATH' >> $BASH_ENV
                  /tmp/venv/bin/python -m pip install --upgrade pip wheel setuptools

      - run:
          name: Write cache key
          command: |
            echo <<parameters.executor>> > /tmp/cachefile
            pip freeze --all >> /tmp/cachefile
            if [ "<<parameters.determined>>" = "true" ]; then
              cat harness/setup.py >> /tmp/cachefile
            fi
            if [ "<<parameters.model-hub>>" = "true" ]; then
              cat model_hub/setup.py >> /tmp/cachefile
            fi
            echo <<parameters.extras-requires>> >> /tmp/cachefile
            if [ -n <<parameters.extra-requirements-file>> ]; then
              cat <<parameters.extra-requirements-file>> >> /tmp/cachefile
            fi

      - restore_cache:
          keys:
            - det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
      - when:
          condition: <<parameters.determined>>
          steps:
            - install-wheel:
                package-name: determined
                package-location: ~/project/harness
      - when:
          condition: <<parameters.model-hub>>
          steps:
            - run:
                name: Install mmdetection dependencies
                command: |
                  sudo apt-get update
                  sudo apt-get install -y ffmpeg libsm6 libxext6
            - install-wheel:
                package-name: model-hub
                package-location: ~/project/model_hub
      - run:
          name: Install <<parameters.extras-requires>>
          command: |
            if [ -n "<<parameters.extras-requires>>" ]; then
              pip install --progress-bar off <<parameters.extras-requires>>
            fi
      - run:
          name: Install <<parameters.extra-requirements-file>>
          command: |
            if [ -n "<<parameters.extra-requirements-file>>" ]; then
              pip install -r <<parameters.extra-requirements-file>>
            fi
      - save_cache:
          key: det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
          paths:
            - "/tmp/venv"
      - run: python --version
      - run: pip --version
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"

  run-e2e-tests:
    parameters:
      mark:
        type: string
        default: ""
      junit-path:
        type: string
        default: "/tmp/test-results/e2e/tests.xml"
      master-scheme:
        type: string
        default: "http"
      master-host:
        type: string
        default: "localhost"
      master-port:
        type: string
        default: "8080"
      master-cert:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      managed-devcluster:
        type: boolean
        default: false
      extra-pytest-flags:
        type: string
        default: ""
    steps:
      - run:
          name: Split tests
          working_directory: ~/project/e2e_tests
          command: |
            # If a test mark is specified, preselect only files that contain tests with that mark to
            # minimize how wrong CircleCI's splitting can get things.
            if [ -n "<<parameters.mark>>" ]; then
              mark="<<parameters.mark>>"
              # Extract the first mark for complex conditions such as 'mark1 and not mark2'.
              mark_prefix=${mark%% and*}
              find tests -name 'test*.py' -print0 | xargs -0 grep -rl "pytest\.mark\.$mark_prefix"
            else
              circleci tests glob 'tests/**/test*.py'
            fi | circleci tests split --split-by=timings > /tmp/tests-to-run
            echo "Running tests from these files:"
            sed 's/^/- /' </tmp/tests-to-run

      - unless:
          condition: <<parameters.managed-devcluster>>
          steps:
            - wait-for-master:
                scheme: <<parameters.master-scheme>>
                host: <<parameters.master-host>>
                port: <<parameters.master-port>>

      - run:
          name: Run e2e tests
          working_directory: ~/project/e2e_tests
          no_output_timeout: 30m
          command: |
            DET_MASTER_CERT_FILE=<<parameters.master-cert>> DET_MASTER_CERT_NAME=<<parameters.master-cert-name>> \
            pytest -vv -s \
            -m '<<parameters.mark>>' \
            --durations=0 \
            --master-scheme="<<parameters.master-scheme>>" \
            --master-host="<<parameters.master-host>>" \
            --master-port="<<parameters.master-port>>" \
            -o junit_family=xunit1 \
            --junit-xml="<<parameters.junit-path>>" \
            <<parameters.extra-pytest-flags>> \
            $(< /tmp/tests-to-run)
      - upload-test-job:
          only_on_branch: master
          test_results_path: <<parameters.junit-path>>

  run-det-deploy-tests:
    parameters:
      mark:
        type: string
        default: ""
      det-version:
        type: string
        default: ""
    steps:
      - run:
          name: Run det-deploy tests
          working_directory: ~/project/e2e_tests
          command: |
            pytest -vv -s \
            -m <<parameters.mark>> \
            --det-version="<<parameters.det-version>>"

  deploy-aws-cluster:
    parameters:
      cluster-id:
        type: string
      det-version:
        type: string
      keypair:
        type: string
        default: "integrations-test"
      enable-cors:
        type: boolean
        default: false
      master-tls-cert:
        type: string
        default: ""
      master-tls-key:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      compute-agent-instance-type:
        type: string
        default: "p2.xlarge"
      max-dynamic-agents:
        type: integer
        default: 1
      retain-log-group:
        type: boolean
        default: false
      log-group-prefix:
        type: string
        default: ""
      reattach-enabled:
        type: boolean
        default: false
    steps:
      - run:
          name: Initialize extra arguments
          command: touch /tmp/det-deploy-extra-args
      - when:
          condition:
            equal:
               [ true, << parameters.enable-cors >> ]
          steps:
          - run:
              name: Enable CORS
              command: 'echo --enable-cors >> /tmp/det-deploy-extra-args'

      - run:
          name: Configure TLS arguments
          command: |
            if [ -n "<<parameters.master-tls-cert>>" ]; then echo "--master-tls-cert <<parameters.master-tls-cert>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-tls-key>>" ]; then echo "--master-tls-key <<parameters.master-tls-key>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-cert-name>>" ]; then echo "--master-cert-name <<parameters.master-cert-name>>" >> /tmp/det-deploy-extra-args; fi
      - run:
          name: Configure log group arguments
          command: |
            if <<parameters.retain-log-group>>; then echo "--retain-log-group" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.log-group-prefix>>" ]; then echo "--log-group-prefix <<parameters.log-group-prefix>>" >> /tmp/det-deploy-extra-args; fi
      - when:
          condition:
            equal:
              [ true, <<parameters.reattach-enabled>> ]
          steps:
          - run:
              name: set reattach-enabled
              command: echo --agent-reattach-enabled true --agent-reconnect-attempts 24 --no-update-terminate-agents >> /tmp/det-deploy-extra-args
      - run:
          name: Deploy AWS cluster
          command: |
            echo "-----BEGIN ARGS-----"
            cat /tmp/det-deploy-extra-args
            echo "-----END ARGS-----"
            det deploy aws up \
              $(< /tmp/det-deploy-extra-args) \
              --cluster-id <<parameters.cluster-id>> \
              --det-version <<parameters.det-version>> \
              --aux-agent-instance-type <<parameters.aux-agent-instance-type>> \
              --compute-agent-instance-type <<parameters.compute-agent-instance-type>> \
              --max-dynamic-agents <<parameters.max-dynamic-agents>> \
              --keypair <<parameters.keypair>> \
              --yes

  terminate-aws-cluster:
    parameters:
      cluster-id:
        type: string
    steps:
      - run:
          name: Terminate AWS Cluster
          when: always
          command: |
            det deploy aws down \
              --cluster-id <<parameters.cluster-id>> --yes

  setup-aws-cluster:
    parameters:
      cluster-id:
        type: string
      det-version:
        type: string
      compute-agent-instance-type:
        type: string
        default: "p2.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      max-dynamic-agents:
        type: integer
        default: 1
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
      master-cert-name:
        type: string
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - deploy-aws-cluster:
          cluster-id: ${CLUSTER_ID}
          det-version: <<parameters.det-version>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>
          master-cert-name: <<parameters.master-cert-name>>
          log-group-prefix: determined-ci
          retain-log-group: true
      - set-master-address-aws:
          cluster-id: ${CLUSTER_ID}
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>

  terminate-gke-cluster:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
    steps:
      # Use a run instead of `gke/helm` orbs because circle CI orbs do not support `when`.
      - run:
          name: Delete Helm release
          when: always
          command: helm delete ci
      - run:
          name: Terminate GKE Cluster
          when: always
          command: |
            gcloud container clusters delete <<parameters.cluster-id>> --quiet --region=<<parameters.region>>

  setup-gke-cluster:
    parameters:
      cluster-id:
        type: string
      det-version:
        type: string
      gke-version:
        type: string
      machine-type:
        type: string
      num-machines:
        type: integer
      gpu-type:
        type: string
      gpus-per-machine:
        type: integer
      slot-type:
        type: string
      slot-resource-requests-cpu:
        type: integer
      region:
        type: string
      node-locations:
        type: string
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - gcloud/install:
          version: "319.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: gcloud container clusters create ${CLUSTER_ID} --machine-type=n1-standard-8 --cluster-version=<<parameters.gke-version>> --region=<<parameters.region>> --node-locations=<<parameters.node-locations>> --scopes storage-rw,cloud-platform --num-nodes 1 --labels environment=ci --no-enable-ip-alias --subnetwork default
          name: Create GKE cluster
          no_output_timeout: 30m
      - run:
          command: gcloud container clusters get-credentials ${CLUSTER_ID} --project determined-ai --region <<parameters.region>>
          name: Get Kubeconfig
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
          - run:
              command: kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
              name: Install NVIDIA drivers
          - helm/install-helm-chart:
              chart: helm/charts/determined
              helm-version: v3.2.4
              namespace: "default"
              wait: true
              release-name: "ci"
              values-to-override: |
                detVersion=<<parameters.det-version>>,\
                maxSlotsPerPod=<<parameters.gpus-per-machine>>,\
                checkpointStorage.type=gcs,\
                checkpointStorage.bucket=det-ci
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
          - helm/install-helm-chart:
              chart: helm/charts/determined
              helm-version: v3.2.4
              namespace: "default"
              wait: true
              release-name: "ci"
              values-to-override: |
                detVersion=<<parameters.det-version>>,\
                maxSlotsPerPod=1,\
                checkpointStorage.type=gcs,\
                checkpointStorage.bucket=det-ci,\
                slotType=<<parameters.slot-type>>,\
                slotResourceRequests.cpu=<<parameters.slot-resource-requests-cpu>>
      - set-master-address-gke:
          release-name: "ci"
          namespace: "default"
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
          - run:
              command: gcloud container node-pools create accel --cluster ${CLUSTER_ID} --region <<parameters.region>> --num-nodes <<parameters.num-machines>> --accelerator type=<<parameters.gpu-type>>,count=<<parameters.gpus-per-machine>> --machine-type=<<parameters.machine-type>> --scopes cloud-platform
              name: Create GPU node pool
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
          - run:
              command: gcloud container node-pools create accel --cluster ${CLUSTER_ID} --region <<parameters.region>> --num-nodes <<parameters.num-machines>> --machine-type=<<parameters.machine-type>> --scopes cloud-platform
              name: Create CPU node pool

  setup-gke-cluster-cuda-11:
    parameters:
      cluster-id:
        type: string
      det-version:
        type: string
      gke-version:
        type: string
      machine-type:
        type: string
      num-machines:
        type: integer
      gpu-type:
        type: string
      gpus-per-machine:
        type: integer
      region:
        type: string
      node-locations:
        type: string
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
      environment-image:
        default: determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-0.19.1
        type: string
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - gcloud/install:
          version: "319.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: gcloud container clusters create ${CLUSTER_ID} --machine-type=n1-standard-8 --cluster-version=<<parameters.gke-version>> --region=<<parameters.region>> --node-locations=<<parameters.node-locations>> --scopes storage-rw,cloud-platform --num-nodes 1 --no-enable-ip-alias --subnetwork default
          name: Create GKE cluster
          no_output_timeout: 30m
      - run:
          command: gcloud container node-pools create accel --cluster ${CLUSTER_ID} --region <<parameters.region>> --num-nodes <<parameters.num-machines>> --accelerator type=<<parameters.gpu-type>>,count=<<parameters.gpus-per-machine>> --machine-type=<<parameters.machine-type>> --scopes cloud-platform
          name: Create GPU node pool
      - run:
          command: gcloud container clusters get-credentials ${CLUSTER_ID} --project determined-ai --region <<parameters.region>>
          name: Get Kubeconfig
      - run:
          command: kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
          name: Install NVIDIA drivers
      - run:
          command: kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user "$(gcloud config get-value account)"
          name: Setup cluster role binding
      - helm/install-helm-chart:
          chart: helm/charts/determined
          helm-version: v3.2.4
          namespace: "default"
          wait: true
          release-name: "ci"
          values-to-override: |
            detVersion=<<parameters.det-version>>,\
            maxSlotsPerPod=<<parameters.gpus-per-machine>>,\
            taskContainerDefaults.cpuImage=<<parameters.environment-image>>,\
            taskContainerDefaults.gpuImage=<<parameters.environment-image>>,\
            checkpointStorage.type=gcs,\
            checkpointStorage.bucket=det-ci
      - set-master-address-gke:
          release-name: "ci"
          namespace: "default"

  generate-tls-cert:
    steps:
      - run: |
          .circleci/scripts/generate_cert.sh /tmp/master
          echo 'export MASTER_TLS_CERT=/tmp/master.crt MASTER_TLS_KEY=/tmp/master.key MASTER_CERT_NAME=determined-master-ci' >> $BASH_ENV

  set-master-address-aws:
    parameters:
      cluster-id:
        type: string
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
    steps:
      - run: |
          MASTER_HOST=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> DeterminedAddress)
          echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV

      - run: |
          if [ -n "<<parameters.master-tls-cert>>" ] && [ -n "<<parameters.master-tls-key>>" ]; then
            echo "export MASTER_PORT=8443" >> $BASH_ENV
            echo "export MASTER_SCHEME=https" >> $BASH_ENV
          fi

  set-master-address-gke:
    parameters:
      release-name:
        type: string
      namespace:
        type: string
    steps:
      - run:
          name: Set Master Address
          command: |
            MASTER_HOST=$(kubectl get -n <<parameters.namespace>>  service determined-master-service-<<parameters.release-name>> \
            --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
            echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV
            echo "${MASTER_HOST}"

  set-google-application-credentials:
    steps:
      - run:
          name: Set Google Application Credentials
          command: |
            GOOGLE_APPLICATION_CREDENTIALS=${HOME}/gcloud-service-key.json
            echo "export GOOGLE_APPLICATION_CREDENTIALS=\"${GOOGLE_APPLICATION_CREDENTIALS}\"" >> $BASH_ENV

  set-cluster-id:
    parameters:
      cluster-id:
        type: string
    steps:
      - run: echo "export CLUSTER_ID=\"<<parameters.cluster-id>>\"" >> $BASH_ENV

  wait-for-master:
    parameters:
      scheme:
        type: string
        default: "http"
      host:
        type: string
      port:
        type: string
        default: "8080"
    steps:
      - run: python .circleci/scripts/wait_for_master.py <<parameters.scheme>>://<<parameters.host>>:<<parameters.port>>

  upload-test-job:
    parameters:
      only_on_branch:
        type: string
        default: ""
      test_results_path:
        type: string
        default: ""
    steps:
      - when:
          condition:
            equal: [ <<parameters.only_on_branch>>, << pipeline.git.branch >> ]
          steps:
            - run:
                name: Test run success -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'success' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_success
            - run:
                name: Test run failed -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'failed' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_fail

  locate-cloudwatch-logs:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
        default: us-west-2
    steps:
      - run:
          name: Locate CloudWatch logs
          when: always
          command: |
            LOG_GROUP=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> LogGroup)
            echo "Cluster logs can be found in CloudWatch under the log group $LOG_GROUP"
            echo "or the URL below (if the log group is in your default region):"
            ENC_LOG_GROUP=$(echo "$LOG_GROUP" | sed 's|/|$252F|g')
            echo "https://console.aws.amazon.com/cloudwatch/home#logsV2:log-groups/log-group/$ENC_LOG_GROUP"

  pre-package-and-push-system:
    steps:
      - go-get-deps
      - run: make -C proto build
      - run: make -C master check
      - run: make -C agent check

  install-devcluster:
    steps:
      - run: pip install git+https://github.com/determined-ai/devcluster.git@v1.1.0#egg=devcluster
      - run:
          command: |
            if ! [ -x "$(command -v socat)" ]; then
              apt update && apt install -y socat
            fi

  start-devcluster:
    parameters:
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
    steps:
      - run:
          command: devcluster --oneshot -c .circleci/devcluster/<<parameters.devcluster-config>> --target-stage <<parameters.target-stage>>
          background: true

jobs:
  build-helm:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - attach_workspace:
          at: .
      - run: make -C helm build
      - persist_to_workspace:
          root: .
          paths:
            - helm/build
      - store_artifacts:
          path: helm/build

  build-docs:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.4.3 torch==1.9.0 torchvision==0.10.0"
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C examples build
      - run: make -C model_hub examples
      - run: make -C helm build
      - attach_workspace:
          at: .
      - run: make -C docs build
      - persist_to_workspace:
          root: .
          paths:
            - examples/build
            - cli/dist
            - common/dist
            - harness/dist
            - model_hub/dist
            - docs/site/html
      - store_artifacts:
          path: docs/site/html

  publish-docs:
    docker:
      - image: hashicorp/terraform:light
    steps:
      - checkout
      - attach_workspace:
          at: .
      - run: apk add make curl python3 py3-pip
      - run: pip3 install awscli
      - run: make -C docs publish

  package-and-push-system-local:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: 19.03.12
      - pre-package-and-push-system
      - run: make package
      - run: mkdir -p build/
      - run: docker save -o build/master.image determinedai/determined-master:${CIRCLE_SHA1}
      - run: docker save -o build/agent.image determinedai/determined-agent:${CIRCLE_SHA1}
      - persist_to_workspace:
          root: .
          paths:
            - "master/dist/*linux_amd64.deb"
            - "master/dist/*linux_amd64.rpm"
            - "agent/dist/*linux_amd64.deb"
            - "agent/dist/*linux_amd64.rpm"
            - "build/*.image"

  package-and-push-system-dev:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    resource_class: medium+
    steps:
      - checkout
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: 19.03.12
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system
      - run: make package
      - run: make -C master publish-dev
      - run: make -C agent publish-dev
      - run:
          name: Build and publish model_hub docker images
          command: |
            if [ ${CIRCLE_BRANCH} = 'master' ] || [[ ${CIRCLE_BRANCH} == *"release-"* ]]; then
                # For master and release branches, we will tag and publish both the environment
                # with the git hash as well as the version.  This will make that image available
                # immediately for nightly tests.
                make -C model_hub build-docker
                make -C model_hub publish-docker
            else
                # Otherwise, only tag and publish the environment with the git hash.
                make -C model_hub build-docker-dev
                make -C model_hub publish-docker-dev
            fi
      - store_artifacts:
          path: master/dist/
      - store_artifacts:
          path: agent/dist/

  package-and-push-system-rc:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    resource_class: medium+
    steps:
      - checkout
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: 19.03.12
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - login-docker:
          repository: nvcr.io
          username: ${NGC_API_USERNAME}
          password: ${NGC_API_KEY}
      - pre-package-and-push-system
      - run: make package
      - run: make -C master publish
      - run: make -C agent publish
      - run: make -C model_hub build-docker
      - run: make -C model_hub publish-docker
      - store_artifacts:
          path: master/dist/
      - store_artifacts:
          path: agent/dist/

  package-and-push-system-release:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    resource_class: medium+
    steps:
      - checkout
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: 19.03.12
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - login-docker:
          repository: nvcr.io
          username: ${NGC_API_USERNAME}
          password: ${NGC_API_KEY}
      - pre-package-and-push-system
      - run: make -C master release
      - run: make -C agent release
      - run: make -C model_hub build-docker
      - run: make -C model_hub publish-docker
      - store_artifacts:
          path: master/dist/
      - store_artifacts:
          path: agent/dist/

  publish-helm:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - helm/install-helm-client
      - run: helm plugin install https://github.com/chartmuseum/helm-push.git
      - login-helm
      - run: make -C helm release

  publish-python-package:
    parameters:
      path:
        type: string
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "twine"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C <<parameters.path>> build
      - run: make -C <<parameters.path>> publish

  check-ts-bindings:
    docker:
      - image: cimg/openjdk:14.0.1
    steps:
      - checkout
      - skip-if-docs-only
      - attach_workspace:
          at: .
      - run: make -C bindings get-deps
      - run: make -C bindings check/typescript-fetch

  check-py-bindings:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C bindings force-gen
      - run: make -C bindings check/python

  upload-try-now-template:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "awscli"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C harness upload-try-now-template

  test-debian-packaging:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - run: sudo apt-get install -y $(pwd)/master/dist/determined-master*.deb
      - run: sudo apt-get install -y $(pwd)/agent/dist/determined-agent*.deb
      - run: sudo cp .circleci/packaging/master.yaml /etc/determined/master.yaml
      - run: sudo cp .circleci/packaging/agent.yaml /etc/determined/agent.yaml
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - install-devcluster
      - start-devcluster:
          target-stage: db
      - run: python3 .circleci/scripts/wait_for_server.py localhost 5432
      - run: sudo systemctl restart determined-master
      - run: python3 .circleci/scripts/wait_for_server.py localhost 8080
      - run: sudo systemctl restart determined-agent
      - run: ./.circleci/scripts/sanity.sh

  lint-react:
    docker:
      - image: cimg/node:16.14.2
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - react-get-deps
      - run: make -C webui/react check

  build-react:
    parameters:
      dev-mode:
        type: boolean
        default: false
    docker:
      - image: cimg/node:16.14.2
    resource_class: large
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - react-get-deps
      - run: |
            if <<parameters.dev-mode>>; then
              echo 'Setting development mode...'
              export DET_NODE_ENV=development
            fi
            make -C webui/react build
      - persist_to_workspace:
          root: .
          paths:
            - webui/react/build

  test-unit-react:
    docker:
      - image: cimg/node:16.14.2
        environment:
          CI: "true"
    resource_class: xlarge
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - react-get-deps
      - run: make -C webui/react test-ci
      - codecov/upload:
          flags: "web"
          xtra_args: "-v"
      - store_test_results:
          path: webui/react/coverage
      - store_artifacts:
          path: webui/react/coverage/lcov-report

  test-intg-downstream:
    docker:
      - image: cimg/node:16.14
    steps:
      - checkout-with-sm
      - skip-if-not-shared
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - run: make -C webui/react test-shared

  build-storybook:
    docker:
      - image: cimg/node:16.14.2
    resource_class: medium+
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - react-get-deps
      - run: make -C webui/react build-storybook
      - store_artifacts:
          path: webui/react/build-storybook

  lint-go:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    resource_class: large
    steps:
      - checkout
      - skip-if-docs-only
      - go-get-deps
      - run: sudo apt-get update && sudo apt-get install -y clang-format
      - run: make -C proto build
      - run: make -C proto check
      - run: make -C master check
      - run: make -C agent check

  build-go:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - skip-if-docs-only
      - go-get-deps
      - run: make -C proto build
      - run: make -C master build
      - run: make -C agent build
      - persist_to_workspace:
          root: .
          paths:
            - "master/build"
            - "agent/build"

  build-proto:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - go-get-deps
      - run: make -C proto build
      - persist_to_workspace:
          root: .
          paths:
            - "proto/build/**/*"

  test-unit-go:
    docker:
      - image: cimg/go:1.18
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - skip-if-docs-only
      - go-get-deps
      - run: make -C proto build
      - run: make -C master test
      - run: make -C agent test
      - codecov/upload:
          flags: "backend"
          xtra_args: "-v"

  test-intg-master:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    steps:
      - checkout
      - skip-if-docs-only
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: make -C master test-intg
      - persist_to_workspace:
          root: .
          paths:
            - master/coverage.out

  test-intg-agent:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    steps:
      - checkout
      - skip-if-docs-only
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: make -C agent test-intg
      - persist_to_workspace:
          root: .
          paths:
            - agent/coverage.out

  go-coverage:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - attach_workspace:
          at: .
      - reinstall-go
      - run: cd master && go tool cover -func coverage.out
      - run: cd master && go tool cover -html coverage.out -o coverage.html
      - run: cd agent && go tool cover -func coverage.out
      - run: cd agent && go tool cover -html coverage.out -o coverage.html
      - run: mkdir go-coverage
      - run: mv master/coverage.html go-coverage/master-coverage.html
      - run: mv agent/coverage.html go-coverage/agent-coverage.html
      - store_artifacts:
          path: go-coverage
          destination: go-coverage

  lint-docs:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C docs check

  lint-secrets:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - run: git clone https://github.com/awslabs/git-secrets /tmp/git-secrets
      - run: cd /tmp/git-secrets && sudo make install
      - checkout
      - run: git secrets --install
      - run: git secrets --register-aws
      - run: git secrets --add '"private_key":\s"-----BEGIN\sPRIVATE\sKEY-----'
      - run: git secrets --scan-history

  lint-python:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extras-requires: "torch==1.9.0"
          extra-requirements-file: "requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C cli check
      - run: make -C common check
      - run: make -C harness check
      - run: make -C deploy check
      - run: make -C model_hub check
      - run: make -C e2e_tests check
      - run: make -C examples check
      - run: make -C tools check
      - run: make -C schemas check

  test-unit-harness-cpu:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.4.3 torch==1.9.0 torchvision==0.10.0 codecov"
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-cpu-pycov make -C harness test-cpu
      - run: coverage xml -i --data-file=./test-unit-harness-cpu-pycov
      - run: codecov -v -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-cpu-pycov

  test-unit-harness-gpu:
    machine:
      resource_class: gpu.nvidia.small
      image: <<pipeline.parameters.gpu-machine-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          determined: true
          extras-requires: "tensorflow==2.4.3 torch==1.9.0 torchvision==0.10.0 codecov"
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-gpu-pycov make -C harness test-gpu
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-pycov
      - run: codecov -v -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-pycov

  test-unit-harness-tf2:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.4.3 torch==1.9.0 codecov"
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-tf2-pycov make -C harness test-tf2
      - run: coverage xml -i --data-file=./test-unit-harness-tf2-pycov
      - run: codecov -v -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-tf2-pycov

  test-unit-storage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.4.3 torch==1.9.0 codecov"
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-storage-pycov coverage run -m pytest -v --durations=0 --require-secrets -m cloud harness/tests
      - run: coverage xml -i --data-file=./test-unit-storage-pycov
      - run: codecov -v -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-storage-pycov

  test-unit-model-hub:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extras-requires: "torch==1.9.0 torchvision==0.10.0 codecov"
          extra-requirements-file: "model_hub/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-model-hub-pycov make -C model_hub test
      - run: coverage xml -i --data-file=./test-model-hub-pycov
      - run: codecov -v -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-model-hub-pycov

  python-coverage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          determined: false
          model-hub: false
          extras-requires: "coverage"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: coverage combine *-pycov
      - run: coverage report --include 'harness/determined/*' --skip-covered
      - run: coverage report --include 'model_hub/model_hub/*' --skip-covered
      - run: coverage html --include 'harness/determined/*' --skip-covered -d cov-html/harness
      - run: coverage html --include 'model_hub/model_hub/*' --skip-covered -d cov-html/model_hub
      - store_artifacts:
          path: cov-html
          destination: cov-html

  test-examples:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "examples/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: pytest -vv -s --junit-xml="/tmp/test-results/examples.xml" --durations=0 examples/tests
      - upload-test-job:
          only_on_branch: master
          test_results_path: /tmp/test-results/examples.xml

  test-cli:
    parameters:
      executor-name:
        type: string
    executor: << parameters.executor-name >>
    steps:
      - checkout
      - run: python --version
      - run: pip install --upgrade --user pip
      - run: pip --version
      - run: pip install wheel
      - run: cd harness; python setup.py bdist_wheel -d ../build
      - run: pip install --find-links build determined==<< pipeline.parameters.det-version >>
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"
      # Ensure Determined cli can run without installing cli test requirements
      - run: det --help
      - run: pip install setuptools_scm
      - run: pip install -r harness/tests/requirements/requirements-cli.txt
      - run: pip freeze --all
      - run: sh -c "pip check || true"
      - run: pytest harness/tests/cli

  test-e2e:
    parameters:
      tf1:
        type: boolean
        default: false
      tf2:
        type: boolean
        default: false
      mark:
        type: string
      parallelism:
        type: integer
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
      managed-devcluster:
        type: boolean
        default: false
      postgres-version:
        type: string
        default: '10'
      agent-version:
        type: string
        default: ''
      extra-pytest-flags:
        type: string
        default: ""
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    environment:
      DET_POSTGRES_VERSION: <<parameters.postgres-version>>
      DET_AGENT_VERSION: <<parameters.agent-version>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-webui-only
      - attach_workspace:
          at: .

      - reinstall-go

      - setup-python-venv:
          determined: True
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - run:
          name: Get master dependencies
          command: make -C master get-deps

      - install-devcluster
      - unless:
          condition: <<parameters.managed-devcluster>>
          steps:
            - start-devcluster:
                devcluster-config: <<parameters.devcluster-config>>
                target-stage: <<parameters.target-stage>>

      - pull-task-images:
          tf1: <<parameters.tf1>>
          tf2: <<parameters.tf2>>

      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: localhost
          managed-devcluster: <<parameters.managed-devcluster>>
          extra-pytest-flags: <<parameters.extra-pytest-flags>>

      - store_test_results:
          path: /tmp/test-results/

      - when:
          condition: <<parameters.managed-devcluster>>
          steps:
            - store_artifacts:
                path: /tmp/devcluster/
                destination: devcluster-logs
            - store_artifacts:
                path: /tmp/priority_scheduler
                destination: devcluster-priority_scheduler-logs

  deploy:
    parameters:
      compute-agent-instance-type:
        type: string
        default: "p2.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      cluster-id:
        type: string
        default: determined-${CIRCLE_BRANCH////--}
      max-dynamic-agents:
        type: integer
        default: 1
      enable-cors:
        type: boolean
        default: false
      reattach-enabled:
        type: boolean
        default: false
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - queue/until_front_of_line:
          only-on-branch: master
          # Basically wait forever -- we would prefer not to fail deploys and
          # we'll likely never be this backed up.
          time: '10000'
      - checkout
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          executor: <<pipeline.parameters.docker-image>>
      - deploy-aws-cluster:
          cluster-id: <<parameters.cluster-id>>
          det-version: ${CIRCLE_SHA1}
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          enable-cors: <<parameters.enable-cors>>
          reattach-enabled: <<parameters.reattach-enabled>>
      - slack/status:
          fail_only: true
          failure_message: ':thisisfine: A \`${CIRCLE_JOB}\` job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: "${SLACK_USER_ID}"

  test-e2e-aws:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      compute-agent-instance-type:
        type: string
        default: p2.xlarge
      aux-agent-instance-type:
        type: string
        default: m5.large
      max-dynamic-agents:
        type: integer
        default: 1
      parallelism:
        type: integer
        default: 1
      enable-tls:
        type: boolean
        default: false
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      environment-gpu-enabled:
        type: string
        default: "1"
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - when:
          condition: <<parameters.enable-tls>>
          steps:
            - generate-tls-cert
      - setup-aws-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          det-version: ${CIRCLE_SHA1}
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
          master-scheme: ${MASTER_SCHEME:-http}
          master-port: ${MASTER_PORT:-8080}
          master-cert: ${MASTER_TLS_CERT}
          master-cert-name: ${MASTER_CERT_NAME}
      - locate-cloudwatch-logs:
          cluster-id: ${CLUSTER_ID}
      - terminate-aws-cluster:
          cluster-id: ${CLUSTER_ID}
      - store_test_results:
          path: /tmp/test-results/
      - slack/status:
          fail_only: True
          only_for_branches: master
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E GPU job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>


  test-e2e-gke:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      gke-version:
        type: string
        default: "1.21.12"
      machine-type:
        type: string
        default: "n1-standard-8"
      num-machines:
        type: integer
        default: 1
      gpu-type:
        type: string
        default: "nvidia-tesla-k80"
      gpus-per-machine:
        type: integer
        default: 1
      slot-type:
        type: string
        default: gpu
      slot-resource-requests-cpu:
        type: integer
        default: 0
      region:
        type: string
        default: "us-west1"
      node-locations:
        type: string
        default: "us-west1-b"
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      environment-gpu-enabled:
        type: string
        default: "1"
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - setup-gke-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          det-version: ${CIRCLE_SHA1}
          gke-version: <<parameters.gke-version>>
          machine-type: <<parameters.machine-type>>
          num-machines: <<parameters.num-machines>>
          gpu-type: <<parameters.gpu-type>>
          gpus-per-machine: <<parameters.gpus-per-machine>>
          slot-type: <<parameters.slot-type>>
          slot-resource-requests-cpu: <<parameters.slot-resource-requests-cpu>>
          region: <<parameters.region>>
          node-locations: <<parameters.node-locations>>
      - set-google-application-credentials
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
      - terminate-gke-cluster:
          cluster-id: ${CLUSTER_ID}
          region: <<parameters.region>>
      - slack/status:
          fail_only: True
          only_for_branches: master
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E GKE GPU job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>

  test-e2e-gke-cuda-11:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      gke-version:
        type: string
        default: "1.21.12"
      machine-type:
        type: string
        default: "n1-standard-8"
      num-machines:
        type: integer
        default: 1
      gpu-type:
        type: string
        default: "nvidia-tesla-k80"
      gpus-per-machine:
        type: integer
        default: 1
      region:
        type: string
        default: "us-west1"
      node-locations:
        type: string
        default: "us-west1-b"
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      environment-image:
        default: determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-0.19.1
        type: string
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    environment:
      # Using the TF2-only image here is a hack to get more test coverage since TF1 is still the
      # default system-wide. TF1 tests that fail in this configuration should be skipped when the
      # "CUDA" environment variable is set to 11.
      # TODO: (DET-4918): we should ensure a consistent way to have tests that can run against
      # both major TensorFlow versions do so regularly (but not others).
      TF1_GPU_IMAGE: <<parameters.environment-image>>
      TF2_GPU_IMAGE: <<parameters.environment-image>>
      CUDA: 11
    steps:
      - checkout
      - skip-if-docs-only
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - setup-gke-cluster-cuda-11:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          det-version: ${CIRCLE_SHA1}
          gke-version: <<parameters.gke-version>>
          machine-type: <<parameters.machine-type>>
          num-machines: <<parameters.num-machines>>
          gpu-type: <<parameters.gpu-type>>
          gpus-per-machine: <<parameters.gpus-per-machine>>
          region: <<parameters.region>>
          node-locations: <<parameters.node-locations>>
          environment-image: <<parameters.environment-image>>
      - set-google-application-credentials
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
      - terminate-gke-cluster:
          cluster-id: ${CLUSTER_ID}
          region: <<parameters.region>>
      - slack/status:
          fail_only: True
          only_for_branches: master
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E GKE GPU job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>

  test-det-deploy:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - run: docker load --input build/master.image
      - run: docker load --input build/agent.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - pull-task-images:
          tf1: True

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  test-stress:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - run: docker load --input build/master.image
      - run: docker load --input build/agent.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - pull-task-images:
          tf1: True

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  scan-docker-images:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout
      - set-slack-user-id
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
          extras-requires: anchorecli
      - run: sudo snap install yq && sudo snap install jq
      - run: (cd tools/scripts; ./scan-docker-images.sh bumpenvs.yaml)
      - slack/status:
          fail_only: True
          only_for_branches: master
          failure_message: ':thisisfine: A docker image scan on branch \`master\` has failed!'
          channel: "infra-eng-team"

workflows:
  lint:
    jobs:
      - build-proto
      - check-py-bindings:
          requires:
            - build-proto
      - check-ts-bindings:
          requires:
            - build-proto
      - lint-docs
      - lint-python
      - lint-go
      - lint-react
      - lint-secrets

  test-cli:
    jobs:
      - test-cli:
          matrix:
            parameters:
              executor-name:
                [
                  "python-37",
                  "python-38",
                  "python-39",
                  "win/default",
                ]

  test-unit:
    jobs:
      - test-unit-go
      - test-unit-react
      - test-unit-harness-cpu
      # Exclude gpu tests for now since we do not have access to those instance types on circleci.
      #- test-unit-harness-gpu
      - test-unit-harness-tf2
      - test-unit-model-hub
      - test-unit-storage:
          context: storage-unit-tests
          filters: *any-upstream
      - test-examples
      - python-coverage:
          requires:
            - test-unit-harness-cpu
            - test-unit-harness-tf2
            - test-unit-model-hub
            - test-unit-storage

  test-intg:
    jobs:
      - test-intg-downstream
      - test-intg-master
      - test-intg-agent
      - go-coverage:
          requires:
            - test-intg-master
            - test-intg-agent

  test-e2e:
    jobs:
      - build-proto
      - build-helm
      - build-react:
          dev-mode: true
      - build-storybook
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - build-go
      - package-and-push-system-local:
          requires:
            - build-react
            - build-docs

      - package-and-push-system-dev:
          requires:
            - build-react
            - build-docs
          filters: *any-upstream

      - test-debian-packaging:
          requires:
            - package-and-push-system-local

      - test-e2e:
          name: test-e2e-tf2
          requires:
            - build-go
          parallelism: 1
          tf2: true
          mark: "tensorflow2_cpu"

      - test-e2e:
          name: test-e2e-tf1
          requires:
            - build-go
          parallelism: 1
          tf1: true
          mark: "tensorflow1_cpu"

      - test-e2e:
          name: test-e2e-cpu
          requires:
            - build-go
          parallelism: 4
          tf1: true
          tf2: true
          mark: e2e_cpu

      - test-e2e:
          name: test-e2e-cpu-double
          requires:
            - build-go
          parallelism: 1
          tf1: false
          tf2: true
          mark: e2e_cpu_2a
          target-stage: agent2

      - test-e2e:
          name: test-e2e-managed-devcluster
          requires:
            - build-go
          parallelism: 2
          tf1: false
          tf2: true
          mark: managed_devcluster
          managed-devcluster: true
          # Managed devcluster restarts the master over the course of the tests,
          # so `compare_stats` cannot get the full logs from `det master logs`.
          extra-pytest-flags: "--no-compare-stats"

      - test-e2e:
          name: test-e2e-cpu-elastic
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_elastic
          devcluster-config: elastic.devcluster.yaml
          target-stage: agent

      - test-e2e:
          name: test-e2e-postgres10-with-ssl
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: '10'

      - test-e2e:
          name: test-e2e-postgres14-with-ssl
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: '14'

      - test-e2e:
          name: test-e2e-old-agent-versions
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: custom-agent-version.devcluster.yaml
          target-stage: agent
          matrix:
            parameters:
              agent-version: ['0.17.10']

      - deploy:
          name: deploy-latest-master-cluster
          enable-cors: true
          reattach-enabled: true
          context: aws
          filters:
            branches:
              only:
                - master
          requires:
            - package-and-push-system-dev
          max-dynamic-agents: 2

      - deploy:
          name: deploy-preview-cluster
          enable-cors: true
          context: aws
          filters:
            branches:
              only:
                - master
          requires:
            - package-and-push-system-dev
          max-dynamic-agents: 1
          cluster-id: determined-preview
          compute-agent-instance-type: t2.medium

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context: aws
          filters:
            branches:
              only: master
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context: aws
          filters:
            branches:
              only: master
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context: gcp
          filters:
            branches:
              only: master
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context: gcp
          filters:
            branches:
              only: master
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context: gcp
          filters:
            branches:
              only: master
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]

      - test-stress:
          name: test-stress
          filters: *any-upstream
          requires:
            - package-and-push-system-local
          matrix:
            parameters:
              parallelism: [1]
              mark: ["stress_test"]
              det-version: [$CIRCLE_SHA1]

      - test-det-deploy:
          name: test-det-deploy-local
          requires:
            - package-and-push-system-local
          matrix:
            parameters:
              parallelism: [2]
              mark: ["det_deploy_local"]
              det-version: [$CIRCLE_SHA1]

  test-e2e-longrunning:
    jobs:
      # Build and publish artifacts used in tests
      - build-helm:
          filters: *any-upstream
      - build-proto:
          filters: *any-upstream
      - build-react:
          dev-mode: true
          filters: *any-upstream
      - build-docs:
          filters: *any-upstream
          requires:
            - build-helm
            - build-proto
      - package-and-push-system-dev:
          requires:
            - build-react
            - build-docs
          filters: *any-upstream

      # Deploy full cluster
      - request-dev-deploy:
          type: approval
          filters: *upstream-feature-branch

      - deploy:
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-dev-deploy
            - package-and-push-system-dev

      # Dsitributed tests
      - request-e2e-cpu-distributed:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-e2e-cpu-distributed
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [16]

      # Deepseed tests
      - request-deepspeed-tests:
          type: approval
          filters: *upstream-feature-branch

        # DeepSpeed tests do not work on K80s so we need V100 instances.
      - test-e2e-aws:
          name: test-e2e-deepspeed
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-deepspeed-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["p3.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

      # K8s GPU tests
      - request-k8-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context: gcp
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context: gcp
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]

      # K8s CPU tests
      - request-k8-tests-cpu:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context: gcp
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cpu
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]

      # K8s Cuda 11 tests
      - request-k8-tests-cuda-11:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-single-gpu-cuda-11
          context: gcp
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cuda-11
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              environment-image:
                - determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-0.19.1

      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-parallel-cuda-11
          context: gcp
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cuda-11
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              environment-image:
                - determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-0.19.1

      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-single-gpu-tfonly
          context: gcp
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cuda-11
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.5-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.6-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.7-gpu-0.19.1

      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-parallel-tfonly
          context: gcp
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cuda-11
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.5-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.6-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.7-gpu-0.19.1

      # Nightly distributed tests
      - request-gpu-distributed-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-distributed-nightly
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-distributed-nightly
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]

      # Nightly tests
      - request-gpu-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-nightly
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]

      # GPU tests
      - request-gpu-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]
              slack-mentions: ["${SLACK_USER_ID}"]

      # mmdetection tests
      - request-mmdetection-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-mmdetection
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-mmdetection-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["mmdetection"]
              mark: ["model_hub_mmdetection"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

      # Transformers tests
      - request-transformers-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-transformers
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-transformers-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["transformers"]
              mark: ["model_hub_transformers"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

      - test-e2e-aws:
          name: test-e2e-transformers-amp
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-transformers-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["transformers-amp"]
              mark: ["model_hub_transformers_amp"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

  nightly:
    triggers:
      - schedule:
          cron: "0 5 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context: aws
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]
      - test-e2e-aws:
          name: test-e2e-gpu-distributed
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-transformers
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["transformers"]
              mark: ["model_hub_transformers"]
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-transformers-amp
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["transformers-amp"]
              mark: ["model_hub_transformers_amp"]
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-mmdetection
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["mmdetection"]
              mark: ["model_hub_mmdetection"]
              compute-agent-instance-type: ["p2.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
        # DeepSpeed tests do not work on K80s so we need V100 instances.
      - test-e2e-aws:
          name: test-e2e-gpu-deepspeed
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              compute-agent-instance-type: ["p3.8xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [16]

  weekly-cuda-11:
    triggers:
      - schedule:
          cron: "0 0 * * 1"
          filters:
            branches:
              only:
                - master
    jobs:
      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-single-gpu-cuda-11
          context: gcp
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              environment-image:
                - determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-0.19.1

      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-parallel-cuda-11
          context: gcp
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              environment-image:
                - determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-0.19.1

      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-single-gpu-tfonly
          context: gcp
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.5-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.6-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.7-gpu-0.19.1

      - test-e2e-gke-cuda-11:
          name: test-e2e-gke-parallel-tfonly
          context: gcp
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.5-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.6-gpu-0.19.1
                - determinedai/environments:cuda-11.2-tf-2.7-gpu-0.19.1

  weekly-vuln-scan:
    triggers:
      - schedule:
          cron: "0 0 * * 0"
          filters:
            branches:
              only:
                - master
    jobs:
      - scan-docker-images

  release:
    jobs:
      - build-helm:
          filters: *release-and-rc-filters
      - build-proto:
          filters: *release-and-rc-filters
      - build-react:
          context: determined-production
          filters: *release-and-rc-filters
      - build-docs:
          context: determined-production
          filters: *release-and-rc-filters
          requires:
            - build-helm
            - build-proto

      - package-and-push-system-rc:
          requires:
            - build-react
            - build-docs
          context: determined-production
          filters: *rc-filters

      - publish-python-package:
          name: publish-python-package-rc
          matrix:
            parameters:
              path: ["harness", "common", "cli", "deploy", "model_hub"]
          context: determined-production
          filters: *rc-filters
          requires:
            - package-and-push-system-rc

      - package-and-push-system-release:
          requires:
            - build-react
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-python-package:
          name: publish-python-package-release
          matrix:
            parameters:
              path: ["harness", "common", "cli", "deploy", "model_hub"]
          context: determined-production
          filters: *release-filters
          requires:
            - package-and-push-system-release

      - publish-docs:
          requires:
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-helm:
          requires:
            - build-helm
          context: determined-production
          filters: *release-and-rc-filters

      - upload-try-now-template:
          context: determined-production
          filters: *release-filters
