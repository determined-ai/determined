# Use the latest 2.1 version of CircleCI pipeline process engine. See: https://circleci.com/docs/2.0/configuration-reference
version: 2.1

orbs:
  win: circleci/windows@5.0.0
  slack: circleci/slack@3.4.2
  kubernetes: circleci/kubernetes@0.11.0
  helm: circleci/helm@1.2.0
  gcloud: circleci/gcp-cli@2.1.0
  queue: eddiewebb/queue@volatile
  codecov: codecov/codecov@3.0.0

executors:
  python-37:
    docker:
      - image: python:3.7-slim-buster
  python-38:
    docker:
      - image: python:3.8-slim-buster
  python-39:
    docker:
      - image: python:3.9-slim-buster

parameters:
  det-version:
    type: string
    default: 0.24.0-dev0
  docker-image:
    type: string
    default: determinedai/cimg-base:latest
  machine-image:
    type: string
    default: ubuntu-2004:202201-02
  gpu-machine-image:
    type: string
    default: ubuntu-2004-cuda-11.2:202103-01
  # DEFAULT_PT_GPU_IMAGE: Pytorch training image reference used by the tests
  # Inject here as a parameter so that it is updated by bumpversion, and can
  # be referenced by --ee testing.
  default-pt-gpu-image:
    type: string
    default: determinedai/environments:cuda-11.3-pytorch-1.12-gpu-2b7e2a1
  # Some python, go, and react dependencies are cached by circleci via `save_cache`/`restore_cache`.
  # If the dependencies stay the same, but the circleci code that would produce them is changed,
  # it may be necessary to invalidate the cache by incrementing this value.
  # For example, if you change an env variable affecting a build of a python package with the same version,
  # the old build may be cached, and you may need to invalidate it.
  cache-buster:
    type: string
    default: v1dev23
  gke-version:
    type: string
    default: "1.25.10"
  master-build-resource-class:
    type: string
    default: xlarge

release-and-rc-filters: &release-and-rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+/
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+/

rc-filters: &rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+/

release-filters: &release-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+/

upstream-feature-branch: &upstream-feature-branch
  branches:
    ignore:
      - /pull\/.*/
      - /release-.*/
      - main

any-upstream: &any-upstream
  branches:
    ignore:
      - /pull\/.*/

any-fork: &any-fork
  branches:
    only:
      - /pull\/.*/

commands:
  fix-circle-working-directory:
    description: "Fix CIRCLE_WORKING_DIRECTORY"
    steps:
      - run: echo 'CIRCLE_WORKING_DIRECTORY="${CIRCLE_WORKING_DIRECTORY/#\~/$HOME}"' >> $BASH_ENV

  # circleci's checkout does not fetch submodules by default
  # https://circleci.com/docs/2.0/configuration-reference/#checkout
  checkout-with-sm:
    steps:
      - checkout
      - run: git submodule sync
      - run: git submodule update --init

  skip-if-docs-only:
    steps:
      - run: git remote add upstream https://github.com/determined-ai/determined
      - run: git fetch upstream
      - run:
          name: check for docs-only changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            if [ "$DIFF_DIRS" = "docs" ] ; then
              echo "docs-only change detected, halting job now"
              circleci-agent step halt
            else
              echo "detected non-docs-only changes: $DIFF_DIRS"
            fi

  skip-if-not-docs:
    steps:
      - run: git remote add upstream https://github.com/determined-ai/determined
      - run: git fetch upstream
      - run:
          name: check for any docs changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            # check "shared" is in the list of changed directories
            if [[ "$DIFF_DIRS" == *"docs"* ]]; then
              echo "docs change detected, running downstream steps"
            else
              echo "detected non-docs changes, halting job now"
              circleci-agent step halt
            fi

  skip-if-webui-only:
    steps:
      - run: git fetch upstream
      - run:
          name: check for webui-only changes # must be run after skip-if-docs-only
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            if [ "$DIFF_DIRS" = "webui" ] ; then
              echo "webui-only change detected, halting job now"
              circleci-agent step halt
            else
              echo "detected non-webui-only changes: $DIFF_DIRS"
            fi

  skip-if-not-shared:
    steps:
      - run: git remote add upstream https://github.com/determined-ai/determined
      - run: git fetch upstream
      - run:
          name: check for shared changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE":webui/react/src HEAD:webui/react/src)"
            # check "shared" is in the list of changed directories
            if [[ "$DIFF_DIRS" == *"shared"* ]]; then
              echo "shared code changed, running downstream test"
            else
              echo "shared code not affected, halting job now"
              circleci-agent step halt
            fi

  skip-if-github-only:
    steps:
      - run: git fetch upstream
      - run:
          name: check for github-only changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            if [ "$DIFF_DIRS" = ".github" ] ; then
              echo "github-only change detected, halting job now"
              circleci-agent step halt
            else
              echo "detected changes outside of .github: $DIFF_DIRS"
            fi

  set-slack-user-id:
    steps:
      - run:
          name: Set Slack variables
          command: |
            if ! [ -x "$(command -v jq)" ]; then
              apt update && apt install -y jq
            fi

            AUTHOR_EMAIL="$(git show -s --format='%ae' $CIRCLE_SHA1)"
            echo "export AUTHOR_EMAIL=\"${AUTHOR_EMAIL}\"" >> $BASH_ENV
            LOOKUP_RESPONSE=$(curl -s "https://slack.com/api/users.lookupByEmail?token=${SLACK_API_TOKEN}&email=${AUTHOR_EMAIL}")
            SUCCESS=$(echo "$LOOKUP_RESPONSE" | jq ".ok")
            if [[ "$SUCCESS" == "true" ]]; then
              SLACK_USER_ID=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.id")
              SLACK_NAME=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.name")
              echo "export SLACK_NAME=\"${SLACK_NAME}\"" >> $BASH_ENV
              echo "export SLACK_USER_ID=\"${SLACK_USER_ID}\"" >> $BASH_ENV
            else
              echo "Unable to find Slack user ID for  \"${AUTHOR_EMAIL}\"."
            fi

  pull-task-images:
    parameters:
      tf1:
        type: boolean
        default: false
      tf2:
        type: boolean
        default: false
    steps:
      - when:
          condition: <<parameters.tf1>>
          steps:
            - run: docker pull determinedai/environments:py-3.7-pytorch-1.7-tf-1.15-cpu-6eceaca
      - when:
          condition: <<parameters.tf2>>
          steps:
            - run: docker pull determinedai/environments:py-3.8-pytorch-1.12-tf-2.11-cpu-2b7e2a1

  login-docker:
    parameters:
      repository:
        type: string
        default: ""
      username:
        type: string
      password:
        type: string
    steps:
      - run: echo "<<parameters.password>>" | docker login <<parameters.repository>> -u "<<parameters.username>>" --password-stdin

  login-helm:
    steps:
      - run: helm repo add determined https://helm.ngc.nvidia.com/isv-ngc-partner/determined --username=$NGC_API_USERNAME --password=$NGC_API_KEY

  reinstall-go:
    steps:
      - run: sudo rm -rf /usr/local/go # Remove system go.
      - run: tools/scripts/retry.sh curl --retry-connrefused --retry 10 https://dl.google.com/go/go1.20.linux-amd64.tar.gz -o /tmp/go.linux-amd64.tar.gz
      - run: sudo tar -C /usr/local -xzf /tmp/go.linux-amd64.tar.gz
      - run: echo 'export PATH=$PATH:$HOME/go/bin' >> $BASH_ENV


  install-protoc:
    steps:
      - run: curl --retry-connrefused --retry 10 -o /tmp/protoc.zip -L https://github.com/protocolbuffers/protobuf/releases/download/v3.20.3/protoc-3.20.3-linux-x86_64.zip
      - run: unzip -o /tmp/protoc.zip -d $HOME/.local

  install-codecov:
    steps:
      - run: mkdir -p $HOME/.local/bin
      - run: curl --output $HOME/.local/bin/codecov https://uploader.codecov.io/latest/linux/codecov
      - run: chmod +x $HOME/.local/bin/codecov
      - run: echo 'export PATH=$PATH:$HOME/.local/bin/codecov' >> $BASH_ENV

  setup-go-intg-deps:
    steps:
      - install-protoc # Install newer version of protoc into $HOME/.local/bin, since default is proto2.
      - run: PATH=$HOME/.local/bin:$PATH make -C proto get-deps
      - run: PATH=$HOME/.local/bin:$PATH make -C proto build
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - install-devcluster
      - start-devcluster:
          target-stage: elastic
          devcluster-config: elastic-base.devcluster.yaml

  go-get-deps:
    steps:
      - install-protoc
      - restore_cache:
          keys:
            - det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "master/get-deps.sh" }}-{{ checksum  "master/go.sum" }}-{{ checksum  "agent/get-deps.sh" }}-{{ checksum  "agent/go.sum" }}-{{ checksum  "proto/get-deps.sh" }}-{{ checksum  "proto/go.sum" }}
      - run: make -C proto get-deps
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - save_cache:
          key: det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "master/get-deps.sh" }}-{{ checksum  "master/go.sum" }}-{{ checksum  "agent/get-deps.sh" }}-{{ checksum  "agent/go.sum" }}-{{ checksum  "proto/get-deps.sh" }}-{{ checksum  "proto/go.sum" }}
          paths:
            - "/home/circleci/go/"
            - "/home/circleci/.cache/go-build/"
  react-get-deps:
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Write cache suffix
          command: |
            echo '<<pipeline.parameters.cache-buster>>' > /tmp/react-get-deps.cachekey
            cat webui/react/package-lock.json >> /tmp/react-get-deps.cachekey
            echo $HOME >> /tmp/react-get-deps.cachekey
            cat /tmp/react-get-deps.cachekey
      - restore_cache:
          keys:
            - det-react-deps-{{ checksum "/tmp/react-get-deps.cachekey" }}
      - run:
          name: Get React dependencies
          command: |
            make -C webui/react node_modules/done.stamp
      - save_cache:
          key: det-react-deps-{{ checksum "/tmp/react-get-deps.cachekey" }}
          paths:
            - "webui/react/node_modules"

  install-wheel:
    parameters:
      package-name:
        type: string
      package-location:
        type: string
    steps:
      - run:
          name: Install <<parameters.package-name>>
          working_directory: <<parameters.package-location>>
          command: |
            make build
            pip install --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>
            pip install --no-deps --force-reinstall --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>

  setup-python-venv:
    description: Set up and create Python venv.
    parameters:
      determined:
        type: boolean
        default: false
      model-hub:
        type: boolean
        default: false
      install-python:
        type: boolean
        default: true
      extras-requires:
        type: string
        default: ""
      extra-requirements-file:
        type: string
        default: ""
      executor:
        type: string
      python-version:
        type: string
        default: "3.8.16"
    steps:
      - when:
          condition: <<parameters.install-python>>
          steps:
            - run:
                name: prepare conda PATH
                command: |
                  echo 'export PATH=/opt/conda/bin:${PATH}' >> $BASH_ENV

            - run:
                name: prepare conda install_python.sh
                command: |
                  cat \<<'EOF' > /tmp/install_python.sh
                  #!/bin/bash
                  PYTHON_VERSION="${1}"

                  CONDA_DIR="/opt/conda"
                  CONDA_INSTALLER="Miniconda3-py39_23.5.2-0-Linux-x86_64.sh"
                  CONDA_SHA256="9829d95f639bd0053b2ed06d1204e60644617bf37dd5cc57523732e0e8d64516"
                  CONDA_URL="https://repo.anaconda.com/miniconda"

                  mkdir -p /etc/determined/conda.d
                  mkdir -p "${CONDA_DIR}"

                  cd /tmp
                  curl --retry 3 -fsSL -O "${CONDA_URL}/${CONDA_INSTALLER}"
                  echo "${CONDA_SHA256}  ${CONDA_INSTALLER}" | sha256sum ${CONDA_INSTALLER}
                  bash "./${CONDA_INSTALLER}" -u -b -p "${CONDA_DIR}"
                  rm -f "./${CONDA_INSTALLER}"

                  ${CONDA_DIR}/bin/conda install python=${PYTHON_VERSION}
                  ${CONDA_DIR}/bin/conda update --prefix ${CONDA_DIR} --all -y
                  ${CONDA_DIR}/bin/conda clean --all -y
                  EOF

            - run:
                name: run conda install_python.sh
                command: |
                  sudo bash /tmp/install_python.sh <<parameters.python-version>>

            - run:
                name: Setup venv
                command: |
                  python3 -m venv /tmp/venv
                  echo 'export PATH=/tmp/venv/bin:$PATH' >> $BASH_ENV
                  /tmp/venv/bin/python -m pip install --upgrade pip wheel setuptools

      # Either of make -C {harness,model_hub} build require pypa's build module.
      - when:
          condition:
            or:
              - <<parameters.determined>>
              - <<parameters.model-hub>>
          steps:
            - run:
                name: Install pypa builder
                command: python3 -m pip install build

      - run:
          name: Write cache key
          command: |
            echo <<parameters.executor>> > /tmp/cachefile
            pip freeze --all >> /tmp/cachefile
            if [ "<<parameters.determined>>" = "true" ]; then
              cat harness/setup.py >> /tmp/cachefile
            fi
            if [ "<<parameters.model-hub>>" = "true" ]; then
              cat model_hub/setup.py >> /tmp/cachefile
            fi
            echo <<parameters.extras-requires>> >> /tmp/cachefile
            if [ -n <<parameters.extra-requirements-file>> ]; then
              cat <<parameters.extra-requirements-file>> >> /tmp/cachefile
            fi

      - restore_cache:
          keys:
            - det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
      - when:
          condition: <<parameters.determined>>
          steps:
            - install-wheel:
                package-name: determined
                package-location: ~/project/harness
      - when:
          condition: <<parameters.model-hub>>
          steps:
            - run:
                name: Install mmdetection dependencies
                command: |
                  sudo apt-get update
                  sudo apt-get install -y ffmpeg libsm6 libxext6
            - install-wheel:
                package-name: model-hub
                package-location: ~/project/model_hub
      - run:
          name: Install <<parameters.extras-requires>>
          command: |
            if [ -n "<<parameters.extras-requires>>" ]; then
              tools/scripts/retry.sh pip install --progress-bar off <<parameters.extras-requires>>
            fi
      - run:
          name: Install <<parameters.extra-requirements-file>>
          command: |
            if [ -n "<<parameters.extra-requirements-file>>" ]; then
              tools/scripts/retry.sh pip install -r <<parameters.extra-requirements-file>>
            fi
      - save_cache:
          key: det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
          paths:
            - "/tmp/venv"
      - run: python --version
      - run: pip --version
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"

  setup-docker-env:
    description: Set up and create docker env.
    parameters:
      image:
        type: string
      image-name:
        type: string
        default: "test_container"
      bind-mount:
        type: string
        default: "$PWD"
      extras-requires:
        type: string
        default: ""
      gpus:
        type: boolean
        default: true
    steps:
      - when:
          condition: <<parameters.gpus>>
          steps:
            - run: docker run -v <<parameters.bind-mount>>:/mnt/project --gpus=all -it -d --name <<parameters.image-name>> --workdir /mnt/project <<parameters.image>> /bin/bash
      - when:
          condition:
            not: <<parameters.gpus>>
          steps:
            - run: docker run -v <<parameters.bind-mount>>:/mnt/project -it -d --name <<parameters.image-name>> --workdir /mnt/project <<parameters.image>> /bin/bash
      - run:
          name: Install <<parameters.extras-requires>>
          command: |
            if [ -n "<<parameters.extras-requires>>" ]; then
              docker exec <<parameters.image-name>> tools/scripts/retry.sh pip install --progress-bar off <<parameters.extras-requires>>
            fi

  setup-paths:
    steps:
      - run: echo 'export PATH=$PATH:$HOME/.local/bin' >> $BASH_ENV
      - run: echo 'export PATH=$PATH:/usr/local/nvidia/bin' >> $BASH_ENV
      - run: echo 'export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64' >> $BASH_ENV

  run-e2e-tests:
    parameters:
      mark:
        type: string
        default: ""
      junit-path:
        type: string
        default: "/tmp/test-results/e2e/tests.xml"
      master-scheme:
        type: string
        default: "http"
      master-host:
        type: string
        default: "localhost"
      master-port:
        type: string
        default: "8080"
      master-cert:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      managed-devcluster:
        type: boolean
        default: false
      extra-pytest-flags:
        type: string
        default: ""
      wait-for-master:
        type: boolean
        default: true
    steps:
      - run:
          name: Split tests
          working_directory: ~/project/e2e_tests
          command: |
            # If a test mark is specified, preselect only files that contain tests with that mark to
            # minimize how wrong CircleCI's splitting can get things.
            if [ -n "<<parameters.mark>>" ]; then
              mark="<<parameters.mark>>"
              # Extract the first mark for complex conditions such as 'mark1 and not mark2'.
              mark_prefix=${mark%% and*}
              find tests -name 'test*.py' -print0 | xargs -0 grep -rl "pytest\.mark\.$mark_prefix"
            else
              circleci tests glob 'tests/**/test*.py'
            fi | circleci tests split --split-by=timings > /tmp/tests-to-run
            echo "Running tests from these files:"
            sed 's/^/- /' </tmp/tests-to-run

      - when:
          condition:
            and:
              - not: <<parameters.managed-devcluster>>
              - <<parameters.wait-for-master>>
          steps:
            - wait-for-master:
                scheme: <<parameters.master-scheme>>
                host: <<parameters.master-host>>
                port: <<parameters.master-port>>

      - run:
          name: Run e2e tests
          working_directory: ~/project/e2e_tests
          no_output_timeout: 30m
          command: |
            DET_MASTER_CERT_FILE=<<parameters.master-cert>> DET_MASTER_CERT_NAME=<<parameters.master-cert-name>> \
            pytest -vv -s \
            -m '<<parameters.mark>>' \
            --durations=0 \
            --master-scheme="<<parameters.master-scheme>>" \
            --master-host="<<parameters.master-host>>" \
            --master-port="<<parameters.master-port>>" \
            -o junit_family=xunit1 \
            --junit-xml="<<parameters.junit-path>>" \
            <<parameters.extra-pytest-flags>> \
            $(< /tmp/tests-to-run)
      - upload-test-job:
          only_on_branch: main
          test_results_path: <<parameters.junit-path>>

  run-det-deploy-tests:
    parameters:
      mark:
        type: string
        default: ""
      det-version:
        type: string
        default: ""
    steps:
      - run:
          name: Run det-deploy tests
          working_directory: ~/project/e2e_tests
          command: |
            pytest -vv -s \
            -m <<parameters.mark>> \
            --det-version="<<parameters.det-version>>"

  deploy-aws-cluster:
    parameters:
      cluster-id:
        type: string
      extra-tags:
        type: string
        default: ""
      det-version:
        type: string
      keypair:
        type: string
        default: "integrations-test"
      enable-cors:
        type: boolean
        default: false
      master-tls-cert:
        type: string
        default: ""
      master-tls-key:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      max-dynamic-agents:
        type: integer
        default: 1
      retain-log-group:
        type: boolean
        default: false
      log-group-prefix:
        type: string
        default: ""
      reattach-enabled:
        type: boolean
        default: false
      deployment-type:
        type: string
        default: simple-rds
    steps:
      - run:
          name: Initialize extra arguments
          command: touch /tmp/det-deploy-extra-args
      - when:
          condition:
            equal: [true, << parameters.enable-cors >>]
          steps:
            - run:
                name: Enable CORS
                command: "echo --enable-cors >> /tmp/det-deploy-extra-args"
      - when:
          condition:
            << parameters.extra-tags >>
          steps:
            - run:
                name: Add extra tags
                command: |
                  echo "<< parameters.extra-tags >>" | \
                  sed 's/ /-/g' | \
                  sed -r 's/([^,=]*=[^,=]*),?/--add-tag \1 /g' >> \
                    /tmp/det-deploy-extra-args

      - run:
          name: Configure TLS arguments
          command: |
            if [ -n "<<parameters.master-tls-cert>>" ]; then echo "--master-tls-cert <<parameters.master-tls-cert>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-tls-key>>" ]; then echo "--master-tls-key <<parameters.master-tls-key>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-cert-name>>" ]; then echo "--master-cert-name <<parameters.master-cert-name>>" >> /tmp/det-deploy-extra-args; fi
      - run:
          name: Configure log group arguments
          command: |
            if <<parameters.retain-log-group>>; then echo "--retain-log-group" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.log-group-prefix>>" ]; then echo "--log-group-prefix <<parameters.log-group-prefix>>" >> /tmp/det-deploy-extra-args; fi
      - when:
          condition:
            equal: [true, <<parameters.reattach-enabled>>]
          steps:
            - run:
                name: set reattach-enabled
                command: echo --agent-reattach-enabled true --agent-reconnect-attempts 24 --no-update-terminate-agents --notebook-timeout 1800 >> /tmp/det-deploy-extra-args
      - run:
          name: Deploy AWS cluster
          no_output_timeout: 20m
          command: |
            echo "-----BEGIN ARGS-----"
            cat /tmp/det-deploy-extra-args
            echo "-----END ARGS-----"
            MAX_RETRIES=6 DET_DEBUG=1 tools/scripts/retry.sh det deploy aws up \
              $(< /tmp/det-deploy-extra-args) \
              --cluster-id <<parameters.cluster-id>> \
              --det-version <<parameters.det-version>> \
              --aux-agent-instance-type <<parameters.aux-agent-instance-type>> \
              --compute-agent-instance-type <<parameters.compute-agent-instance-type>> \
              --max-dynamic-agents <<parameters.max-dynamic-agents>> \
              --keypair <<parameters.keypair>> \
              --deployment-type <<parameters.deployment-type>> \
              --yes

  terminate-aws-cluster:
    parameters:
      cluster-id:
        type: string
    steps:
      - run:
          name: Terminate AWS Cluster
          when: always
          no_output_timeout: 20m
          command: |
            MAX_RETRIES=6 DET_DEBUG=1 tools/scripts/retry.sh det deploy aws down \
              --cluster-id <<parameters.cluster-id>> --yes

  setup-aws-cluster:
    parameters:
      cluster-id:
        type: string
      extra-tags:
        type: string
        default: ""
      det-version:
        type: string
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      max-dynamic-agents:
        type: integer
        default: 1
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
      master-cert-name:
        type: string
      deployment-type:
        type: string
        default: simple-rds
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - deploy-aws-cluster:
          cluster-id: ${CLUSTER_ID}
          extra-tags: <<parameters.extra-tags>>
          det-version: <<parameters.det-version>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>
          master-cert-name: <<parameters.master-cert-name>>
          log-group-prefix: determined-ci
          retain-log-group: true
          deployment-type: <<parameters.deployment-type>>
      - set-master-address-aws:
          cluster-id: ${CLUSTER_ID}
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>

  terminate-gke-cluster:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
    steps:
      # Use a run instead of `gke/helm` orbs because circle CI orbs do not support `when`.
      - run:
          name: Delete Helm release
          when: always
          command: helm delete ci
      - run:
          name: Terminate GKE Cluster
          when: always
          command: |
            gcloud container clusters delete <<parameters.cluster-id>> --quiet --region=<<parameters.region>>

  setup-gke-cluster:
    parameters:
      cluster-id:
        type: string
      det-version:
        type: string
      gke-version:
        type: string
      machine-type:
        type: string
      num-machines:
        type: integer
      labels:
        type: string
      gpu-type:
        type: string
      gpus-per-machine:
        type: integer
      slot-type:
        type: string
      slot-resource-requests-cpu:
        type: integer
      region:
        type: string
      node-locations:
        type: string
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
      environment-image:
        type: string
      storage-bucket:
        type: string
        default: "det-ci-373"
      accel-node-taints:
        type: string
        default: ""
      cluster-ipv4-cidr:
        type: string
        description: The IP address range for the pods in this cluster.
        # We are specifying this since the cluster doesn't need to be very large and GKE was intermittently
        # defaulting to an excessively large range. "/19" is the minimum range it will let us allocate and
        # allows the cluster to create at most 16 nodes.
        # Kubernetes services default to 4096 IPs, or a "/20", and the remainder gets split up into a "/24" per node.
        # So with a "/19", there are 8192 total addresses. After 4096 get used for services there are 4096 left.
        # That's 2^12 / 2^8 = 16 nodes for our tests. Thanks @Danny Sauer!
        # If we find we need more nodes in our tests in the future, we can bump this up.
        default: "/19"
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - set-cluster-labels:
          labels: <<parameters.labels>>
      - gcloud/install:
          version: "412.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: |
            tries=5
            until gcloud components install gke-gcloud-auth-plugin --quiet; do
              if [[ $((--tries)) -eq 0 ]]; then
                exit 1
              fi
              sleep 15
            done

            echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $BASH_ENV
          name: Install GKE auth plugin
      - run:
          command: |
            echo "Console URL for cluster: https://console.cloud.google.com/kubernetes/clusters/details/<<parameters.region>>/${CLUSTER_ID}?project=determined-ai"
            gcloud container clusters create ${CLUSTER_ID} --machine-type=n1-standard-8 --cluster-version=<<parameters.gke-version>> --region=<<parameters.region>> --node-locations=<<parameters.node-locations>> --scopes storage-rw,cloud-platform --num-nodes 1 --labels environment=ci,${LABELS} --no-enable-ip-alias --subnetwork default --cluster-ipv4-cidr=<<parameters.cluster-ipv4-cidr>>
          name: Create GKE cluster
          no_output_timeout: 30m
      - run:
          command: gcloud container clusters get-credentials ${CLUSTER_ID} --project ${GOOGLE_PROJECT_ID} --region <<parameters.region>>
          name: Get Kubeconfig
      - run:
          command: |
            echo 'export HELM_VALUES="detVersion=<<parameters.det-version>>,maxSlotsPerPod=<<parameters.gpus-per-machine>>,checkpointStorage.type=gcs,checkpointStorage.bucket=<<parameters.storage-bucket>>"' >> "$BASH_ENV"
          name: Prepare helm overrides
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
                name: Install NVIDIA drivers
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},slotType=<<parameters.slot-type>>,slotResourceRequests.cpu=<<parameters.slot-resource-requests-cpu>>,resourcePools[0].agent_reattach_enabled=true,resourcePools[0].pool_name=default,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].effect=NoSchedule,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].effect=NoSchedule"' >> "$BASH_ENV"
                name: CPU setup helm overrides
      - when:
          condition: <<parameters.environment-image>>
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},taskContainerDefaults.cpuImage=<<parameters.environment-image>>,taskContainerDefaults.gpuImage=<<parameters.environment-image>>"' >> "$BASH_ENV"
                name: env image helm overrides
      - helm/install-helm-chart:
          chart: helm/charts/determined
          helm-version: v3.2.4
          namespace: "default"
          wait: true
          release-name: "ci"
          values-to-override: ${HELM_VALUES}
      - set-master-address-gke:
          release-name: "ci"
          namespace: "default"
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                no_output_timeout: 30m
                command: |
                  (sleep 29.5m && pkill python3) &
                  gcloud container node-pools create accel \
                    --cluster ${CLUSTER_ID} \
                    --region '<<parameters.region>>' \
                    --num-nodes '<<parameters.num-machines>>' \
                    --accelerator 'type=<<parameters.gpu-type>>,count=<<parameters.gpus-per-machine>>' \
                    --machine-type='<<parameters.machine-type>>' \
                    --scopes cloud-platform \
                    --node-taints='<<parameters.accel-node-taints>>' \
                    || (
                      curl "$SLACK_WEBHOOK" \
                        -H 'Content-Type: application/json' \
                        -d "{\"text\":\"GKE node pool creation failed on branch \`$CIRCLE_BRANCH\`! $CIRCLE_BUILD_URL\"}"
                      echo "================"
                      echo "GKE node pool creation failed, but we are marking the job as successful because this is not directly our problem. Ask the infrastructure engineering team about increasing the quota if you want."
                      echo "================"
                      circleci-agent step halt
                    )
                name: Create GPU node pool
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: gcloud container node-pools create accel --cluster ${CLUSTER_ID} --region <<parameters.region>> --num-nodes <<parameters.num-machines>> --machine-type=<<parameters.machine-type>> --scopes cloud-platform --node-taints=<<parameters.accel-node-taints>>
                name: Create CPU node pool

  generate-tls-cert:
    steps:
      - run: |
          .circleci/scripts/generate_cert.sh /tmp/master
          echo 'export MASTER_TLS_CERT=/tmp/master.crt MASTER_TLS_KEY=/tmp/master.key MASTER_CERT_NAME=determined-master-ci' >> $BASH_ENV

  set-master-address-aws:
    parameters:
      cluster-id:
        type: string
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
    steps:
      - run: |
          MASTER_HOST=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> DeterminedAddress)
          echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV
          if [ -n "<<parameters.master-tls-cert>>" ] && [ -n "<<parameters.master-tls-key>>" ]; then
            echo "export MASTER_PORT=8443" >> $BASH_ENV
            echo "export MASTER_SCHEME=https" >> $BASH_ENV
          fi

  set-master-address-gke:
    parameters:
      release-name:
        type: string
      namespace:
        type: string
    steps:
      - run:
          name: Set Master Address
          command: |
            MASTER_HOST=$(kubectl get -n <<parameters.namespace>>  service determined-master-service-<<parameters.release-name>> \
            --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
            echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV
            echo "${MASTER_HOST}"

  set-google-application-credentials:
    steps:
      - run:
          name: Set Google Application Credentials
          command: |
            GOOGLE_APPLICATION_CREDENTIALS=${HOME}/gcloud-service-key.json
            echo "export GOOGLE_APPLICATION_CREDENTIALS=\"${GOOGLE_APPLICATION_CREDENTIALS}\"" >> $BASH_ENV

  set-cluster-id:
    parameters:
      cluster-id:
        type: string
    steps:
      - run: echo "export CLUSTER_ID=\"<<parameters.cluster-id>>\"" >> $BASH_ENV

  set-cluster-labels:
    parameters:
      labels:
        type: string
    steps:
      - run: echo "export LABELS=\"$(echo '<<parameters.labels>>' | sed 's/ /-/g')\"" >> $BASH_ENV

  wait-for-master:
    parameters:
      scheme:
        type: string
        default: "http"
      host:
        type: string
      port:
        type: string
        default: "8080"
    steps:
      - run: python .circleci/scripts/wait_for_master.py <<parameters.scheme>>://<<parameters.host>>:<<parameters.port>>

  upload-test-job:
    parameters:
      only_on_branch:
        type: string
        default: ""
      test_results_path:
        type: string
        default: ""
    steps:
      - when:
          condition:
            equal: [<<parameters.only_on_branch>>, << pipeline.git.branch >>]
          steps:
            - run:
                name: Test run success -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'success' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_success
            - run:
                name: Test run failed -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'failed' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_fail

  locate-cloudwatch-logs:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
        default: us-west-2
    steps:
      - run:
          name: Locate CloudWatch logs
          when: always
          command: |
            LOG_GROUP=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> LogGroup)
            echo "Cluster logs can be found in CloudWatch under the log group $LOG_GROUP"
            echo "or the URL below (if the log group is in your default region):"
            ENC_LOG_GROUP=$(echo "$LOG_GROUP" | sed 's|/|$252F|g')
            echo "https://console.aws.amazon.com/cloudwatch/home#logsV2:log-groups/log-group/$ENC_LOG_GROUP"

  pre-package-and-push-system:
    parameters:
      check:
        type: boolean
        default: true
    steps:
      - go-get-deps
      - run: make -C proto build
      - when:
          condition: <<parameters.check>>
          steps:
            - run: make -C master check
            - run: make -C agent check

  make-package:
    steps:
      - attach_workspace:
          at: .
      - run:
          no_output_timeout: 30m
          command: make package


  install-devcluster:
    steps:
      - run: pip install git+https://github.com/determined-ai/devcluster.git@v1.1.0#egg=devcluster
      - run:
          command: |
            if ! [ -x "$(command -v socat)" ]; then
              apt update && apt install -y socat
            fi

  start-devcluster:
    parameters:
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
    steps:
      - run:
          command: devcluster --oneshot -c .circleci/devcluster/<<parameters.devcluster-config>> --target-stage <<parameters.target-stage>>
          background: true

jobs:
  build-helm:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - attach_workspace:
          at: .
      - run: make -C helm build
      - persist_to_workspace:
          root: .
          paths:
            - helm/build
      - store_artifacts:
          path: helm/build

  build-docs:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.12.0 torch==1.9.0 torchvision==0.10.0"
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C examples build
      - run: make -C model_hub examples
      - run: make -C helm build
      - attach_workspace:
          at: .
      - run: make -C docs build
      - persist_to_workspace:
          root: .
          paths:
            - examples/build
            - harness/dist
            - model_hub/dist
            - docs/build
            - docs/site
      - run: tar czf docs.tgz docs/site/html
      - store_artifacts:
          path: docs.tgz

  upload-docs-search-index:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.12.0 torch==1.9.0 torchvision==0.10.0"
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C docs upload-search-index

  publish-docs:
    docker:
      - image: hashicorp/terraform:light
    steps:
      - checkout
      - attach_workspace:
          at: .
      - run: apk add make curl python3 py3-pip
      - run: pip3 install awscli
      - run: make -C docs publish

  # TODO(danh): eventually replace 'publish-docs' with this new workflow after
  # all dependent scripts are updated.
  # Related work: https://hpe-aiatscale.atlassian.net/browse/INFENG-183
  publish-docs-new:
    parameters:
      preview:
        type: boolean
        default: true
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-not-docs
      - attach_workspace:
          at: .
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.12.0 torch==1.9.0 torchvision==0.10.0"
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - when:
          condition: <<parameters.preview>>
          steps:
            - run: make -C docs preview

  package-and-push-system-local:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - reinstall-go
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: 20.10.18
      - pre-package-and-push-system:
          check: false
      - make-package
      - run: mkdir -p build/
      - run: docker tag determinedai/determined-master:${CIRCLE_SHA1}-amd64 determinedai/determined-master:${CIRCLE_SHA1}
      - run: docker save -o build/master.image determinedai/determined-master:${CIRCLE_SHA1}
      - run: docker tag determinedai/determined-agent:${CIRCLE_SHA1}-amd64 determinedai/determined-agent:${CIRCLE_SHA1}
      - run: docker save -o build/agent.image determinedai/determined-agent:${CIRCLE_SHA1}
      - persist_to_workspace:
          root: .
          paths:
            - "master/dist/*linux_amd64.deb"
            - "master/dist/*linux_amd64.rpm"
            - "agent/dist/*linux_amd64.deb"
            - "agent/dist/*linux_amd64.rpm"
            - "build/*.image"

  package-and-push-system-dev:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - reinstall-go
      - setup_remote_docker:
          version: 20.10.18
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system:
          check: false
      - make-package
      - run: tools/scripts/retry.sh make -C master publish-dev
      - run: tools/scripts/retry.sh make -C agent publish-dev
      - run:
          name: Build and publish model_hub docker images
          command: |
            if [ ${CIRCLE_BRANCH} = 'master' ] || [[ ${CIRCLE_BRANCH} == *"release-"* ]]; then
                # For master and release branches, we will tag and publish both the environment
                # with the git hash as well as the version.  This will make that image available
                # immediately for nightly tests.
                make -C model_hub build-docker
                tools/scripts/retry.sh make -C model_hub publish-docker
            else
                # Otherwise, only tag and publish the environment with the git hash.
                make -C model_hub build-docker-dev
                tools/scripts/retry.sh make -C model_hub publish-docker-dev
            fi
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  package-and-push-system-rc:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - setup_remote_docker:
          version: 20.10.18
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - login-docker:
          repository: nvcr.io
          username: ${NGC_API_USERNAME}
          password: ${NGC_API_KEY}
      - pre-package-and-push-system
      - make-package
      - run: make -C master publish
      - run: make -C agent publish
      - run: make -C model_hub build-docker
      - run: tools/scripts/retry.sh make -C model_hub publish-docker
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  package-and-push-system-release:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - setup_remote_docker:
          version: 20.10.18
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - login-docker:
          repository: nvcr.io
          username: ${NGC_API_USERNAME}
          password: ${NGC_API_KEY}
      - pre-package-and-push-system
      - run:
          no_output_timeout: 30m
          command: make -C master release
      - run:
          no_output_timeout: 30m
          command: make -C agent release
      - run: make -C model_hub build-docker
      - run: make -C model_hub publish-docker
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  publish-helm:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - helm/install-helm-client
      - run: helm plugin install https://github.com/chartmuseum/helm-push.git
      - login-helm
      - run: make -C helm release

  publish-helm-gh:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - run: make -C helm release-gh

  publish-python-package:
    parameters:
      path:
        type: string
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "build twine"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C <<parameters.path>> build
      - run: make -C <<parameters.path>> publish

  check-ts-bindings:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "bindings/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C bindings force-gen
      - run: make -C bindings check/typescript

  check-py-bindings:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "bindings/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C bindings force-gen
      - run: make -C bindings check/python

  upload-try-now-template:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "awscli"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C harness upload-try-now-template

  test-debian-packaging:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - run: sudo apt-get install -y $(pwd)/master/dist/determined-master*.deb
      - run: sudo apt-get install -y $(pwd)/agent/dist/determined-agent*.deb
      - run: sudo cp .circleci/packaging/master.yaml /etc/determined/master.yaml
      - run: sudo cp .circleci/packaging/agent.yaml /etc/determined/agent.yaml
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - install-devcluster
      - start-devcluster:
          target-stage: db
      - run: python3 .circleci/scripts/wait_for_server.py localhost 5432
      - run: sudo systemctl restart determined-master
      - run: python3 .circleci/scripts/wait_for_server.py localhost 8080 || { journalctl --no-pager -u determined-master; exit 1; }
      - run: sudo systemctl restart determined-agent
      - run: ./.circleci/scripts/sanity.sh

  lint-react:
    docker:
      - image: cimg/node:18.16.0
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - skip-if-github-only
      - react-get-deps
      - run: make -C webui/react check

  build-react:
    parameters:
      dev-mode:
        type: boolean
        default: false
    docker:
      - image: cimg/node:18.16.0
    resource_class: large
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - skip-if-github-only
      - react-get-deps
      - run: |
          if <<parameters.dev-mode>>; then
            echo 'Setting development mode...'
            export DET_NODE_ENV=development
          fi
          make -C webui/react build
      - persist_to_workspace:
          root: .
          paths:
            - webui/react/build

  test-e2e-react:
    docker:
      - image: mcr.microsoft.com/playwright:v1.33.0-focal
    resource_class: large
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - skip-if-github-only
      - run: apt-get update && apt-get -y install make
      - react-get-deps
      - run: npx playwright install --with-deps && npx playwright install chrome
      - run: SERVER_ADDRESS=${PW_SERVER_ADDRESS} npm run build --prefix webui/react
      - run: PW_USER_NAME=${PW_USER_NAME} PW_PASSWORD=${PW_PASSWORD} npm run e2e --prefix webui/react
      - store_artifacts:
          path: webui/react/src/e2e/playwright-report
      - store_artifacts:
          path: webui/react/src/e2e/test-results
      - store_test_results:
          path: webui/react/src/e2e/junit-results.xml

  test-unit-react:
    docker:
      - image: cimg/node:18.16.0
        environment:
          CI: "true"
    resource_class: xlarge
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - skip-if-github-only
      - react-get-deps
      - run: make -C webui/react test-ci
      - codecov/upload:
          flags: "web"
          xtra_args: "-v"
      - store_test_results:
          path: webui/react/coverage/junit.xml
      - store_artifacts:
          path: webui/react/coverage

  test-unit-react-quarantined:
    docker:
      - image: cimg/node:18.16.0
        environment:
          CI: "true"
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - skip-if-github-only
      - react-get-deps
      - run: make -C webui/react test-ci-flaky

  test-unit-react-screenshot:
    docker:
      - image: cimg/node:18.16.0
        environment:
          CI: "true"
    resource_class: xlarge
    steps:
      - checkout-with-sm
      - skip-if-docs-only
      - skip-if-github-only
      - run:
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_PATHS="webui/react/src/components/kit webui/react/src/pages/DesignKit.tsx webui/react/design"
            if git diff --quiet $MERGE_BASE -- $DIFF_PATHS; then
              echo "DesignKit not changed, halting job now."
              circleci-agent step halt
            else
              echo "DesignKit changed, running downstream check."
            fi
          name: check for designkit changes
      - react-get-deps
      - run:
          command: make -C webui/react visual-test
          name: generate screenshots
      - store_artifacts:
          path: webui/react/screenshot-summary.html
      - run:
          command: |
            export ARTIFACT_URL="https://output.circle-artifacts.com/output/job/$CIRCLE_WORKFLOW_JOB_ID/artifacts/0/webui/react/screenshot-summary.html"
            if [ -f webui/react/.diff-detected ] ; then
              export MESSAGE="Hello! DesignKit diffs for commit $CIRCLE_SHA1 are available for you to view [here]($ARTIFACT_URL)"
            else
              export MESSAGE="Hello! No changes detected to DesignKit for commit $CIRCLE_SHA1. Verify [here]($ARTIFACT_URL)"
            fi
            node webui/react/scripts/make-github-comment.mjs "$MESSAGE"
          name: report results

  test-intg-downstream:
    docker:
      - image: cimg/node:18.16.0
    steps:
      - checkout-with-sm
      - skip-if-not-shared
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - run: make -C webui/react test-shared

  lint-sql:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-webui-only
      - setup-python-venv:
          install-python: false
          extras-requires: "sqlfluff"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C master check-sql

  lint-go:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: large
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - reinstall-go
      - go-get-deps
      - run: sudo apt-get update && sudo apt-get install -y clang-format
      - run: make -C proto build
      - run: make -C proto check
      - run: make -C master check
      - run: make -C agent check

  build-go:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - reinstall-go
      - go-get-deps
      - run: make -C proto build
      - run: make -C master build
      - run: make -C agent build
      - persist_to_workspace:
          root: .
          paths:
            - "master/build"
            - "agent/build"

  build-proto:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - reinstall-go
      - go-get-deps
      - run: make -C proto build
      - persist_to_workspace:
          root: .
          paths:
            - "proto/build/**/*"

  test-unit-go:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - reinstall-go
      - go-get-deps
      - run: make -C proto build
      - run: make -C master test
      - run: make -C agent test
      - codecov/upload:
          flags: "backend"
          xtra_args: "-v"
      - store_test_results:
          path: master/test.junit.xml
      - store_test_results:
          path: agent/test.junit.xml

  test-intg-master:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: make -C master test-intg
      - store_test_results:
          path: master/test-intg.junit.xml
      - persist_to_workspace:
          root: .
          paths:
            - master/coverage.out

  test-intg-agent:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: make -C agent test-intg
      - store_test_results:
          path: agent/test-intg.junit.xml
      - persist_to_workspace:
          root: .
          paths:
            - agent/coverage.out

  go-coverage:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - reinstall-go
      - run: cd master && go tool cover -func coverage.out
      - run: cd master && go tool cover -html coverage.out -o coverage.html
      - run: cd agent && go tool cover -func coverage.out
      - run: cd agent && go tool cover -html coverage.out -o coverage.html
      - run: mkdir go-coverage
      - run: mv master/coverage.html go-coverage/master-coverage.html
      - run: mv agent/coverage.html go-coverage/agent-coverage.html
      - store_artifacts:
          path: go-coverage
          destination: go-coverage

  lint-docs:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C docs check

  lint-secrets:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - run: git clone https://github.com/awslabs/git-secrets /tmp/git-secrets
      - run: cd /tmp/git-secrets && sudo make install
      - checkout
      - run: git secrets --install
      - run: git secrets --register-aws
      - run: git secrets --add '"private_key":\s"-----BEGIN\sPRIVATE\sKEY-----'
      - run: git secrets --scan-history

  lint-python:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extras-requires: "torch==1.9.0"
          extra-requirements-file: "requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C harness check
      - run: make -C model_hub check
      - run: make -C e2e_tests check
      - run: make -C examples check
      - run: make -C tools check
      - run: make -C schemas check

  test-unit-harness-cpu:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.12.0 torch==1.9.0 torchvision==0.10.0"
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-cpu-pycov make -C harness test-cpu
      - run: coverage xml -i --data-file=./test-unit-harness-cpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-cpu-pycov

  test-unit-harness-cpu-tf1:
    docker:
      - image: determinedai/environments:py-3.7-pytorch-1.7-tf-1.15-cpu-6eceaca
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - run: pip install mypy pytest coverage
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-cpu-tf1-pycov make -C harness test-cpu-tf1
      - run: coverage xml -i --data-file=./test-unit-harness-cpu-tf1-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness

      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-cpu-tf1-pycov

  test-unit-harness-gpu:
    docker:
      - image: determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-0.24.0
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-pycov make -C harness test-gpu
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-pycov

  test-unit-harness-pytorch2-gpu:
    docker:
      - image: determinedai/environments:cuda-11.8-pytorch-2.0-gpu-2b7e2a1
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-pytorch2-gpu-pycov make -C harness test-pytorch-gpu
      - run: coverage xml -i --data-file=./test-unit-harness-pytorch2-gpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-pytorch2-gpu-pycov

  test-unit-harness-pytorch2-cpu:
    docker:
      - image: determinedai/environments:py-3.10-pytorch-2.0-cpu-2b7e2a1
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - run: pip install mypy pytest coverage
      - install-codecov
      - run: echo 'export PATH=$PATH:$HOME/.local/bin' >> $BASH_ENV
      - run: COVERAGE_FILE=/root/project/test-unit-harness-pytorch2-cpu-pycov make -C harness test-pytorch-cpu
      - run: coverage xml -i --data-file=./test-unit-harness-pytorch2-cpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-pytorch2-cpu-pycov


  test-unit-harness-gpu-parallel:
    docker:
      - image: determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-0.24.0
    resource_class: determined-ai/container-runner-multi-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-parallel-pycov make -C harness test-gpu-parallel
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-parallel-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-parallel-pycov

  test-unit-harness-gpu-deepspeed:
    docker:
      - image: determinedai/environments:cuda-11.3-pytorch-1.10-deepspeed-0.8.3-gpu-0.22.1
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-deepspeed-pycov make -C harness test-gpu-deepspeed
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-deepspeed-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-deepspeed-pycov

  test-unit-harness-tf2:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.12.0 torch==1.9.0"
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-tf2-pycov make -C harness test-tf2
      - run: coverage xml -i --data-file=./test-unit-harness-tf2-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-tf2-pycov

  test-unit-storage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extras-requires: "tensorflow==2.12.0 torch==1.9.0"
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-storage-pycov coverage run -m pytest -v --durations=0 --require-secrets -m cloud harness/tests
      - run: coverage xml -i --data-file=./test-unit-storage-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-storage-pycov

  test-unit-model-hub:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    resource_class: medium+
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extras-requires: "torch==1.9.0 torchvision==0.10.0"
          extra-requirements-file: "model_hub/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-model-hub-pycov make -C model_hub test
      - run: coverage xml -i --data-file=./test-model-hub-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-model-hub-pycov

  python-coverage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - setup-python-venv:
          install-python: false
          determined: false
          model-hub: false
          extras-requires: "coverage"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: coverage combine *-pycov
      - run: coverage report --include 'harness/determined/*' --skip-covered
      - run: coverage report --include 'model_hub/model_hub/*' --skip-covered
      - run: coverage html --include 'harness/determined/*' --skip-covered -d cov-html/harness
      - run: coverage html --include 'model_hub/model_hub/*' --skip-covered -d cov-html/model_hub
      - store_artifacts:
          path: cov-html
          destination: cov-html

  test-examples:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    resource_class: medium+
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "examples/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: pytest -vv -s --junit-xml="/tmp/test-results/examples.xml" --durations=0 examples/tests
      - upload-test-job:
          only_on_branch: main
          test_results_path: /tmp/test-results/examples.xml

  test-cli:
    parameters:
      executor-name:
        type: string
    executor: << parameters.executor-name >>
    steps:
      - checkout
      - run: python --version
      # Running the pip executable causes an error with the win/default executor for some reason.
      - run: python -m pip install --upgrade --user pip
      - run: pip --version
      - run: pip install wheel
      - run: cd harness; python setup.py bdist_wheel -d ../build
      - run: pip install --find-links build determined==<< pipeline.parameters.det-version >>
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"
      # Ensure Determined cli can run without installing cli test requirements
      - run: det --help
      - run: pip install setuptools_scm
      - run: pip install -r harness/tests/requirements/requirements-cli.txt
      - run: pip freeze --all
      - run: sh -c "pip check || true"
      - run: pytest harness/tests/cli

  test-e2e:
    parameters:
      tf1:
        type: boolean
        default: false
      tf2:
        type: boolean
        default: false
      mark:
        type: string
      parallelism:
        type: integer
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
      managed-devcluster:
        type: boolean
        default: false
      postgres-version:
        type: string
        default: "10"
      agent-version:
        type: string
        default: ""
      extra-pytest-flags:
        type: string
        default: ""
      wait-for-master:
        type: boolean
        default: true
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    parallelism: <<parameters.parallelism>>
    environment:
      DET_POSTGRES_VERSION: <<parameters.postgres-version>>
      DET_AGENT_VERSION: <<parameters.agent-version>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .

      - reinstall-go

      - setup-python-venv:
          determined: True
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - run:
          name: Get master dependencies
          command: make -C master get-deps

      - install-devcluster
      - unless:
          condition: <<parameters.managed-devcluster>>
          steps:
            - start-devcluster:
                devcluster-config: <<parameters.devcluster-config>>
                target-stage: <<parameters.target-stage>>

      - pull-task-images:
          tf1: <<parameters.tf1>>
          tf2: <<parameters.tf2>>

      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: localhost
          managed-devcluster: <<parameters.managed-devcluster>>
          extra-pytest-flags: <<parameters.extra-pytest-flags>>
          wait-for-master: <<parameters.wait-for-master>>

      - store_test_results:
          path: /tmp/test-results/

      - when:
          condition: <<parameters.managed-devcluster>>
          steps:
            - store_artifacts:
                path: /tmp/devcluster/
                destination: devcluster-logs
            - store_artifacts:
                path: /tmp/priority_scheduler
                destination: devcluster-priority_scheduler-logs

  deploy:
    parameters:
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      cluster-id:
        type: string
        default: determined-${CIRCLE_BRANCH////--}
      max-dynamic-agents:
        type: integer
        default: 1
      enable-cors:
        type: boolean
        default: false
      reattach-enabled:
        type: boolean
        default: false
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - queue/until_front_of_line:
          only-on-branch: main
          # Basically wait forever -- we would prefer not to fail deploys, and
          # we'll likely never be this backed up.
          time: "10000"
      - checkout
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          executor: <<pipeline.parameters.docker-image>>
      - deploy-aws-cluster:
          cluster-id: <<parameters.cluster-id>>
          det-version: ${CIRCLE_SHA1}
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          enable-cors: <<parameters.enable-cors>>
          reattach-enabled: <<parameters.reattach-enabled>>
          deployment-type: simple
      - slack/status:
          fail_only: true
          failure_message: ':thisisfine: A \`${CIRCLE_JOB}\` job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: "${SLACK_USER_ID}"

  test-e2e-aws:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      compute-agent-instance-type:
        type: string
        default: g4dn.xlarge
      aux-agent-instance-type:
        type: string
        default: m5.large
      max-dynamic-agents:
        type: integer
        default: 1
      parallelism:
        type: integer
        default: 1
      enable-tls:
        type: boolean
        default: false
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      environment-gpu-enabled:
        type: string
        default: "1"
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - when:
          condition: <<parameters.enable-tls>>
          steps:
            - generate-tls-cert
      - setup-aws-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          extra-tags: test-mark=<<parameters.mark>>
          det-version: ${CIRCLE_SHA1}
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
          master-scheme: ${MASTER_SCHEME:-http}
          master-port: ${MASTER_PORT:-8080}
          master-cert: ${MASTER_TLS_CERT}
          master-cert-name: ${MASTER_CERT_NAME}
          wait-for-master: false
      - locate-cloudwatch-logs:
          cluster-id: ${CLUSTER_ID}
      - terminate-aws-cluster:
          cluster-id: ${CLUSTER_ID}
      - store_test_results:
          path: /tmp/test-results/
      - slack/status:
          fail_only: True
          only_for_branches: main
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E GPU job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>

  test-e2e-gke:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      gke-version:
        type: string
        default: <<pipeline.parameters.gke-version>>
      machine-type:
        type: string
        default: "n1-standard-8"
      num-machines:
        type: integer
        default: 1
      gpu-type:
        type: string
        default: "nvidia-tesla-t4"
      gpus-per-machine:
        type: integer
        default: 1
      slot-type:
        type: string
        default: gpu
      slot-resource-requests-cpu:
        type: integer
        default: 0
      region:
        type: string
        default: "us-west1"
      node-locations:
        type: string
        default: "us-west1-b"
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      environment-gpu-enabled:
        type: string
        default: "1"
      environment-image:
        default: determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-0.24.0
        type: string
      accel-node-taints:
        type: string
        default: ""
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - queue/until_front_of_line:
          only-on-branch: main
          # Basically wait forever -- we would prefer not to fail tests, and
          # we'll likely never be this backed up.
          time: "10000"
      - setup-gke-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          labels: test-mark=<<parameters.mark>>
          det-version: ${CIRCLE_SHA1}
          gke-version: <<parameters.gke-version>>
          machine-type: <<parameters.machine-type>>
          num-machines: <<parameters.num-machines>>
          gpu-type: <<parameters.gpu-type>>
          gpus-per-machine: <<parameters.gpus-per-machine>>
          slot-type: <<parameters.slot-type>>
          slot-resource-requests-cpu: <<parameters.slot-resource-requests-cpu>>
          region: <<parameters.region>>
          node-locations: <<parameters.node-locations>>
          environment-image: <<parameters.environment-image>>
          accel-node-taints: <<parameters.accel-node-taints>>
      - set-google-application-credentials
      - when:
          condition: <<parameters.environment-image>>
          steps:
            - run:
                command: |
                  echo 'export TF1_GPU_IMAGE="<<parameters.environment-image>>"' >> "$BASH_ENV"
                  echo 'export TF2_GPU_IMAGE="<<parameters.environment-image>>"' >> "$BASH_ENV"
                name: override test env images.
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
      - terminate-gke-cluster:
          cluster-id: ${CLUSTER_ID}
          region: <<parameters.region>>
      - slack/status:
          fail_only: True
          only_for_branches: main
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E GKE GPU job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>

  test-det-deploy:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - run: docker load --input build/master.image
      - run: docker load --input build/agent.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - pull-task-images:
          tf1: True

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  test-stress:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - attach_workspace:
          at: .
      - run: docker load --input build/master.image
      - run: docker load --input build/agent.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - pull-task-images:
          tf1: True

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  test-aws-fs:
    parameters:
      cluster-id-prefix:
        type: string
      deployment-type:
        type: string
      compute-agent-instance-type:
        type: string
        default: g4dn.xlarge
      aux-agent-instance-type:
        type: string
        default: m5.large
      max-dynamic-agents:
        type: integer
        default: 1
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - skip-if-docs-only
      - skip-if-github-only
      - skip-if-webui-only
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - setup-aws-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          extra-tags: test-mark=fs
          det-version: ${CIRCLE_SHA1}
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
          deployment-type: <<parameters.deployment-type>>
      - run:
          name: Set DET_MASTER for further action.
          command: |
            echo "export DET_MASTER=${MASTER_SCHEME:-http}://${MASTER_HOST}:${MASTER_PORT:-8080}" >> "$BASH_ENV"
      - when:
          condition:
            equal: [<<parameters.deployment-type>>, "fsx"]
          steps:
            - run:
                name: Test FSx/Lustre
                command: |
                  det cmd run --config resources.slots=1 'mount | grep lustre && echo MARKER' | grep MARKER
      - when:
          condition:
            equal: [<<parameters.deployment-type>>, "efs"]
          steps:
            - run:
                name: Test EFS
                command: |
                  det cmd run --config resources.slots=1 'mount | grep efs && echo MARKER' | grep MARKER
      - locate-cloudwatch-logs:
          cluster-id: ${CLUSTER_ID}
      - terminate-aws-cluster:
          cluster-id: ${CLUSTER_ID}
      - slack/status:
          fail_only: True
          only_for_branches: main
          failure_message: ':thisisfine: AWS FS \`<<parameters.deployment-type>>\` job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>

workflows:
  lint:
    jobs:
      - build-proto
      - check-py-bindings:
          requires:
            - build-proto
      - check-ts-bindings:
          requires:
            - build-proto
      - lint-docs
      - lint-python
      - lint-go
      - lint-sql
      - lint-react
      - lint-secrets

  previews:
    jobs:
      - build-proto
      - build-helm
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - publish-docs-new:
          requires:
            - build-docs
          context: aws
          filters: *any-upstream

  test-cli:
    jobs:
      - test-cli:
          matrix:
            parameters:
              executor-name:
                ["python-37", "python-38", "python-39", "win/default"]

  test-unit:
    jobs:
      - test-unit-go
      - test-unit-react
      - test-unit-react-screenshot:
          context: github-write
          filters: *any-upstream
      - test-unit-harness-cpu
      - test-unit-harness-tf2
      - test-unit-harness-pytorch2-cpu
      - test-unit-harness-pytorch2-gpu

      - test-unit-harness-cpu-tf1:
          filters: *any-upstream
      - test-unit-harness-gpu:
          filters: *any-upstream
      - test-unit-harness-gpu-deepspeed:
          filters: *any-upstream
      - test-unit-harness-gpu-parallel:
          filters: *any-upstream

      # allows forks to request run approval instead of failing
      - request-gpu-unit-tests:
          type: approval
          filters: *any-fork
      - test-unit-harness-cpu-tf1:
          name: f-test-unit-harness-cpu-tf1
          filters: *any-fork
      - test-unit-harness-gpu:
          name: f-test-unit-harness-gpu
          filters: *any-fork
      - test-unit-harness-gpu-deepspeed:
          name: f-test-unit-harness-gpu-deepspeed
          filters: *any-fork
      - test-unit-harness-gpu-parallel:
          name: f-test-unit-harness-gpu-parallel
          filters: *any-fork

      - test-unit-model-hub
      - test-unit-storage:
          context: storage-unit-tests
          filters: *any-upstream
      - test-examples
      - python-coverage:
          requires:
            - test-unit-harness-cpu
            - test-unit-harness-gpu
            - test-unit-harness-gpu-parallel
            - test-unit-harness-tf2
            - test-unit-harness-pytorch2-cpu
            - test-unit-harness-pytorch2-gpu
            - test-unit-model-hub
            - test-unit-storage

  test-intg:
    jobs:
      # https://determined-ai.slack.com/archives/C03H5KZPU30/p1667241463073989
      # - test-intg-downstream
      - test-intg-master:
          context: storage-unit-tests
      - test-intg-agent
      - go-coverage:
          requires:
            - test-intg-master
            - test-intg-agent

  test-e2e:
    jobs:
      - build-proto
      - build-helm
      - build-react:
          dev-mode: true
      - test-e2e-react:
          context: playwright
          filters: *any-upstream
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - build-go
      - package-and-push-system-local:
          requires:
            - build-react
            - build-docs

      - package-and-push-system-dev:
          requires:
            - build-react
            - build-docs
          filters: *any-upstream

      - upload-docs-search-index:
          requires:
            - build-docs
          context: determined-production
          filters:
            branches:
              only:
                - main

      - test-debian-packaging:
          requires:
            - package-and-push-system-local

      - test-e2e:
          name: test-e2e-cpu
          requires:
            - build-go
          parallelism: 4
          tf1: true
          tf2: true
          mark: e2e_cpu

      - test-e2e:
          name: test-e2e-cpu-double
          requires:
            - build-go
          parallelism: 1
          tf1: false
          tf2: true
          mark: e2e_cpu_2a
          target-stage: agent2
          devcluster-config: double-priority.devcluster.yaml

      - test-e2e:
          name: test-e2e-managed-devcluster
          requires:
            - build-go
          parallelism: 2
          tf1: false
          tf2: true
          mark: managed_devcluster
          managed-devcluster: true
          # Managed devcluster restarts the master over the course of the tests,
          # so `compare_stats` cannot get the full logs from `det master logs`.
          extra-pytest-flags: "--no-compare-stats"

      - test-e2e:
          name: test-e2e-port-registry
          requires:
            - build-go
          parallelism: 1
          devcluster-config: port-registry.devcluster.yaml
          mark: port_registry
          target-stage: agent

      - test-e2e:
          name: test-e2e-cpu-elastic
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_elastic
          devcluster-config: elastic.devcluster.yaml
          target-stage: agent

      - test-e2e:
          name: test-e2e-postgres10-with-ssl
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: "10"

      - test-e2e:
          name: test-e2e-postgres14-with-ssl
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: "14"

      - test-e2e:
          name: test-e2e-old-agent-versions
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: custom-agent-version.devcluster.yaml
          target-stage: agent
          matrix:
            parameters:
              agent-version: ["0.17.10"]

      - test-e2e:
          name: test-e2e-agent-connection-loss
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_agent_connection_loss
          devcluster-config: agent-no-connection.devcluster.yaml
          target-stage: agent
          wait-for-master: false

      - deploy:
          name: deploy-latest-master-cluster
          enable-cors: true
          reattach-enabled: true
          context: aws
          filters:
            branches:
              only:
                - main
          requires:
            - package-and-push-system-dev
          max-dynamic-agents: 2

      - deploy:
          name: deploy-preview-cluster
          enable-cors: true
          context: aws
          filters:
            branches:
              only:
                - main
          requires:
            - package-and-push-system-dev
          max-dynamic-agents: 1
          cluster-id: determined-preview
          compute-agent-instance-type: t2.medium

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context: aws
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context: aws
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context: gcp-ci
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context: gcp-ci
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]

      # Legacy tf 2.4 tests.
      - test-e2e-gke:
          name: test-e2e-gke-single-gpu-tfonly
          context: gcp-ci
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu-tfonly"]
              mark: ["e2e_gpu and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.10

      - test-e2e-gke:
          name: test-e2e-gke-parallel-tfonly
          context: gcp-ci
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel-tfonly"]
              mark: ["parallel and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.10

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context: gcp-ci
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]

      - test-e2e-gke:
          name: test-e2e-gke-k8s-reattach
          context: gcp-ci
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-k8s"]
              mark: ["e2e_k8s"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              accel-node-taints: ["accel=truth:NoSchedule"]

  test-e2e-quarantined:
    jobs:
      - build-proto
      - build-helm
      - build-react:
          dev-mode: true
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - package-and-push-system-local:
          requires:
            - build-react
            - build-docs

      - test-stress:
          name: test-stress
          filters: *any-upstream
          requires:
            - package-and-push-system-local
          matrix:
            parameters:
              parallelism: [1]
              mark: ["stress_test"]
              det-version: [$CIRCLE_SHA1]

      - test-det-deploy:
          name: test-det-deploy-local
          requires:
            - package-and-push-system-local
          matrix:
            parameters:
              parallelism: [2]
              mark: ["det_deploy_local"]
              det-version: [$CIRCLE_SHA1]

      - test-unit-react-quarantined

  test-e2e-longrunning:
    jobs:
      # Build and publish artifacts used in tests
      - build-helm:
          filters: *any-upstream
      - build-proto:
          filters: *any-upstream
      - build-react:
          dev-mode: true
          filters: *any-upstream
      - build-docs:
          filters: *any-upstream
          requires:
            - build-helm
            - build-proto
      - package-and-push-system-dev:
          requires:
            - build-react
            - build-docs
          filters: *any-upstream

      # Distributed tests
      - request-e2e-cpu-distributed:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-e2e-cpu-distributed
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [16]

      # DeepSpeed tests
      - request-deepspeed-tests:
          type: approval
          filters: *upstream-feature-branch

        # DeepSpeed tests do not work on K80s, so we need V100 instances.
      - test-e2e-aws:
          name: test-e2e-deepspeed
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-deepspeed-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["g4dn.12xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

      # K8s GPU tests
      - request-k8-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context: gcp-ci
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context: gcp-ci
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]

      # Requestable legacy tf 2.4 tests.
      - test-e2e-gke:
          name: test-e2e-gke-single-gpu-tfonly
          context: gcp-ci
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu-tfonly"]
              mark: ["e2e_gpu and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.10

      - test-e2e-gke:
          name: test-e2e-gke-parallel-tfonly
          context: gcp-ci
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel-tfonly"]
              mark: ["parallel and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.10

      # K8s CPU tests
      - request-k8-tests-cpu:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context: gcp-ci
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cpu
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]

      - test-e2e-gke:
          name: test-e2e-gke-k8s-reattach
          context: gcp-ci
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cpu
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-k8s"]
              mark: ["e2e_k8s"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              accel-node-taints: ["accel=truth:NoSchedule"]

      # Nightly distributed tests
      - request-gpu-distributed-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-distributed-nightly
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-distributed-nightly
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]

      # Nightly tests
      - request-gpu-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-nightly
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]

      # GPU tests
      - request-gpu-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              slack-mentions: ["${SLACK_USER_ID}"]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]
              slack-mentions: ["${SLACK_USER_ID}"]

      # mmdetection tests
      - request-mmdetection-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-mmdetection
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-mmdetection-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["mmdetection"]
              mark: ["model_hub_mmdetection"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

      # Transformers tests
      - request-transformers-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-transformers
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-transformers-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["transformers"]
              mark: ["model_hub_transformers"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

      - test-e2e-aws:
          name: test-e2e-transformers-amp
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-transformers-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["transformers-amp"]
              mark: ["model_hub_transformers_amp"]
              slack-mentions: ["${SLACK_USER_ID}"]
              max-dynamic-agents: [2]

      # Request AWS FS tests
      - request-aws-fs-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-aws-fs:
          name: test-aws-fs-fsx
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-aws-fs-tests
            - package-and-push-system-dev
          cluster-id-prefix: aws-fs-fsx
          deployment-type: fsx
          slack-mentions: "${SLACK_USER_ID}"

      - test-aws-fs:
          name: test-aws-fs-efs
          context: aws
          filters: *upstream-feature-branch
          requires:
            - request-aws-fs-tests
            - package-and-push-system-dev
          cluster-id-prefix: aws-fs-efs
          deployment-type: efs
          slack-mentions: "${SLACK_USER_ID}"

  nightly:
    triggers:
      - schedule:
          cron: "0 5 * * *"
          filters:
            branches:
              only:
                - main
    jobs:
      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context: aws
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]
      - test-e2e-aws:
          name: test-e2e-gpu-distributed
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-transformers
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["transformers"]
              mark: ["model_hub_transformers"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-transformers-amp
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["transformers-amp"]
              mark: ["model_hub_transformers_amp"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-mmdetection
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["mmdet"]
              mark: ["model_hub_mmdetection"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-mmdetection-quarantine
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["mmdet-quarantine"]
              mark: ["model_hub_mmdetection_quarantine"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
        # DeepSpeed tests do not work on K80s, so we need V100 instances.
      - test-e2e-aws:
          name: test-e2e-gpu-deepspeed
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              compute-agent-instance-type: ["g4dn.12xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context: aws
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [16]
      - test-aws-fs:
          name: test-aws-fs-fsx
          context: aws
          cluster-id-prefix: aws-fs-fsx
          deployment-type: fsx
          slack-mentions: "${SLACK_USER_ID}"

      - test-aws-fs:
          name: test-aws-fs-efs
          context: aws
          cluster-id-prefix: aws-fs-efs
          deployment-type: efs
          slack-mentions: "${SLACK_USER_ID}"

  release:
    jobs:
      - build-helm:
          filters: *release-and-rc-filters
      - build-proto:
          filters: *release-and-rc-filters
      - build-react:
          context: determined-production
          filters: *release-and-rc-filters
      - build-docs:
          context: determined-production
          filters: *release-and-rc-filters
          requires:
            - build-helm
            - build-proto

      - upload-docs-search-index:
          requires:
            - build-docs
          context: determined-production
          filters: *release-and-rc-filters

      - package-and-push-system-rc:
          requires:
            - build-react
            - build-docs
          context: determined-production
          filters: *rc-filters

      - publish-python-package:
          name: publish-python-package-rc
          matrix:
            parameters:
              path: ["harness", "model_hub"]
          context: determined-production
          filters: *rc-filters
          requires:
            - package-and-push-system-rc

      - package-and-push-system-release:
          requires:
            - build-react
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-python-package:
          name: publish-python-package-release
          matrix:
            parameters:
              path: ["harness", "model_hub"]
          context: determined-production
          filters: *release-filters
          requires:
            - package-and-push-system-release

      - publish-docs:
          requires:
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-helm:
          requires:
            - build-helm
          context: determined-production
          filters: *release-and-rc-filters

      - publish-helm-gh:
          requires:
            - build-helm
          context: determined-production
          filters: *release-filters

      - upload-try-now-template:
          context: determined-production
          filters: *release-filters
