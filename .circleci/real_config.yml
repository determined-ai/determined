# Use the latest 2.1 version of CircleCI pipeline process engine. See: https://circleci.com/docs/2.0/configuration-reference
version: 2.1

orbs:
  codecov: codecov/codecov@3.3.0
  gcloud: circleci/gcp-cli@2.1.0
  gh: circleci/github-cli@2.0
  helm: circleci/helm@1.2.0
  kubernetes: circleci/kubernetes@0.11.0
  queue: eddiewebb/queue@2.2.1
  slack: circleci/slack@3.4.2
  win: circleci/windows@5.0.0

executors:
  python-38:
    docker:
      - image: python:3.8-slim-bookworm
  python-39:
    docker:
      - image: python:3.9-slim-bookworm

parameters:
  det-version:
    type: string
    default: 0.31.0-dev0
  docker-image:
    type: string
    default: determinedai/cimg-base:latest
  machine-image:
    type: string
    default: ubuntu-2004:2024.01.1
  gpu-machine-image:
    type: string
    default: ubuntu-2004-cuda-11.2:202103-01
  # DEFAULT_PT_GPU_IMAGE: Pytorch training image reference used by the tests
  # Inject here as a parameter so that it is updated by bumpversion, and can
  # be referenced by --ee testing.
  default-pt-gpu-image:
    type: string
    default: determinedai/environments:cuda-11.3-pytorch-1.12-gpu-079eb6d
  # Some python, go, and react dependencies are cached by circleci via `save_cache`/`restore_cache`.
  # If the dependencies stay the same, but the circleci code that would produce them is changed,
  # it may be necessary to invalidate the cache by incrementing this value.
  # For example, if you change an env variable affecting a build of a python package with the same version,
  # the old build may be cached, and you may need to invalidate it.
  cache-buster:
    type: string
    default: v1dev25
  gke-version:
    type: string
    default: "1.29"
  master-build-resource-class:
    type: string
    default: xlarge
  default-slack-channel:
    type: string
    default: C04PTNKLR8W # ci-bots-ee
  do_nightly_tests:
    type: boolean
    default: false

release-and-rc-filters: &release-and-rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+-ee/
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+-ee/

rc-filters: &rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+-ee/

release-filters: &release-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+-ee/

upstream-feature-branch: &upstream-feature-branch
  branches:
    ignore:
      - /pull\/.*/
      - /release-.*/
      - main

any-upstream: &any-upstream
  branches:
    ignore:
      - /pull\/.*/

any-fork: &any-fork
  branches:
    only:
      - /pull\/.*/

commands:
  fix-circle-working-directory:
    description: "Fix CIRCLE_WORKING_DIRECTORY"
    steps:
      - run: echo 'CIRCLE_WORKING_DIRECTORY="${CIRCLE_WORKING_DIRECTORY/#\~/$HOME}"' >> $BASH_ENV

  # circleci's checkout does not fetch submodules by default
  # https://circleci.com/docs/2.0/configuration-reference/#checkout
  checkout-with-sm:
    steps:
      - checkout
      - run: git submodule sync
      - run: git submodule update --init

  add-and-fetch-upstream:
    steps:
      - run: git remote add upstream https://github.com/determined-ai/determined-ee
      - run: tools/scripts/retry.sh git fetch upstream

  skip-if-only-dir:
    parameters:
      dir:
        type: string
    steps:
      - run:
          name: skip if << parameters.dir >>-only changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            if [ "$DIFF_DIRS" = "<< parameters.dir >>" ] ; then
              echo "<< parameters.dir >>-only change detected, halting job now"
              circleci-agent step halt
            else
              echo "detected non-<< parameters.dir >>-only changes: $DIFF_DIRS"
            fi

  skip-if-not-dir:
    parameters:
      dir:
        type: string
    steps:
      - run:
          name: continue if << parameters.dir >> changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            if ! git diff --quiet "$MERGE_BASE" HEAD -- "<< parameters.dir >>"; then
              echo "<< parameters.dir >> change detected, running downstream steps"
            else
              echo "<< parameters.dir >> code not affected, halting job now"
              circleci-agent step halt
            fi

  skip-if-only-docs:
    steps:
      - skip-if-only-dir:
          dir: docs

  skip-if-not-docs:
    steps:
      - skip-if-not-dir:
          dir: docs

  skip-if-only-webui:
    steps:
      - skip-if-only-dir:
          dir: webui

  skip-if-only-github:
    steps:
      - skip-if-only-dir:
          dir: .github

  skip-on-dev-branch:
    steps:
      # Install the GitHub CLI.
      - gh/install
      - run:
          name: Check if extra GCP tests specfied
          command: |
            # Check if the 'ci-run-allgcp' label was set on the pull request,
            # which indicates that we want the GCP tests to run on demand.
            if gh pr view --json labels -q '.labels.[] | .name' | grep -w ci-run-allgcp
            then
              echo "The 'ci-run-allgcp' label was found in github pull request. Proceeding to run test-e2e-slurm-*-gcp tests."
            else
              echo "The 'ci-run-allgcp' label was not found in github pull request. Skipping job."
              circleci-agent step halt
            fi

  set-slack-user-id:
    steps:
      - run:
          name: Set Slack variables
          command: |
            if ! [ -x "$(command -v jq)" ]; then
              apt update && apt install -y jq
            fi

            AUTHOR_EMAIL="$(git show -s --format='%ae' $CIRCLE_SHA1)"
            echo "export AUTHOR_EMAIL=\"${AUTHOR_EMAIL}\"" >> $BASH_ENV
            LOOKUP_RESPONSE=$(curl -s "https://slack.com/api/users.lookupByEmail?token=${SLACK_API_TOKEN}&email=${AUTHOR_EMAIL}")
            SUCCESS=$(echo "$LOOKUP_RESPONSE" | jq ".ok")
            if [[ "$SUCCESS" == "true" ]]; then
              SLACK_USER_ID=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.id")
              SLACK_NAME=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.name")
              echo "export SLACK_NAME=\"${SLACK_NAME}\"" >> $BASH_ENV
              echo "export SLACK_USER_ID=\"${SLACK_USER_ID}\"" >> $BASH_ENV
            else
              echo "Unable to find Slack user ID for  \"${AUTHOR_EMAIL}\"."
            fi

  license-gen:
    steps:
      - run: cd .. && git clone https://${GITHUB_TOKEN}@github.com/determined-ai/license-key-gen.git
      - run: cd ../license-key-gen && go run ./licensegen.go
      - run: mv ../license-key-gen/license.txt ./license.txt
      - run: mv ../license-key-gen/public.txt ./public.txt

  pull-task-images:
    parameters:
      tf2:
        type: boolean
        default: false
    steps:
      - when:
          condition: <<parameters.tf2>>
          steps:
            - run: docker pull determinedai/environments:py-3.9-pytorch-1.12-tf-2.11-cpu-079eb6d

  login-docker:
    parameters:
      repository:
        type: string
        default: ""
      username:
        type: string
      password:
        type: string
    steps:
      - run: echo "<<parameters.password>>" | docker login <<parameters.repository>> -u "<<parameters.username>>" --password-stdin

  login-helm:
    steps:
      - run: helm repo add determined https://helm.ngc.nvidia.com/isv-ngc-partner/determined --username=$NGC_API_USERNAME --password=$NGC_API_KEY

  reinstall-go:
    steps:
      - run: sudo rm -rf /usr/local/go # Remove system go.
      - run: tools/scripts/retry.sh curl --retry-connrefused --retry 10 https://dl.google.com/go/go1.22.0.linux-amd64.tar.gz -o /tmp/go.linux-amd64.tar.gz
      - run: sudo tar -C /usr/local -xzf /tmp/go.linux-amd64.tar.gz
      - run: echo 'export PATH=$PATH:$HOME/go/bin' >> $BASH_ENV

  install-protoc:
    steps:
      - run: curl --retry-connrefused --retry 10 -o /tmp/protoc.zip -L https://github.com/protocolbuffers/protobuf/releases/download/v3.20.3/protoc-3.20.3-linux-x86_64.zip
      - run: unzip -o /tmp/protoc.zip -d $HOME/.local

  install-codecov:
    steps:
      - run: mkdir -p $HOME/.local/bin
      - run: curl --output $HOME/.local/bin/codecov https://uploader.codecov.io/latest/linux/codecov
      - run: chmod +x $HOME/.local/bin/codecov
      - run: echo 'export PATH=$PATH:$HOME/.local/bin/codecov' >> $BASH_ENV

  setup-go-intg-deps:
    steps:
      - install-protoc # Install newer version of protoc into $HOME/.local/bin, since default is proto2.
      - run: PATH=$HOME/.local/bin:$PATH make -C proto get-deps
      - run: PATH=$HOME/.local/bin:$PATH make -C proto build
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - install-devcluster
      - start-devcluster:
          target-stage: elastic
          devcluster-config: elastic-base.devcluster.yaml

  go-get-deps: # Build Go doesn't use this because it doesn't need it.
    steps:
      - install-protoc
      - restore_cache:
          keys:
            - det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}-{{ checksum  "master/get-deps.sh" }}-{{ checksum  "agent/get-deps.sh" }}-{{ checksum  "proto/get-deps.sh" }}
      - run: make -C proto get-deps
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - save_cache:
          key: det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}-{{ checksum  "master/get-deps.sh" }}-{{ checksum  "agent/get-deps.sh" }}-{{ checksum  "proto/get-deps.sh" }}
          paths:
            - "/home/circleci/go/"
            - "/home/circleci/.cache/go-build/"
  react-get-deps:
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Write cache suffix
          command: |
            echo '<<pipeline.parameters.cache-buster>>' > /tmp/react-get-deps.cachekey
            cat webui/react/package-lock.json >> /tmp/react-get-deps.cachekey
            echo $HOME >> /tmp/react-get-deps.cachekey
            cat /tmp/react-get-deps.cachekey
      - restore_cache:
          keys:
            - det-react-deps-{{ checksum "/tmp/react-get-deps.cachekey" }}
      - run:
          name: Get React dependencies
          command: |
            make -C webui/react node_modules/done.stamp
      - save_cache:
          key: det-react-deps-{{ checksum "/tmp/react-get-deps.cachekey" }}
          paths:
            - "webui/react/node_modules"

  install-wheel:
    parameters:
      package-name:
        type: string
      package-location:
        type: string
    steps:
      - run:
          name: Install <<parameters.package-name>>
          working_directory: <<parameters.package-location>>
          command: |
            make build
            pip install --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>
            pip install --no-deps --force-reinstall --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>

  setup-python-venv:
    description: Set up and create Python venv.
    parameters:
      determined:
        type: boolean
        default: false
      model-hub:
        type: boolean
        default: false
      install-python:
        type: boolean
        default: true
      extras-requires:
        type: string
        default: ""
      extra-requirements-file:
        type: string
        default: ""
      executor:
        type: string
      python-version:
        type: string
        default: "3.8.18"
    steps:
      - run:
          name: Write cache key
          command: |
            echo <<parameters.executor>> > /tmp/cachefile
            pip freeze --all >> /tmp/cachefile
            if [ "<<parameters.determined>>" = "true" ]; then
              cat harness/setup.py >> /tmp/cachefile
            fi
            if [ "<<parameters.model-hub>>" = "true" ]; then
              cat model_hub/setup.py >> /tmp/cachefile
            fi
            echo <<parameters.extras-requires>> >> /tmp/cachefile
            if [ -n <<parameters.extra-requirements-file>> ]; then
              for i in <<parameters.extra-requirements-file>>; do
                cat $i >> /tmp/cachefile
              done
            fi
            echo '<<parameters.python-version>>' >> /tmp/cachefile
            echo '<<parameters.install-python>>' >> /tmp/cachefile
            date -u '+%y/%m/%d' >> /tmp/cachefile

      - restore_cache:
          keys:
            - det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
      - when:
          condition: <<parameters.install-python>>
          steps:
            - run:
                name: prepare conda PATH
                command: |
                  echo 'export PATH=/tmp/conda/bin:${PATH}' >> $BASH_ENV

            - run:
                name: prepare conda install_python.sh
                command: |
                  cat \<<'EOF' > /tmp/install_python.sh
                  #!/bin/bash
                  PYTHON_VERSION="${1}"

                  CONDA_DIR="/tmp/conda"
                  CONDA_INSTALLER="Miniconda3-py39_23.5.2-0-Linux-x86_64.sh"
                  CONDA_SHA256="9829d95f639bd0053b2ed06d1204e60644617bf37dd5cc57523732e0e8d64516"
                  CONDA_URL="https://repo.anaconda.com/miniconda"

                  ${CONDA_DIR}/bin/python --version
                  installed_python_version=$(${CONDA_DIR}/bin/python --version 2>&1 | awk '{print $2}')
                  if [ -x "${CONDA_DIR}/bin/python" ] && [ "$installed_python_version" = "$PYTHON_VERSION" ]; then
                      echo "Python already installed, version $PYTHON_VERSION"
                      exit 0
                  fi
                  echo "Python not installed or wrong version, installing conda and Python now"

                  mkdir -p /etc/determined/conda.d
                  mkdir -p "${CONDA_DIR}"

                  cd /tmp
                  curl --retry 3 -fsSL -O "${CONDA_URL}/${CONDA_INSTALLER}"
                  echo "${CONDA_SHA256}  ${CONDA_INSTALLER}" | sha256sum ${CONDA_INSTALLER}
                  bash "./${CONDA_INSTALLER}" -u -b -p "${CONDA_DIR}"
                  rm -f "./${CONDA_INSTALLER}"

                  ${CONDA_DIR}/bin/conda install python=${PYTHON_VERSION}
                  ${CONDA_DIR}/bin/conda update --prefix ${CONDA_DIR} --all -y
                  ${CONDA_DIR}/bin/conda clean --all -y
                  EOF

            - run:
                name: run conda install_python.sh
                command: |
                  sudo bash /tmp/install_python.sh <<parameters.python-version>>

      - run:
          name: Setup venv
          command: |
            python3 -m venv /tmp/venv
            echo 'export PATH=/tmp/venv/bin:$PATH' >> $BASH_ENV
            /tmp/venv/bin/python -m pip install --upgrade pip wheel setuptools

      # Either of make -C {harness,model_hub} build require pypa's build module.
      - when:
          condition:
            or:
              - <<parameters.determined>>
              - <<parameters.model-hub>>
          steps:
            - run:
                name: Install pypa builder
                command: python3 -m pip install build

      - when:
          condition: <<parameters.determined>>
          steps:
            - install-wheel:
                package-name: determined
                package-location: ./harness
      - when:
          condition: <<parameters.model-hub>>
          steps:
            - run:
                name: Install mmdetection dependencies
                command: |
                  sudo apt-get update
                  sudo apt-get install -y ffmpeg libsm6 libxext6
            - install-wheel:
                package-name: model-hub
                package-location: ./model_hub
      - run:
          name: Install <<parameters.extras-requires>>
          command: |
            if [ -n "<<parameters.extras-requires>>" ]; then
              tools/scripts/retry.sh pip install --progress-bar off <<parameters.extras-requires>>
            fi
      - run:
          name: Install <<parameters.extra-requirements-file>>
          command: |
            if [ -n "<<parameters.extra-requirements-file>>" ]; then
              for i in <<parameters.extra-requirements-file>>; do
                tools/scripts/retry.sh pip install -r $i
              done
            fi
      - save_cache:
          key: det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
          paths:
            - "/tmp/venv"
            - "/tmp/conda"
      - run: python --version
      - run: pip --version
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"

  setup-docker-env:
    description: Set up and create docker env.
    parameters:
      image:
        type: string
      image-name:
        type: string
        default: "test_container"
      bind-mount:
        type: string
        default: "$PWD"
      extras-requires:
        type: string
        default: ""
      gpus:
        type: boolean
        default: true
    steps:
      - when:
          condition: <<parameters.gpus>>
          steps:
            - run: docker run -v <<parameters.bind-mount>>:/mnt/project --gpus=all -it -d --name <<parameters.image-name>> --workdir /mnt/project <<parameters.image>> /bin/bash
      - when:
          condition:
            not: <<parameters.gpus>>
          steps:
            - run: docker run -v <<parameters.bind-mount>>:/mnt/project -it -d --name <<parameters.image-name>> --workdir /mnt/project <<parameters.image>> /bin/bash
      - run:
          name: Install <<parameters.extras-requires>>
          command: |
            if [ -n "<<parameters.extras-requires>>" ]; then
              docker exec <<parameters.image-name>> tools/scripts/retry.sh pip install --progress-bar off <<parameters.extras-requires>>
            fi

  setup-paths:
    steps:
      - run: echo 'export PATH=$PATH:$HOME/.local/bin' >> $BASH_ENV
      - run: echo 'export PATH=$PATH:/usr/local/nvidia/bin' >> $BASH_ENV
      - run: echo 'export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64' >> $BASH_ENV

  run-e2e-tests:
    parameters:
      mark:
        type: string
        default: ""
      junit-path:
        type: string
        default: "/tmp/test-results/e2e/tests.xml"
      master-scheme:
        type: string
        default: "http"
      master-host:
        type: string
        default: "localhost"
      master-port:
        type: string
        default: "8080"
      master-cert:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      managed-devcluster:
        type: boolean
        default: false
      extra-pytest-flags:
        type: string
        default: ""
      wait-for-master:
        type: boolean
        default: true
    steps:
      # Wait for master before splitting tests, since so many splits depend on
      # asking master for its configuration in order to apply skipifs.
      - when:
          condition:
            and:
              - not: <<parameters.managed-devcluster>>
              - <<parameters.wait-for-master>>
          steps:
            - wait-for-master:
                scheme: <<parameters.master-scheme>>
                host: <<parameters.master-host>>
                port: <<parameters.master-port>>

      - run:
          name: Run e2e tests
          working_directory: ./e2e_tests
          no_output_timeout: 30m
          command: |

            # Remove prior test results
            rm -f "<<parameters.junit-path>>"
            
            pytest --setup-plan -m '<<parameters.mark>>' | egrep '\.py s*$' | sed 's/\.py.*$/.py/' > /tmp/all-relevant-files
            if [ ! -s /tmp/all-relevant-files ]; then
              echo 'No test files found!'
              exit 1
            fi
            
            if nc -zv <<parameters.master-host>> <<parameters.master-port>> 2>/dev/null; then
              echo "Determined user list from master '<<parameters.master-scheme>>://<<parameters.master-host>>:<<parameters.master-port>>'"
              DET_MASTER_CERT_FILE=noverify DET_USER=admin DET_PASS=${INITIAL_USER_PASSWORD} \
              det --master "<<parameters.master-scheme>>://<<parameters.master-host>>:<<parameters.master-port>>" user list
            else
              echo "No Determined master listening on '<<parameters.master-scheme>>://<<parameters.master-host>>:<<parameters.master-port>>'"
            fi

            cat /tmp/all-relevant-files | circleci tests run --command="DD_CIVISIBILITY_AGENTLESS_ENABLED=true \
            DD_SITE='datadoghq.com' \
            DD_ENV=ci DD_SERVICE='pytest-<<parameters.mark>>' \
            DET_MASTER_CERT_FILE=<<parameters.master-cert>> \
            DET_MASTER_CERT_NAME=<<parameters.master-cert-name>> \
            IS_CIRCLECI_JOB=1 XDG_CONFIG_HOME=/tmp \
            xargs pytest --capture=tee-sys -vv \
            -m '<<parameters.mark>>' \
            --durations=0 \
            --ddtrace \
            --master-scheme="<<parameters.master-scheme>>" \
            --master-host="<<parameters.master-host>>" \
            --master-port="<<parameters.master-port>>" \
            -o junit_family=xunit1 \
            --junit-xml="<<parameters.junit-path>>" \
            <<parameters.extra-pytest-flags>>" \
            --verbose --split-by=timings

            pytest_status=$?
            echo Pytest exited with $pytest_status
            exit $pytest_status

      - upload-test-job:
          only_on_branch: main
          test_results_path: <<parameters.junit-path>>

  run-det-deploy-tests:
    parameters:
      mark:
        type: string
        default: ""
      det-version:
        type: string
        default: ""
    steps:
      - run:
          name: Run det-deploy tests
          # TODO(Brad): backport workdir fixes to oss determined
          working_directory: ./e2e_tests
          command: |
            pytest -vv -s \
            -m <<parameters.mark>> \
            --no-compare-stats \
            --det-version="<<parameters.det-version>>"

  deploy-aws-cluster:
    parameters:
      cluster-id:
        type: string
      extra-tags:
        type: string
        default: ""
      det-version:
        type: string
      keypair:
        type: string
        default: "integrations-test"
      enable-cors:
        type: boolean
        default: false
      master-tls-cert:
        type: string
        default: ""
      master-tls-key:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      max-dynamic-agents:
        type: integer
        default: 1
      retain-log-group:
        type: boolean
        default: false
      log-group-prefix:
        type: string
        default: ""
      reattach-enabled:
        type: boolean
        default: false
      deployment-type:
        type: string
        default: simple-rds
    steps:
      - run:
          name: Initialize extra arguments
          command: touch /tmp/det-deploy-extra-args
      - when:
          condition:
            equal: [true, << parameters.enable-cors >>]
          steps:
            - run:
                name: Enable CORS
                command: "echo --enable-cors >> /tmp/det-deploy-extra-args"
      - when:
          condition: << parameters.extra-tags >>
          steps:
            - run:
                name: Add extra tags
                command: |
                  echo "<< parameters.extra-tags >>" | \
                  sed 's/ /-/g' | \
                  sed -r 's/([^,=]*=[^,=]*),?/--add-tag \1 /g' >> \
                    /tmp/det-deploy-extra-args

      - run:
          name: Configure TLS arguments
          command: |
            if [ -n "<<parameters.master-tls-cert>>" ]; then echo "--master-tls-cert <<parameters.master-tls-cert>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-tls-key>>" ]; then echo "--master-tls-key <<parameters.master-tls-key>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-cert-name>>" ]; then echo "--master-cert-name <<parameters.master-cert-name>>" >> /tmp/det-deploy-extra-args; fi
      - run:
          name: Configure log group arguments
          command: |
            if <<parameters.retain-log-group>>; then echo "--retain-log-group" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.log-group-prefix>>" ]; then echo "--log-group-prefix <<parameters.log-group-prefix>>" >> /tmp/det-deploy-extra-args; fi
      - when:
          condition:
            equal: [true, <<parameters.reattach-enabled>>]
          steps:
            - run:
                name: set reattach-enabled
                command: echo --agent-reattach-enabled true --agent-reconnect-attempts 24 --no-update-terminate-agents --notebook-timeout 1800 >> /tmp/det-deploy-extra-args
      - run:
          name: Deploy AWS cluster
          no_output_timeout: 20m
          command: |
            echo "-----BEGIN ARGS-----"
            cat /tmp/det-deploy-extra-args
            echo "-----END ARGS-----"
            MAX_RETRIES=6 DET_DEBUG=1 tools/scripts/retry.sh det deploy aws up \
              $(< /tmp/det-deploy-extra-args) \
              --add-tag owner=determined_ci \
              --add-tag gh_team=machine-users \
              --cluster-id <<parameters.cluster-id>> \
              --det-version <<parameters.det-version>> \
              --aux-agent-instance-type <<parameters.aux-agent-instance-type>> \
              --compute-agent-instance-type <<parameters.compute-agent-instance-type>> \
              --max-dynamic-agents <<parameters.max-dynamic-agents>> \
              --keypair <<parameters.keypair>> \
              --deployment-type <<parameters.deployment-type>> \
              --enterprise-edition \
              --docker-user $DOCKER_USER \
              --docker-pass $DOCKER_PASS \
              --retain-log-group \
              --initial-user-password ${INITIAL_USER_PASSWORD} \
              --yes

  terminate-aws-cluster:
    parameters:
      cluster-id:
        type: string
    steps:
      - run:
          name: Terminate AWS Cluster
          when: always
          no_output_timeout: 20m
          command: |
            MAX_RETRIES=6 DET_DEBUG=1 tools/scripts/retry.sh det deploy aws down \
              --cluster-id <<parameters.cluster-id>> --yes

  setup-aws-cluster:
    parameters:
      cluster-id:
        type: string
      extra-tags:
        type: string
        default: ""
      det-version:
        type: string
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      max-dynamic-agents:
        type: integer
        default: 1
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
      master-cert-name:
        type: string
      deployment-type:
        type: string
        default: simple-rds
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - deploy-aws-cluster:
          cluster-id: ${CLUSTER_ID}
          extra-tags: <<parameters.extra-tags>>
          det-version: <<parameters.det-version>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>
          master-cert-name: <<parameters.master-cert-name>>
          log-group-prefix: determined-ci
          retain-log-group: true
          deployment-type: <<parameters.deployment-type>>
      - set-master-address-aws:
          cluster-id: ${CLUSTER_ID}
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>

  terminate-gke-cluster:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
    steps:
      # Use a run instead of `gke/helm` orbs because circle CI orbs do not support `when`.
      - run:
          name: Delete Helm release
          when: always
          command: helm delete ci
      - run:
          name: Terminate GKE Cluster
          when: always
          command: |
            gcloud container clusters delete <<parameters.cluster-id>> --quiet --region=<<parameters.region>>

  setup-gke-cluster:
    parameters:
      cluster-id:
        type: string
      det-version:
        type: string
      gke-version:
        type: string
      machine-type:
        type: string
      num-machines:
        type: integer
      labels:
        type: string
      gpu-type:
        type: string
      gpus-per-machine:
        type: integer
      slot-type:
        type: string
      max-slots-per-pod:
        type: integer
      slot-resource-requests-cpu:
        type: integer
      region:
        type: string
      node-locations:
        type: string
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
      environment-image:
        type: string
      storage-bucket:
        type: string
        default: "det-ci-373"
      accel-node-taints:
        type: string
        default: ""
      cluster-ipv4-cidr:
        type: string
        description: The IP address range for the pods in this cluster.
        # We are specifying this since the cluster doesn't need to be very large and GKE was intermittently
        # defaulting to an excessively large range. "/19" is the minimum range it will let us allocate and
        # allows the cluster to create at most 16 nodes.
        # Kubernetes services default to 4096 IPs, or a "/20", and the remainder gets split up into a "/24" per node.
        # So with a "/19", there are 8192 total addresses. After 4096 get used for services there are 4096 left.
        # That's 2^12 / 2^8 = 16 nodes for our tests. Thanks @Danny Sauer!
        # If we find we need more nodes in our tests in the future, we can bump this up.
        default: "/19"
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - set-cluster-labels:
          labels: owner=determined_ci,gh_team=machine-users,<<parameters.labels>>
      - gcloud/install:
          version: "412.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: |
            tries=5
            until gcloud components install gke-gcloud-auth-plugin --quiet; do
              if [[ $((--tries)) -eq 0 ]]; then
                exit 1
              fi
              sleep 15
            done

            echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $BASH_ENV
          name: Install GKE auth plugin
      - run:
          command: |
            echo "Console URL for cluster: https://console.cloud.google.com/kubernetes/clusters/details/<<parameters.region>>/${CLUSTER_ID}?project=determined-ai"
            gcloud container clusters create ${CLUSTER_ID} --machine-type=n1-standard-8 --cluster-version=<<parameters.gke-version>> --region=<<parameters.region>> --node-locations=<<parameters.node-locations>> --scopes storage-rw,cloud-platform --num-nodes 1 --labels environment=ci,${LABELS} --no-enable-ip-alias --subnetwork default --cluster-ipv4-cidr=<<parameters.cluster-ipv4-cidr>>
          name: Create GKE cluster
          no_output_timeout: 30m
      - run:
          command: gcloud container clusters get-credentials ${CLUSTER_ID} --project ${GOOGLE_PROJECT_ID} --region <<parameters.region>>
          name: Get Kubeconfig
      - run:
          command: |
            echo 'export HELM_VALUES="detVersion=<<parameters.det-version>>,maxSlotsPerPod=<<parameters.max-slots-per-pod>>,checkpointStorage.type=gcs,checkpointStorage.bucket=<<parameters.storage-bucket>>,taskContainerDefaults.startupHook=echo hello from master tcd startup hook"' >> "$BASH_ENV"
          name: Prepare helm overrides
      - run:
          command: |
            echo 'export HELM_VALUES="${HELM_VALUES},enterpriseEdition=true,imagePullSecretName=docker-cred"' >> "$BASH_ENV"
            kubectl create secret docker-registry docker-cred --docker-username=$DOCKER_USER --docker-password=$DOCKER_PASS
          name: Set EE helm overrides and create docker secret
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
                name: Install NVIDIA drivers
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},slotType=<<parameters.slot-type>>,slotResourceRequests.cpu=<<parameters.slot-resource-requests-cpu>>,resourcePools[0].agent_reattach_enabled=true,resourcePools[0].pool_name=default,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].effect=NoSchedule,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].effect=NoSchedule"' >> "$BASH_ENV"
                name: CPU setup helm overrides
      - when:
          condition: <<parameters.environment-image>>
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},taskContainerDefaults.cpuImage=<<parameters.environment-image>>,taskContainerDefaults.gpuImage=<<parameters.environment-image>>"' >> "$BASH_ENV"
                name: env image helm overrides
      - helm/install-helm-chart:
          chart: helm/charts/determined
          helm-version: v3.2.4
          namespace: "default"
          wait: true
          release-name: "ci"
          values-to-override: ${HELM_VALUES}
      - set-master-address-gke:
          release-name: "ci"
          namespace: "default"
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                no_output_timeout: 30m
                command: |
                  (sleep 29.5m && pkill python3) &
                  gcloud container node-pools create accel \
                    --cluster ${CLUSTER_ID} \
                    --region '<<parameters.region>>' \
                    --num-nodes '<<parameters.num-machines>>' \
                    --accelerator 'type=<<parameters.gpu-type>>,count=<<parameters.gpus-per-machine>>' \
                    --machine-type='<<parameters.machine-type>>' \
                    --scopes cloud-platform \
                    --node-taints='<<parameters.accel-node-taints>>' \
                    || (
                      curl "$SLACK_WEBHOOK" \
                        -H 'Content-Type: application/json' \
                        -d "{\"text\":\"GKE node pool creation failed on branch \`$CIRCLE_BRANCH\`! $CIRCLE_BUILD_URL\"}"
                      echo "================"
                      echo "GKE node pool creation failed, but we are marking the job as successful because this is not directly our problem. Ask the infrastructure engineering team about increasing the quota if you want."
                      echo "================"
                      circleci-agent step halt
                    )
                name: Create GPU node pool
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: gcloud container node-pools create accel --cluster ${CLUSTER_ID} --region <<parameters.region>> --num-nodes <<parameters.num-machines>> --machine-type=<<parameters.machine-type>> --scopes cloud-platform --node-taints=<<parameters.accel-node-taints>>
                name: Create CPU node pool

  setup-shared-cluster:
    parameters:
      cluster-id:
        type: string
        default: ${GKE_CLUSTER_NAME}
      labels:
        type: string
        default: ""
      det-version:
        type: string
      region:
        type: string
        default: ${GKE_REGION}
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
      gpus-per-machine:
        type: integer
        default: 20
      slot-type:
        type: string
        default: "cpu"
      slot-resource-requests-cpu:
        type: integer
        default: 1
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
      master-cert-name:
        type: string
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - set-cluster-labels:
          labels: <<parameters.labels>>
      - gcloud/install:
          version: "412.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: |
            echo 'export HELM_VALUES="detVersion=<<parameters.det-version>>,maxSlotsPerPod=<<parameters.gpus-per-machine>>,checkpointStorage.type=gcs,checkpointStorage.bucket=${GENERATED_NAMESPACE}-bucket,createNonNamespacedObjects=false"' >> "$BASH_ENV"
          name: Prepare helm overrides
      - when:
          condition:
            and:
            - <<parameters.gpus-per-machine>>
            - equal: [ "gpu", <<parameters.slot-type>> ]
          steps:
            - run:
                command: kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
                name: Install NVIDIA drivers
      - unless:
          condition:
            equal: [ "gpu", <<parameters.slot-type>> ]
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},slotType=<<parameters.slot-type>>,slotResourceRequests.cpu=<<parameters.slot-resource-requests-cpu>>,resourcePools[0].agent_reattach_enabled=true,resourcePools[0].pool_name=default,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].effect=NoSchedule,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].effect=NoSchedule"' >> "$BASH_ENV"
                name: CPU setup helm overrides
      - run:
          command: |
            echo 'export HELM_VALUES="${HELM_VALUES},security.tls.cert=\"${MASTER_TLS_CERT}\",security.tls.key=\"${MASTER_TLS_KEY}\",masterPort=8443,tlsSecret=<<parameters.master-cert-name>>"' >> "$BASH_ENV"
          name: Setup TLS Helm Values
      - run:
          command: |
            ip_addresses=($(echo "${CI_RANGES}" | tr -d '" ' | tr ',' ' '))
            formattedRange="export HELM_VALUES=${HELM_VALUES},'loadBalancerSourceRanges={"
            for i in ${ip_addresses[@]}; do
              formattedRange+="${i}," 
            done
            formattedRange=${formattedRange::-1}
            formattedRange+="}'"
            echo ${formattedRange} >> "$BASH_ENV"
          name: Setup Firewall Config
      - run:
          command: |
            echo 'export HELM_VALUES="${HELM_VALUES},initialUserPassword=${INITIAL_USER_PASSWORD}"' >> "$BASH_ENV"
      - run:
          command: |
            tries=5
            until gcloud components install gke-gcloud-auth-plugin --quiet; do
              if [[ $((--tries)) -eq 0 ]]; then
                exit 1
              fi
              sleep 15
            done

            echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $BASH_ENV
          name: Install GKE auth plugin
      - run:
          command: gcloud container clusters get-credentials <<parameters.cluster-id>> --project ${<<parameters.google-project-id>>} --region <<parameters.region>>
          name: Get Kubeconfig
      - run: 
          command: kubectl create namespace ${GENERATED_NAMESPACE}
          name: Create namespace
      - run:
          command: kubectl config set-context --current --namespace=${GENERATED_NAMESPACE}
          name: Set context to the created namespace
      - run: 
          command: kubectl create secret tls <<parameters.master-cert-name>> --cert <<parameters.master-tls-cert>> --key <<parameters.master-tls-key>> --namespace ${GENERATED_NAMESPACE} # Create tls secret in namespace w/secret name
      - run:
          command: gsutil mb -p ${<<parameters.google-project-id>>} gs://${GENERATED_NAMESPACE}-bucket
      - helm/install-helm-client:
          version: v3.12.3
      - run:
          command: |
            helm install ${GENERATED_NAMESPACE} helm/charts/determined  --set "${HELM_VALUES}" --namespace="${GENERATED_NAMESPACE}" --wait --timeout 10m0s
          name: Helm Install
      - run:
          command: |
            helm get values ${GENERATED_NAMESPACE} --namespace="${GENERATED_NAMESPACE}"
          name: Get Helm Values
      - set-master-address-gke:
          release-name: ${GENERATED_NAMESPACE}
          namespace: ${GENERATED_NAMESPACE}
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>

  generate-tls-cert:
    steps:
      - run: |
          .circleci/scripts/generate_cert.sh /tmp/master
          echo 'export MASTER_TLS_CERT=/tmp/master.crt MASTER_TLS_KEY=/tmp/master.key MASTER_CERT_NAME=determined-master-ci' >> $BASH_ENV

  set-master-address-aws:
    parameters:
      cluster-id:
        type: string
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
    steps:
      - run: |
          MASTER_HOST=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> DeterminedAddress)
          echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV
          if [ -n "<<parameters.master-tls-cert>>" ] && [ -n "<<parameters.master-tls-key>>" ]; then
            echo "export MASTER_PORT=8443" >> $BASH_ENV
            echo "export MASTER_SCHEME=https" >> $BASH_ENV
          fi

  set-master-address-gke:
    parameters:
      release-name:
        type: string
      namespace:
        type: string
      master-tls-cert:
        type: string
        default: "" 
      master-tls-key:
        type: string
        default: ""
    steps:
      - run:
          name: Set Master Address
          command: |
            MASTER_HOST=$(kubectl get -n <<parameters.namespace>>  service determined-master-service-<<parameters.release-name>> \
            --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
            echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV
            echo "${MASTER_HOST}"
            if [ -n "<<parameters.master-tls-cert>>" ] && [ -n "<<parameters.master-tls-key>>" ]; then
              echo "export MASTER_PORT=8443" >> $BASH_ENV
              echo "export MASTER_SCHEME=https" >> $BASH_ENV
            fi
  
  set-google-application-credentials:
    steps:
      - run:
          name: Set Google Application Credentials
          command: |
            GOOGLE_APPLICATION_CREDENTIALS=${HOME}/gcloud-service-key.json
            echo "export GOOGLE_APPLICATION_CREDENTIALS=\"${GOOGLE_APPLICATION_CREDENTIALS}\"" >> $BASH_ENV

  set-cluster-id:
    parameters:
      cluster-id:
        type: string
    steps:
      - run: echo "export CLUSTER_ID=\"<<parameters.cluster-id>>\"" >> $BASH_ENV

  set-cluster-labels:
    parameters:
      labels:
        type: string
    steps:
      - run: echo "export LABELS=\"$(echo '<<parameters.labels>>' | sed 's/ /-/g')\"" >> $BASH_ENV

  wait-for-master:
    parameters:
      scheme:
        type: string
        default: "http"
      host:
        type: string
      port:
        type: string
        default: "8080"
    steps:
      - run: python .circleci/scripts/wait_for_master.py <<parameters.scheme>>://<<parameters.host>>:<<parameters.port>>

  upload-test-job:
    parameters:
      only_on_branch:
        type: string
        default: ""
      test_results_path:
        type: string
        default: ""
    steps:
      - when:
          condition:
            equal: [<<parameters.only_on_branch>>, << pipeline.git.branch >>]
          steps:
            - run:
                name: Test run success -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'success' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_success
            - run:
                name: Test run failed -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'failed' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_fail

  locate-cloudwatch-logs:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
        default: us-west-2
    steps:
      - run:
          name: Locate CloudWatch logs
          when: always
          command: |
            LOG_GROUP=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> LogGroup)
            echo "Cluster logs can be found in CloudWatch under the log group $LOG_GROUP"
            echo "or the URL below (if the log group is in your default region):"
            ENC_LOG_GROUP=$(echo "$LOG_GROUP" | sed 's|/|$252F|g')
            echo "https://console.aws.amazon.com/cloudwatch/home#logsV2:log-groups/log-group/$ENC_LOG_GROUP"

  pre-package-and-push-system:
    parameters:
      check:
        type: boolean
        default: true
    steps:
      - go-get-deps
      - run: make -C proto build
      - when:
          condition: <<parameters.check>>
          steps:
            - run: make -C master check
            - run: make -C agent check

  make-package:
    steps:
      - attach_workspace:
          at: .
      - run:
          no_output_timeout: 30m
          command: make package

  make-package-small:
    steps:
      - attach_workspace:
          at: .
      - run:
          no_output_timeout: 30m
          command: make -C master package-small

  install-devcluster:
    steps:
      - run: pip install git+https://github.com/determined-ai/devcluster.git@v1.1.0#egg=devcluster
      - run:
          command: |
            if ! [ -x "$(command -v socat)" ]; then
              apt update && apt install -y socat
            fi

  start-devcluster:
    parameters:
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
    steps:
      - run:
          command: devcluster --oneshot -c .circleci/devcluster/<<parameters.devcluster-config>> --target-stage <<parameters.target-stage>>
          background: true

jobs:
  build-helm:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - attach_workspace:
          at: .
      - run: make -C helm build
      - persist_to_workspace:
          root: .
          paths:
            - helm/build
      - store_artifacts:
          path: helm/build

  build-docs:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C examples build
      - run: make -C helm build
      - attach_workspace:
          at: .
      - run: make -C docs attributions.rst rebrand build
      - persist_to_workspace:
          root: .
          paths:
            - examples/build
            - harness/dist
            - model_hub/dist
            - docs/build
            - docs/site
      - run: tar czf docs.tgz docs/site/html
      - store_artifacts:
          path: docs.tgz

  upload-docs-search-index:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C docs upload-search-index

  check-docs-links:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run:
          command: make -C docs check-links

  publish-docs:
    docker:
      - image: hashicorp/terraform:latest
    steps:
      - checkout
      - attach_workspace:
          at: .
      - run: apk add make curl
      - run: pip install --user awscli  # used in the terraform apply
      - run: make -C docs publish

  # TODO(danh): eventually replace 'publish-docs' with this new workflow after
  # all dependent scripts are updated.
  # Related work: https://hpe-aiatscale.atlassian.net/browse/INFENG-183
  publish-docs-new:
    parameters:
      preview:
        type: boolean
        default: true
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-not-docs
      - attach_workspace:
          at: .
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - when:
          condition: <<parameters.preview>>
          steps:
            - run: make -C docs preview

  package-and-push-system-local:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - reinstall-go
      - license-gen
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: previous
      - pre-package-and-push-system:
          check: false
      - make-package
      - run: mkdir -p build/
      - run: docker tag determinedai/hpe-mlde-master:${CIRCLE_SHA1}-amd64 determinedai/hpe-mlde-master:${CIRCLE_SHA1}
      - run: docker save -o build/master.image determinedai/hpe-mlde-master:${CIRCLE_SHA1}
      - run: docker tag determinedai/hpe-mlde-agent:${CIRCLE_SHA1}-amd64 determinedai/hpe-mlde-agent:${CIRCLE_SHA1}
      - run: docker save -o build/agent.image determinedai/hpe-mlde-agent:${CIRCLE_SHA1}
      - persist_to_workspace:
          root: .
          paths:
            - "master/dist/*linux_amd64.deb"
            - "master/dist/*linux_amd64.rpm"
            - "agent/dist/*linux_amd64.deb"
            - "agent/dist/*linux_amd64.rpm"
            - "agent/dist/determined-agent_linux_amd64_v1/determined-agent"
            - "build/*.image"

  package-and-push-system-dev:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - license-gen
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system:
          check: false
      - make-package
      - run: tools/scripts/retry.sh make -C master publish-dev
      - run: tools/scripts/retry.sh make -C agent publish-dev
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  package-and-push-system-dev-small:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - setup-python-venv:
          install-python: false
          determined: true
          executor: <<pipeline.parameters.docker-image>>
      - reinstall-go
      - setup_remote_docker:
          version: 20.10.18
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system:
          check: false
      - make-package-small
      - run: tools/scripts/retry.sh make -C master publish-dev-small
      - persist_to_workspace:
          root: .
          paths:
            - harness/dist

  package-and-push-system-rc:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - license-gen
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system
      - make-package
      - run: make -C master publish
      - run: make -C agent publish

      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  package-and-push-system-release:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - license-gen
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system
      - run:
          no_output_timeout: 30m
          command: make -C master release
      - run:
          no_output_timeout: 30m
          command: make -C agent release
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  publish-helm:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - helm/install-helm-client
      - run: helm plugin install https://github.com/chartmuseum/helm-push.git
      - login-helm
      - run: make -C helm release

  publish-helm-gh:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - run: make -C helm release-gh

  publish-python-package:
    parameters:
      path:
        type: string
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "build twine"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C <<parameters.path>> build
      - run: make -C <<parameters.path>> publish

  check-ts-bindings:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "bindings/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C bindings force-gen
      - run: make -C bindings check/typescript

  check-py-bindings:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "bindings/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C bindings force-gen
      - run: make -C bindings check/python

  upload-try-now-template:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "awscli"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C harness upload-try-now-template

  test-debian-packaging:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - run: sudo apt-get install -y $(pwd)/master/dist/hpe-mlde-master*.deb
      - run: sudo apt-get install -y $(pwd)/agent/dist/hpe-mlde-agent*.deb
      - run: sudo cp .circleci/packaging/master.yaml /etc/determined/master.yaml
      - run: sudo cp .circleci/packaging/agent.yaml /etc/determined/agent.yaml
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - install-devcluster
      - start-devcluster:
          target-stage: db
      - run: python3 .circleci/scripts/wait_for_server.py localhost 5432
      - run: sudo systemctl restart determined-master
      - run: python3 .circleci/scripts/wait_for_server.py localhost 8080 || { journalctl --no-pager -u determined-master; exit 1; }
      - run: sudo systemctl restart determined-agent
      - run: ./.circleci/scripts/sanity.sh

  lint-react:
    docker:
      - image: cimg/node:20.9.0
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - react-get-deps
      - run: make -C webui/react check
      - store_artifacts:
          path: /home/circleci/.npm/_logs/

  build-react:
    parameters:
      dev-mode:
        type: boolean
        default: false
    docker:
      - image: cimg/node:20.9.0
    resource_class: large
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - react-get-deps
      - run: |
          if <<parameters.dev-mode>>; then
            echo 'Setting development mode...'
            export DET_NODE_ENV=development
          fi
          make -C webui/react build
      - persist_to_workspace:
          root: .
          paths:
            - webui/react/build

  test-e2e-react:
    docker:
      - image: mcr.microsoft.com/playwright:v1.40.0-focal
    resource_class: large
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - run: apt-get update && apt-get -y install make
      - react-get-deps
      - run: make -C webui/react get-playwright-ci
      - run: SERVER_ADDRESS=${PW_SERVER_ADDRESS} npm run build --prefix webui/react
      - run: PW_USER_NAME=${PW_USER_NAME} PW_PASSWORD=${PW_PASSWORD} npm run e2e --prefix webui/react
      - store_artifacts:
          path: webui/react/src/e2e/playwright-report
      - store_artifacts:
          path: webui/react/src/e2e/test-results
      - store_test_results:
          path: webui/react/src/e2e/junit-results.xml

  test-unit-react:
    docker:
      - image: cimg/node:20.9.0
        environment:
          CI: "true"
    resource_class: xlarge
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - react-get-deps
      - run: make -C webui/react test-ci
      - codecov/upload:
          flags: "web"
          xtra_args: "-v"
      - store_test_results:
          path: webui/react/junit.xml
      - store_artifacts:
          path: webui/react/coverage

  lint-sql:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          extras-requires: "sqlfluff"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C master check-sql

  lint-go:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: large
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - reinstall-go
      - go-get-deps
      - run: sudo apt-get update && sudo apt-get install -y clang-format
      - run: make -C proto build
      - run: make -C proto check
      - run: make -C master check
      - run: make -C agent check

  build-go:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - reinstall-go
      - license-gen
      - restore_cache:
          keys:
            - det-go-build-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}
      - run: make -C master build
      - run: make -C agent build
      - persist_to_workspace:
          root: .
          paths:
            - "master/build"
            - "agent/build"
      # Save cache after we build master and agent so we can have our build cache there.
      - save_cache:
          key: det-go-build-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}
          paths:
            - "/home/circleci/go/"
            - "/home/circleci/.cache/go-build/"

  build-proto:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - reinstall-go
      - go-get-deps
      - run: make -C proto build
      - persist_to_workspace:
          root: .
          paths:
            - "proto/build/**/*"

  test-intg-master:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: make -C master test-intg
      - codecov/upload:
          flags: "backend"
          xtra_args: "-v -X fixes"
      - store_test_results:
          path: master/test-intg.junit.xml
      - persist_to_workspace:
          root: .
          paths:
            - master/coverage.out
            - master/internal/mocks

  test-intg-agent:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: make -C agent test-intg
      - codecov/upload:
          flags: "backend"
          xtra_args: "-v -X fixes"
      - store_test_results:
          path: agent/test-intg.junit.xml
      - persist_to_workspace:
          root: .
          paths:
            - agent/coverage.out

  go-coverage:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - run: cd master && go tool cover -func coverage.out
      - run: cd master && go tool cover -html coverage.out -o coverage.html
      - run: cd agent && go tool cover -func coverage.out
      - run: cd agent && go tool cover -html coverage.out -o coverage.html
      - run: mkdir go-coverage
      - run: mv master/coverage.html go-coverage/master-coverage.html
      - run: mv agent/coverage.html go-coverage/agent-coverage.html
      - store_artifacts:
          path: go-coverage
          destination: go-coverage

  check-go-coverage-master:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - run: go run master/cmd/cover-regex/cover_regex.go master/coverage.out || true

  check-go-coverage-agent:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - run: go run master/cmd/cover-regex/cover_regex.go agent/coverage.out || true

  lint-docs:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C docs check

  lint-secrets:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - run: git clone https://github.com/awslabs/git-secrets /tmp/git-secrets
      - run: cd /tmp/git-secrets && sudo make install
      - checkout
      - run: git secrets --install
      - run: git secrets --register-aws
      - run: git secrets --add '"private_key":\s"-----BEGIN\sPRIVATE\sKEY-----'
      - run: git secrets --scan-history

  lint-python:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extra-requirements-file: "requirements.txt model_hub/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C harness check
      - run: make -C model_hub check
      - run: make -C e2e_tests check
      - run: make -C tools check
      - run: make -C schemas check

  test-unit-harness-cpu:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-cpu-pycov make -C harness test-cpu
      - run: coverage xml -i --data-file=./test-unit-harness-cpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-cpu-pycov

  test-unit-harness-gpu:
    docker:
      - image: determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-0.30.1
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-pycov make -C harness test-gpu
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-pycov

  test-unit-harness-pytorch2-gpu:
    docker:
      - image: determinedai/environments:cuda-11.8-pytorch-2.0-gpu-079eb6d
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-pytorch2-gpu-pycov make -C harness test-pytorch-gpu
      - run: coverage xml -i --data-file=./test-unit-harness-pytorch2-gpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-pytorch2-gpu-pycov

  test-unit-harness-pytorch2-cpu:
    docker:
      - image: determinedai/environments:py-3.10-pytorch-2.0-cpu-079eb6d
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - run: echo 'export PATH=$PATH:$HOME/.local/bin' >> $BASH_ENV
      - run: COVERAGE_FILE=/root/project/test-unit-harness-pytorch2-cpu-pycov make -C harness test-pytorch-cpu
      - run: coverage xml -i --data-file=./test-unit-harness-pytorch2-cpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-pytorch2-cpu-pycov

  test-unit-harness-gpu-parallel:
    docker:
      - image: determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-0.30.1
    resource_class: determined-ai/container-runner-multi-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-parallel-pycov make -C harness test-gpu-parallel
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-parallel-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-parallel-pycov

  test-unit-harness-gpu-deepspeed:
    docker:
      - image: determinedai/environments:cuda-11.3-pytorch-1.10-deepspeed-0.8.3-gpu-0.22.1
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-deepspeed-pycov make -C harness test-gpu-deepspeed
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-deepspeed-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-deepspeed-pycov

  test-unit-harness-tf2:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-tf2-pycov make -C harness test-tf2
      - run: coverage xml -i --data-file=./test-unit-harness-tf2-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-tf2-pycov

  test-unit-storage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-storage-pycov coverage run -m pytest -v --durations=0 --require-secrets -m cloud harness/tests
      - run: coverage xml -i --data-file=./test-unit-storage-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-storage-pycov

  test-unit-model-hub:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    resource_class: medium+
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          model-hub: true
          extras-requires: "torch==1.9.0 torchvision==0.10.0"
          extra-requirements-file: "model_hub/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-model-hub-pycov make -C model_hub test
      - run: coverage xml -i --data-file=./test-model-hub-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - persist_to_workspace:
          root: .
          paths:
            - test-model-hub-pycov

  python-coverage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          determined: false
          model-hub: false
          extras-requires: "coverage"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: coverage combine *-pycov
      - run: coverage report --include 'harness/determined/*' --skip-covered
      - run: coverage report --include 'model_hub/model_hub/*' --skip-covered
      - run: coverage html --include 'harness/determined/*' --skip-covered -d cov-html/harness
      - run: coverage html --include 'model_hub/model_hub/*' --skip-covered -d cov-html/model_hub
      - store_artifacts:
          path: cov-html
          destination: cov-html

  test-cli:
    parameters:
      executor-name:
        type: string
    executor: << parameters.executor-name >>
    steps:
      - checkout
      - run: python --version
      # Running the pip executable causes an error with the win/default executor for some reason.
      - run: python -m pip install --upgrade --user pip
      - run: pip --version
      - run: pip install wheel
      - run: cd harness; python setup.py bdist_wheel -d ../build
      - run: pip install --find-links build determined==<< pipeline.parameters.det-version >>
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"
      # Ensure Determined cli can run without installing cli test requirements
      - run: det --help
      - run: pip install setuptools_scm
      - run: pip install -r harness/tests/requirements/requirements-cli.txt
      - run: pip freeze --all
      - run: sh -c "pip check || true"
      - run: pytest harness/tests/cli

  test-e2e-slurm-disabled:
    parameters:
      master_config:
        type: string
        default:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - run: echo "Test suite disabled."

  # By default, this job only runs on the main branch unless otherwise
  # specified. To invoke this job on a developer branch, add the 'ci-run-allgcp'
  # label to your pull request on github.
  test-e2e-hpc-gcp:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      always-run:
        type: boolean
        default: false
      extra-pytest-flags:
        type: string
        default: ""
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      container-run-type:
        type: string
        default: singularity
      agent-use:
        type: string
        default: ""
      workload-manager:
        type: string
        default: slurm
      master-scheme:
        type: string
        default: "http"
      master-host:
        type: string
        default: "localhost"
      master-port:
        type: string
        default: "8080"
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      # This is a bit confusing... basically only enter logic for NOT running job
      # when always-run=false
      - unless:
          condition:
            or:
              - equal: [ true, <<parameters.always-run>> ]
              - equal: [ "main", <<pipeline.git.branch>> ]
              - matches: { pattern: "^release-.*$", value: <<pipeline.git.branch>> }
          steps:
            - skip-on-dev-branch

      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id    

      - run:
          name: Create instance name hash and set OPT_DEVBOX_PREFIX env var
          command: |
            # This check is for the scenario when two pushes to main are running concurrently (we don't want this).
            # In this case, we need more specificity to the hash information to make two separate VMs.
            if [[ $CIRCLE_BRANCH == "main" ]]; then
              echo "export OPT_DEVBOX_PREFIX=circleci-job-$(echo -n "${CIRCLE_USERNAME}-${CIRCLE_BRANCH}-${CIRCLE_JOB}-${CIRCLE_WORKFLOW_JOB_ID}" | md5sum | awk '{print $1}')" >> "$BASH_ENV"
            else
              echo "export OPT_DEVBOX_PREFIX=circleci-job-$(echo -n "${CIRCLE_USERNAME}-${CIRCLE_BRANCH}-${CIRCLE_JOB}" | md5sum | awk '{print $1}')" >> "$BASH_ENV"
            fi

      - attach_workspace:
          at: .

      - reinstall-go
      - go-get-deps
      - run: PATH=$HOME/.local/bin:$PATH make -C proto build

      - setup-python-venv:
          install-python: true
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - install-devcluster
      - license-gen

      - gcloud/install:
          version: "412.0.0"
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>

      - run:
          name: Install terraform
          command: |
            wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
            echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
            sudo apt update && sudo apt install terraform
      - run: terraform --version

      - set-google-application-credentials

      - run:
          name: Delete existing resources (if they exist)
          command: |
            # This grabs the zone.default from tools/slurm/terraform/variables.tf used to specify the zone (as this 
            # is not set at this point since no terraform initiation has happened. See workaround section in README) 
            GCLOUD_ZONE=$(grep -A2 'variable "zone"' ~/project/tools/slurm/terraform/variables.tf | grep 'default' \
            | awk -F' = ' '{ print $2 }' | tr -d '\"')
            gcloud compute instances delete ${OPT_DEVBOX_PREFIX}-dev-box --zone=$GCLOUD_ZONE --quiet || true
            gcloud compute firewall-rules delete ${OPT_DEVBOX_PREFIX}-dev-box --quiet || true
            gcloud compute networks delete ${OPT_DEVBOX_PREFIX}-dev-box --quiet || true

      - run:
          name: Make slurmcluster
          # Breaks without apt-get installs for some reason
          # NOTE: TF_LOCK is set to 'false' to catch the case where a developer triggered a workflow and did not
          # let it run completely. This should not be an issue since we delete the instance already in the previous
          # step (this is precautionary). See a similar thing in the 'make unslurmcluster' step.
          command: |
            sudo apt-get update
            sudo apt-get install gettext
            sudo apt-get install iproute2
            yes yes | PATH=$HOME/.local/bin:$PATH make slurmcluster FLAGS="-c <<parameters.container-run-type>> -w <<parameters.workload-manager>> <<parameters.agent-use>>" TF_LOCK=false | tee output.log
          background: true

      - run:
          name: Wait for VM creation
          command: |
            sleep 5
            for i in {1..220}; do
              ELAPSED_TIME=$((i * 5))
              echo "Waiting for VM creation [${ELAPSED_TIME}s]..."
              if grep -q "^Running cluster..." output.log; then
                echo "VM creation has finished."
                break
              fi
              if [[ $i -eq 220 ]]; then
                echo "Timeout waiting for VM creation."
                exit 1
              fi
              sleep 5
            done            

      # For when a user wants to use an agent instead of launcher.
      - when:
          condition:
            equal: ["-A", <<parameters.agent-use>>]
          steps:
            - run:
                name: Build agent resources
                command: |
                  make -C agent build package
            - wait-for-master:
                scheme: <<parameters.master-scheme>>
                host: <<parameters.master-host>>
                port: <<parameters.master-port>>
            - run:
                name: Transfer and Allocate agent resources on VM
                command: |
                  ZONE=$(terraform -chdir=tools/slurm/terraform output --raw zone)
                  INSTANCE_NAME=$(terraform -chdir=tools/slurm/terraform output --raw instance_name)
                  PROJECT=$(terraform -chdir=tools/slurm/terraform output --raw project)
                  gcloud compute scp agent/dist/determined-agent_linux_amd64_v1/determined-agent "$INSTANCE_NAME":~ --zone $ZONE
                  gcloud compute ssh --zone "$ZONE" "$INSTANCE_NAME" --project "$PROJECT" -- \
                  srun determined-agent --master-host=<<parameters.master-host>> --master-port=<<parameters.master-port>> --resource-pool=default --container-runtime=<<parameters.container-run-type>>
                background: true
            - run:
                name: Query the job to ensure Determined Agent is running
                command: |
                  # 60 tries gives 30 minutes of tries until the query times out
                  tries=60
                  # squeue command gives name and state information. There must be an escape character for the gcloud command.
                  ZONE=$(terraform -chdir=tools/slurm/terraform output --raw zone)
                  INSTANCE_NAME=$(terraform -chdir=tools/slurm/terraform output --raw instance_name)
                  PROJECT=$(terraform -chdir=tools/slurm/terraform output --raw project)
                  gcloud compute ssh --zone "$ZONE" "$INSTANCE_NAME" --project "$PROJECT" -- squeue -o "%j\ %T"
                  # Queries until the jobname is shown in a running state. The agent job must be running to run e2e tests
                  until [[ -n $(gcloud compute ssh --zone "$ZONE" "$INSTANCE_NAME" --project "$PROJECT" -- squeue -o "%j\ %T" | grep "determined-agent" | grep "RUNNING") ]] ; do
                      if [[ $((--tries)) -eq 0 ]]; then
                        echo "The job determined-agent did not start. Please check if there are other jobs in the queue preventing this job from starting"
                        exit 1
                      fi
                      echo "Waiting 30s to query for the job name again..."
                      sleep 30
                      echo "Retrying job query..."
                      gcloud compute ssh --zone "$ZONE" "$INSTANCE_NAME" --project "$PROJECT" -- squeue -o "%j\ %T"
                  done
            - run:
                name: Query the slot count to ensure slots are allocated
                command: |
                  tries=20
                  det slot list
                  until [[ $(det slot list | wc -l) -gt 2 ]] ; do
                      if [[ $((--tries)) -eq 0 ]]; then
                        echo "ERROR: determined-agent failed to register at least 2 slots. Check the 'Transfer and Allocate agent resources on VM' for any failures."
                        exit 1
                      fi
                      echo "Waiting 5s to query slots again..."
                      sleep 5
                      echo "Retrying slot query..."
                      det slot list
                  done

      - run-e2e-tests:
          mark: <<parameters.mark>>
          wait-for-master: true
          master-scheme: <<parameters.master-scheme>>
          master-host: <<parameters.master-host>>
          master-port: <<parameters.master-port>>
          extra-pytest-flags: <<parameters.extra-pytest-flags>>

      # Must cancel the srun command to prevent a ssh error
      - when:
          condition:
            equal: ["-A", <<parameters.agent-use>>]
          steps:
            - run:
                name: Deallocate the Resources on VM
                command: |
                  ZONE=$(terraform -chdir=tools/slurm/terraform output --raw zone)
                  INSTANCE_NAME=$(terraform -chdir=tools/slurm/terraform output --raw instance_name)
                  PROJECT=$(terraform -chdir=tools/slurm/terraform output --raw project)
                  gcloud compute ssh --zone "$ZONE" "$INSTANCE_NAME" --project "$PROJECT" -- \
                  scancel -u $USER

      - run:
          name: Make unslurmcluster
          when: always
          command: |
            pkill determined-mast
            (yes yes || true) | make unslurmcluster TF_LOCK=false
            # For some reason, even when `make unslurmcluster` is successful, CircleCI
            # receives exit code 141, so ignore that.
            EXIT_STATUS=$?
            echo $EXIT_STATUS
            if [[ $EXIT_STATUS -eq 141 ]]; then
              echo "Ignoring exit code 141"
              exit 0
            else
              exit $EXIT_STATUS
            fi
      - store_test_results:
          path: /tmp/test-results/

  test-e2e-slurm:
    parameters:
      mark:
        type: string
        default: e2e_slurm
      runner_class:
        type: string
        default: determined-ai/znode-cluster
      master_config:
        type: string
        default: |
          task_container_defaults:
            slurm:
              sbatch_args:
                - --time=04:00:00
            environment_variables:
              # Some ports are not working, disable them so distributed jobs work.
              - NCCL_IB_HCA=mlx6_0:0
          checkpoint_storage:
            type: shared_fs
            host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
            storage_path: determined-checkpoint
            save_experiment_best: 0
            save_trial_best: 1
            save_trial_latest: 1
          db:
            user: postgres
            host: localhost
            port: 5432
            name: determined
            password: launcher
          resource_manager:
            type: slurm
            master_host: $HOSTNAME
            master_port: 8080
            host: localhost
            port: 8181
            protocol: http
            slot_type: cuda
            user_name: launcher
            group_name: hpcd
            singularity_image_root: /lustre/hdd/foundation_engineering/images
            job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
            auth_file: /home/launcher/.launcher.$HOSTNAME.token
            path: /opt/singularity/bin:/usr/local/bin:${PATH}
            ld_library_path:
      reserved_ports_znode50:
        type: string
        default: |
            reserved_ports:
              - 12350
              - 12351
              - 12360
              - 12361
              - 12365
              - 12366
              - 29400
      determined_master_host:
        type: string
        default: localhost:8080
      slack-mentions:
        type: string
        default: ""
      cluster_unix_user:
        type: string
        default: launcher
      cluster_determined_user:
        type: string
        default: determined
      determined_admin_username:
        type: string
        default: admin
      determined_admin_password:
        type: string
        default: ""
      database_username:
        type: string
        default: postgres
      database_password:
        type: string
        default: launcher
      extra-pytest-flags:
        type: string
        default: ""
      agent-use:
        type: string
        default: ""
    # Following https://circleci.com/docs/2.0/runner-installation-linux/index.html#start-the-service
    machine: true
    resource_class: <<parameters.runner_class>>
    environment:
      SHARED_CLUSTER: True
    steps:
      - checkout
      - attach_workspace:
          at: .

      - set-slack-user-id
      - run: sudo yum install -y xmlsec1
      - run:
          name: Remove previous HPE MLDE Master RPM
          # FE-35: the slow exit of a container, or NFS caching can cause incorrect response
          # and failure executing command "rm -rf ${dir}". Wait for 5 seconds and retry the command.
          # Ignore the failure on retry, cause the failure from "rm -rf" should not stop the test suite to run.
          command: |
            export DET_PKG_NAME=$(rpm -qp --queryformat "%{NAME}" master/dist/hpe-mlde-master_*-ee_linux_amd64.rpm)
            if rpm -q $DET_PKG_NAME; then
              sudo rpm -e $DET_PKG_NAME
            fi
            echo "Cleanup state from prior runs on HOSTNAME=$HOSTNAME"

            dir="/scratch/launcher/.launcher.$HOSTNAME/checkpoints/determined-checkpoint"
            if sudo rm -rf ${dir}; then
              echo "Removed ${dir}"
            else
              sleep 5
              echo "Retry cleanup ${dir}"
              sudo rm -rf ${dir} || true
            fi

            dir="/scratch/launcher/.launcher.$HOSTNAME/archiveVolumes/"
            if sudo rm -rf ${dir}; then
              echo "Removed ${dir}"
            else
              sleep 5
              echo "Retry cleanup ${dir}"
              sudo rm -rf ${dir} || true
            fi

            dir="/scratch/launcher/.launcher.$HOSTNAME/jobs"
            if sudo rm -rf ${dir}; then
              echo "Removed ${dir}"
            else
              sleep 5
              echo "Retry cleanup ${dir}"
              sudo rm -rf ${dir} || true
            fi

      - setup-python-venv:
          determined: True
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          install-python: false
          executor: <<parameters.runner_class>>

      - run:
          name: Recreate Fresh Database
          command: |
            if systemctl is-active --quiet determined-master; then
              sudo systemctl stop determined-master
            fi
            PGPASSWORD=<<parameters.database_password>> dropdb --host=localhost --port=5432 --username=<<parameters.database_username>> --if-exists determined
            PGPASSWORD=<<parameters.database_password>> createdb --host=localhost --port=5432 --username=<<parameters.database_username>> determined

      - run:
          name: Install/Configure HPE MLDE Master
          command: |
            sudo rpm -i master/dist/hpe-mlde-master_*-ee_linux_amd64.rpm
            cat \<< EOF > ./master.yaml
            <<parameters.master_config>>
            EOF
            cat \<< EOF > ./reserved.yaml
            <<parameters.reserved_ports_znode50>>
            EOF
            echo "hostname is $(hostname)"
            # Disallow certain ports on znode50 for fewer conflicts with znode51
            if [[ "$(hostname)" == "znode50" ]] ; then cat ./reserved.yaml >> ./master.yaml ; fi
            cat ./master.yaml
            sudo cp ./master.yaml /etc/determined/master.yaml
            sudo systemctl daemon-reload
            sudo systemctl start determined-master
            # Show if there are any drained nodes
            sinfo -R
            # Resume any drained nodes due to problems killing podman processes.
            # This will return an error if all nodes do not require a resume (ignore status)
            sudo scontrol update nodename=znode5[0-1,3-4] state=resume || true
            sudo su - launcher -c "pdsh -R ssh -w znode50,znode51,znode53,znode54 mkdir -p /tmp/launcher_podman" || true
            # Cleanup podman state from any issues in prior runs
            #sudo su - launcher -c "pdsh -R ssh -w znode50,znode51,znode53,znode54 XDG_RUNTIME_DIR=/tmp/launcher_podman podman system migrate" || true
            sinfo -R

      - run:
          name: Download Apptainer/Enroot image
          command: |
            # rocm images are not required at present
            if sudo su - launcher -c "/etc/launcher/scripts/manage-singularity-cache --cuda --cpu <<pipeline.parameters.default-pt-gpu-image>>"; then
                echo "Downloaded Singularity image <<pipeline.parameters.default-pt-gpu-image>> "
            else
                EXIT_STATUS=$?
                echo "Failed downloading Singularity image. Received exit code $EXIT_STATUS"
                #Ignore the other failures except for IMAGE_REF_NOT_FOUND_IN_DOC=18 or DOC_FILE_NOT_FOUND=11
                if [[ $EXIT_STATUS -eq 18 || $EXIT_STATUS -eq 11 ]]; then
                    exit $EXIT_STATUS
                else
                    exit 0
                fi
            fi
            if sudo su - launcher -c "ENROOT_RUNTIME_PATH=/tmp/launcher /etc/launcher/scripts/manage-enroot-cache -s /lustre/ssd/foundation_engineering/ --cuda --cpu <<pipeline.parameters.default-pt-gpu-image>>"; then
                echo "Downloaded Enroot image <<pipeline.parameters.default-pt-gpu-image>> "
            else
                EXIT_STATUS=$?
                echo "Failed downloading Enroot image. Received exit code $EXIT_STATUS"
                #Ignore the other failures except for IMAGE_REF_NOT_FOUND_IN_DOC=18 or DOC_FILE_NOT_FOUND=11
                if [[ $EXIT_STATUS -eq 18 || $EXIT_STATUS -eq 11 ]]; then
                    exit $EXIT_STATUS
                else
                    exit 0
                fi
            fi

      - wait-for-master:
          host: localhost

      - run:
          name: Populate determined user agent values
          command: |
            id <<parameters.cluster_unix_user>> || sudo useradd <<parameters.cluster_unix_user>>
            TOKEN=$(
              curl "<<parameters.determined_master_host>>/api/v1/auth/login" \
                -f \
                -X POST \
                --data-binary @- \<< EOF | jq -r '.token'
              {
                  "username": "<<parameters.determined_admin_username>>",
                  "password": "<<parameters.determined_admin_password>>"
              }
            EOF
            )
            curl "<<parameters.determined_master_host>>/api/v1/users/2" \
              -f \
              -X PATCH \
              -H "Authorization: Bearer ${TOKEN}" \
              --data-binary @- \<< EOF
              {"agentUserGroup": {
                "agentUid": $(id -u <<parameters.cluster_unix_user>>),
                "agentUser": "<<parameters.cluster_unix_user>>",
                "agentGid": $(id -g <<parameters.cluster_unix_user>>),
                "agentGroup": "$(id -gn <<parameters.cluster_unix_user>>)"
                }
              }
            EOF

      - when:
          condition:
            equal: ["-A", <<parameters.agent-use>>]
          steps:
            - run:
                name: Allocate Resources on Cluster
                # Sed command reduces the slots to 8 to match '-N2 --gpus-per-node=4' usage below.
                # Affects test e2e_tests/tests/cluster/test_slurm.py:test_mnist_pytorch_distributed
                command: |
                  DET_MASTER_HOST=<<parameters.determined_master_host>>
                  # determined_master_host is localhost, so use actual hostname to pass to the agent
                  MASTER_HOST=$(hostname)
                  MASTER_PORT=$(echo $DET_MASTER_HOST | cut -d: -f2)
                  sed -i 's/slots_per_trial: 16/slots_per_trial: 8/' examples/tutorials/mnist_pytorch/distributed.yaml
                  sudo cp agent/dist/determined-agent_linux_amd64_v1/determined-agent /scratch/launcher/.launcher.$HOSTNAME
                  # Include 40 minute time limit, name the job (determined-agent-$HOSTNAME) so we can selectively kill it
                  sudo srun --uid launcher --export=ALL -N2 --gpus-per-node=4 --time=40 -Jdetermined-agent-$HOSTNAME \
                  --chdir=/scratch/launcher/.launcher.$HOSTNAME  /scratch/launcher/.launcher.$HOSTNAME/determined-agent \
                  --master-host=$MASTER_HOST  --master-port=$MASTER_PORT --resource-pool=default --container-runtime=singularity  --slot-type=cuda \
                  --image-root=/lustre/hdd/foundation_engineering/images
                background: true
            - run:
                name: Query the job to ensure Determined Agent is running
                command: |
                  # 60 tries gives 30 minutes of tries until the query times out
                  tries=60
                  squeue -u launcher -o "%j %T"
                  # Queries until the jobname is shown in a running state. The agent job must be running to run e2e tests
                  until [[ -n $(squeue -u launcher -o "%j %T" | grep "determined-agent-$HOSTNAME" | grep "RUNNING") ]] ; do
                      if [[ $((--tries)) -eq 0 ]]; then
                        echo "The job determined-agent-$HOSTNAME did not start. Please check if there are jobs in the queue preventing this job from starting"
                        exit 1
                      fi
                      echo "Waiting 30s to query for the job name again..."
                      sleep 30
                      echo "Retrying job query..."
                      squeue -u launcher -o "%j %T"
                  done
            - run:
                name: Query the slot count to ensure slots are allocated
                command: |
                  tries=20
                  det slot list
                  until [[ $(det slot list | wc -l) -gt 2 ]] ; do
                      if [[ $((--tries)) -eq 0 ]]; then
                        echo "ERROR: determined-agent failed to register at least 2 slots. Check the 'Transfer and Allocate agent resources on VM' for any failures."
                        exit 1
                      fi
                      echo "Waiting 5s to query slots again..."
                      sleep 5
                      echo "Retrying slot query..."
                      det slot list
                  done

      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: localhost
          managed-devcluster: false
          extra-pytest-flags: <<parameters.extra-pytest-flags>>

      - store_test_results:
          path: /tmp/test-results/

      - when:
          condition:
            equal: ["-A", <<parameters.agent-use>>]
          steps:
            - run:
                name: Deallocate Agent Resources on cluster
                command: |
                  scancel -u $USER --jobname=determined-agent-$HOSTNAME

      - slack/status:
          fail_only: True
          only_for_branches: main
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E slurm job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<pipeline.parameters.default-slack-channel>>

  test-e2e:
    parameters:
      tf2:
        type: boolean
        default: false
      mark:
        type: string
      parallelism:
        type: integer
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
      managed-devcluster:
        type: boolean
        default: false
      postgres-version:
        type: string
        default: "10"
      agent-version:
        type: string
        default: ""
      extra-pytest-flags:
        type: string
        default: ""
      wait-for-master:
        type: boolean
        default: true
      run-minikubes:
        type: boolean
        default: false
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    parallelism: <<parameters.parallelism>>
    environment:
      DET_POSTGRES_VERSION: <<parameters.postgres-version>>
      DET_AGENT_VERSION: <<parameters.agent-version>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .

      - setup-python-venv:
          determined: True
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - when:
          condition: <<parameters.run-minikubes>>
          steps:
            - kubernetes/install-kubectl
            - run:
                name: Install minikube
                command: |
                  curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && sudo install minikube-linux-amd64 /usr/local/bin/minikube
            - run:
                name: Start defaultrm minikube
                command: minikube start --profile defaultrm
            - run:
                name: Start additionalrm minikube
                command: minikube start --profile additionalrm

      - install-devcluster
      - unless:
          condition: <<parameters.managed-devcluster>>
          steps:
            - start-devcluster:
                devcluster-config: <<parameters.devcluster-config>>
                target-stage: <<parameters.target-stage>>

      - pull-task-images:
          tf2: <<parameters.tf2>>

      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: localhost
          managed-devcluster: <<parameters.managed-devcluster>>
          extra-pytest-flags: <<parameters.extra-pytest-flags>>
          wait-for-master: <<parameters.wait-for-master>>

      - store_test_results:
          path: /tmp/test-results/

      - when:
          condition: <<parameters.managed-devcluster>>
          steps:
            - store_artifacts:
                path: /tmp/devcluster/
                destination: devcluster-logs
            - store_artifacts:
                path: /tmp/priority_scheduler
                destination: devcluster-priority_scheduler-logs

  test-perf:
    parameters:
      snapshot-after-migrations:
        type: boolean
        default: false
      deploy-db:
        type: boolean
        default: false
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - queue/until_front_of_line:
          only-on-branch: main
          time: "120" # Wait two hours at most. Adjust this over time.
      - checkout
      - add-and-fetch-upstream
      - attach_workspace:
          at: .
      - setup-python-venv:
          determined: True
          executor: <<pipeline.parameters.machine-image>>
      - install-devcluster
      - run:
          name: Install upload deps
          command: tools/scripts/retry.sh pip install requests psycopg2-binary

      - when:
          condition: <<parameters.deploy-db>>
          steps:
            - run:
                name: Select snapshot to use
                command: |
                  echo 'export PERF_SNAPSHOT_TO_USE="perf-test-base-snapshot"' >> "$BASH_ENV"

                  SNAPSHOT_COMMITS=$(aws rds describe-db-snapshots \
                    --region="us-west-2" \
                    --query "DBSnapshots[?TagList[?Key=='ci-snapshot']].DBSnapshotIdentifier" \
                    --output json | jq -r '.[] | split("-")[3]')
                  echo "Snapshot commits (${SNAPSHOT_COMMITS})"

                  for ((n=0; n<=1000; n++)); do
                    COMMIT=$(git log --format="%H" -n 1 --skip=$n)

                    if [[ " $SNAPSHOT_COMMITS " =~ .*"$COMMIT".* ]]; then
                      echo "export PERF_SNAPSHOT_TO_USE=\"ci-snapshot-commit-${COMMIT}\"" >> "$BASH_ENV"
                      break
                    fi
                  done

                  source $BASH_ENV
                  echo "Deciding to use $PERF_SNAPSHOT_TO_USE"
            - run:
                name: Wait for snapshot to be available
                command: |
                  aws rds wait db-snapshot-available \
                    --region "us-west-2" \
                    --db-snapshot-identifier "${PERF_SNAPSHOT_TO_USE}"
            - run:
                name: Deploy database
                command: |
                  aws rds restore-db-instance-from-db-snapshot \
                    --region="us-west-2" \
                    --db-snapshot-identifier="${PERF_SNAPSHOT_TO_USE}" \
                    --db-instance-identifier="ci-perf-db-${CIRCLE_BUILD_NUM}" \
                    --no-multi-az \
                    --no-publicly-accessible \
                    --no-auto-minor-version-upgrade \
                    --db-parameter-group-name="logquerieslong" \
                    --tags "Key=ci-snapshot" \
                    --vpc-security-group-ids="${PERF_DB_SECURITY_GROUP_ID}" \
                no_output_timeout: 30m
            - run:
                name: Wait for database to be ready
                command: |
                  aws rds wait db-instance-available \
                    --region="us-west-2" \
                    --db-instance-identifier="ci-perf-db-${CIRCLE_BUILD_NUM}"
                no_output_timeout: 30m
            - run:
                name: Get db instance host
                command: |
                  echo "export PERF_DB_HOST=$(aws rds describe-db-instances \
                    --region us-west-2 \
                    --db-instance-identifier "ci-perf-db-${CIRCLE_BUILD_NUM}" \
                    --query "DBInstances[0].Endpoint.Address" \
                    --output text)" >> "$BASH_ENV"
                  source $BASH_ENV
                  echo "perf db host ${PERF_DB_HOST}"

      - run:
          name: Add SSH key
          command: echo "${PERF_DB_BASTION_SSH_KEY}" | base64 --decode | ssh-add -
      - run:
          name: Port forward to bastion instance
          command: ssh -L 5432:${PERF_DB_HOST}:5432 -N -f ubuntu@$PERF_DB_BASTION_HOST
      - start-devcluster:
          target-stage: master
          devcluster-config: perftest.devcluster.yaml
      - run:
          name: Wait and record any migrations ran
          command: python .circleci/scripts/wait_for_perf_migration_upload_results.py

      - when:
          condition: <<parameters.snapshot-after-migrations>>
          steps:
            - run:
                name: Take and wait for RDS snapshot, only on main and when migrations were applied
                command: |
                  if [ -f /tmp/no-migrations-needed ]; then
                    echo "/tmp/no-migrations-needed exists, no need to take a snapshot"
                    exit 0
                  fi

                  COMMIT=$(git log -1 --pretty=format:%H)
                  echo "Taking snapshot"
                  aws rds create-db-snapshot \
                    --region="us-west-2" \
                    --db-instance-identifier="${PERF_DB_AWS_NAME}" \
                    --db-snapshot-identifier="ci-snapshot-commit-${COMMIT}" \
                    --tags "Key=ci-snapshot"

                  echo "Snapshot taken now waiting for it to become completed"
                  aws rds wait db-snapshot-completed \
                    --region="us-west-2" \
                    --db-snapshot-identifier="ci-snapshot-commit-${COMMIT}"
                  echo "Snapshot completed"
      - run:
          name: Build performance test Docker image
          command: make -C performance build
      - run:
          name: Run performance test
          command: |
            export PERF_DOCKER_FLAGS="--network=host"
            export PERF_K6_FLAGS='-e DET_ADMIN_USERNAME="admin" \
              -e DET_ADMIN_PASSWORD="" \
              -e model_name="tnjpuojqzbluqiyyqilftulsw" \
              -e model_version_number="1" \
              -e trial_id="8282" \
              -e experiment_id="100" \
              -e task_id="backported.8282" \
              -e metric_name="85c9" \
              -e metric_type="METRIC_TYPE_TRAINING" \
              -e batches="1800" \
              -e batches_margin="99" \
              -e resource_pool="default"'
            make -C performance run
      - run:
          name: Upload result of performance test to Postgres result db
          command: python .circleci/scripts/upload_perf_results.py ./performance/reports/latest.results.json

      - run:
          name: Delete RDS instance
          when: always
          command: |
            if [ -n "$PERF_SNAPSHOT_TO_USE" ]; then
              aws rds delete-db-instance \
                --region="us-west-2" \
                --db-instance-identifier="ci-perf-db-${CIRCLE_BUILD_NUM}" \
                --skip-final-snapshot
            else
              echo "PERF_SNAPSHOT_TO_USE is not set. Skipping deletion."
            fi

      - slack/status:
          fail_only: true
          only_for_branches: main
          failure_message: ':thisisfine: A \`${CIRCLE_JOB}\` job on branch \`${CIRCLE_BRANCH}\` has failed!'
          mentions: "U03CP4ZKY2D" # Ping Nick Blaskey for now. Eventually switch this to perf team.

  deploy:
    parameters:
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m5.large"
      cluster-id:
        type: string
        default: determined-${CIRCLE_BRANCH////--}
      max-dynamic-agents:
        type: integer
        default: 1
      enable-cors:
        type: boolean
        default: false
      reattach-enabled:
        type: boolean
        default: false
      extra-tags:
        type: string
        default: ""
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - queue/until_front_of_line:
          only-on-branch: main
          # Basically wait forever -- we would prefer not to fail deploys, and
          # we'll likely never be this backed up.
          time: "10000"
      - checkout
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          executor: <<pipeline.parameters.docker-image>>
      - deploy-aws-cluster:
          cluster-id: <<parameters.cluster-id>>
          det-version: ${CIRCLE_SHA1}
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          enable-cors: <<parameters.enable-cors>>
          reattach-enabled: <<parameters.reattach-enabled>>
          deployment-type: simple
          extra-tags: <<parameters.extra-tags>>
      - slack/status:
          fail_only: true
          failure_message: ':thisisfine: A \`${CIRCLE_JOB}\` job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: "${SLACK_USER_ID}"
          channel: <<pipeline.parameters.default-slack-channel>>

  deploy-latest-gke:
    parameters:
      det-version:
        type: string
      cluster-id:
        type: string
        default: latest-ee-gke
      region:
        type: string
        default: us-west1
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - set-slack-user-id
      - gcloud/install:
          version: "319.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: gcloud container clusters get-credentials <<parameters.cluster-id>> --project determined-ai --region <<parameters.region>>
          name: Get Kubeconfig
      - helm/upgrade-helm-chart:
          chart: helm/charts/determined
          helm-version: v3.2.4
          namespace: "default"
          wait: true
          release-name: <<parameters.cluster-id>>
          values-to-override: |
            detVersion=<<parameters.det-version>>,\
            maxSlotsPerPod=4,\
            checkpointStorage.type=gcs,\
            checkpointStorage.bucket=det-ci,\
            imagePullSecretName=release-cred,\
            masterPort=80,\
            security.authz.type=rbac,\
            enterpriseEdition=true

  test-e2e-aws:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      compute-agent-instance-type:
        type: string
        default: g4dn.xlarge
      aux-agent-instance-type:
        type: string
        default: m5.large
      max-dynamic-agents:
        type: integer
        default: 1
      parallelism:
        type: integer
        default: 1
      enable-tls:
        type: boolean
        default: false
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      environment-gpu-enabled:
        type: string
        default: "1"
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - when:
          condition: <<parameters.enable-tls>>
          steps:
            - generate-tls-cert
      - setup-aws-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          extra-tags: test-mark=<<parameters.mark>>
          det-version: ${CIRCLE_SHA1}
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
          master-scheme: ${MASTER_SCHEME:-http}
          master-port: ${MASTER_PORT:-8080}
          master-cert: ${MASTER_TLS_CERT}
          master-cert-name: ${MASTER_CERT_NAME}
          wait-for-master: false
      - locate-cloudwatch-logs:
          cluster-id: ${CLUSTER_ID}
      - terminate-aws-cluster:
          cluster-id: ${CLUSTER_ID}
      - store_test_results:
          path: /tmp/test-results/
      - slack/status:
          fail_only: True
          only_for_branches: main
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E GPU job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>

  test-e2e-gke:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      gke-version:
        type: string
        default: <<pipeline.parameters.gke-version>>
      machine-type:
        type: string
        default: "n1-standard-8"
      num-machines:
        type: integer
        default: 1
      gpu-type:
        type: string
        default: "nvidia-tesla-t4"
      gpus-per-machine:
        type: integer
        default: 1
      slot-type:
        type: string
        default: gpu
      slot-resource-requests-cpu:
        type: integer
        default: 0
      max-slots-per-pod:
        type: integer
        default: 1
      region:
        type: string
        default: "us-west1"
      node-locations:
        type: string
        default: "us-west1-b"
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
      environment-gpu-enabled:
        type: string
        default: "1"
      environment-image:
        type: string
        default: determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-0.27.1
      accel-node-taints:
        type: string
        default: ""
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - queue/until_front_of_line:
          only-on-branch: main
          # Basically wait forever -- we would prefer not to fail tests, and
          # we'll likely never be this backed up.
          time: "10000"
      - setup-gke-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          labels: test-mark=<<parameters.mark>>
          det-version: ${CIRCLE_SHA1}
          gke-version: <<parameters.gke-version>>
          machine-type: <<parameters.machine-type>>
          num-machines: <<parameters.num-machines>>
          gpu-type: <<parameters.gpu-type>>
          gpus-per-machine: <<parameters.gpus-per-machine>>
          slot-type: <<parameters.slot-type>>
          slot-resource-requests-cpu: <<parameters.slot-resource-requests-cpu>>
          max-slots-per-pod: <<parameters.max-slots-per-pod>>
          region: <<parameters.region>>
          node-locations: <<parameters.node-locations>>
          environment-image: <<parameters.environment-image>>
          accel-node-taints: <<parameters.accel-node-taints>>
      - set-google-application-credentials
      - when:
          condition: <<parameters.environment-image>>
          steps:
            - run:
                command: |
                  echo 'export TF2_GPU_IMAGE="<<parameters.environment-image>>"' >> "$BASH_ENV"
                name: override test env images.
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
      - store_test_results:
          path: /tmp/test-results/
      - terminate-gke-cluster:
          cluster-id: ${CLUSTER_ID}
          region: <<parameters.region>>
      - slack/status:
          fail_only: True
          only_for_branches: main
          failure_message: ':thisisfine: A \`<<parameters.mark>>\` E2E GKE GPU job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>

  test-e2e-shared-cluster:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      environment-gpu-enabled:
        type: string
        default: "0"
      test-type:
        type: string
    circleci_ip_ranges: true
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
      SHARED_CLUSTER: true
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - attach_workspace:
          at: .
      - setup-python-venv:
          install-python: false
          determined: true 
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run:
          name: Create Namespace & Cert Name
          command: |
            TIMESTAMP=$(date +"%Y%m%d%H%M%S")
            uuid=$(cat /proc/sys/kernel/random/uuid)
            uuid=${uuid:0:8}
            echo "GENERATED_NAMESPACE=test-<<parameters.test-type>>-${TIMESTAMP}-${uuid}-${CIRCLE_NODE_INDEX}" >> $BASH_ENV
      - generate-tls-cert
      - setup-shared-cluster:
          det-version: ${CIRCLE_SHA1}-shared-cluster
          labels: test-mark=<<parameters.mark>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
      - set-google-application-credentials
      - run:
          name: Set initial user password
          command: |
            echo "export INITIAL_USER_PASSWORD=${INITIAL_USER_PASSWORD}" >> $BASH_ENV
      - run:
          name: Wait for master connection
          command: |
            set +o pipefail
            export DET_USER=admin DET_PASS=${INITIAL_USER_PASSWORD}
            export DET_MASTER_TLS_CERT=${MASTER_TLS_CERT} DET_MASTER_CERT_NAME=${MASTER_CERT_NAME}
            for i in {1..10}; do
              yes | det -m https://${MASTER_HOST}:${MASTER_PORT} user whoami | grep "logged in" && break || \
              echo "Trying to connect to master host again in 5 seconds" && sleep 5
            done
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
          master-scheme: ${MASTER_SCHEME:-http}
          master-port: ${MASTER_PORT:-8080}
          master-cert: ${MASTER_TLS_CERT}
          master-cert-name: ${MASTER_CERT_NAME}
          wait-for-master: false
      - store_test_results:
          path: /tmp/test-results/

  test-det-deploy:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - run: docker load --input build/master.image
      - run: docker load --input build/agent.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  test-stress:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - run: docker load --input build/master.image
      - run: docker load --input build/agent.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  test-aws-fs:
    parameters:
      cluster-id-prefix:
        type: string
      deployment-type:
        type: string
      compute-agent-instance-type:
        type: string
        default: g4dn.xlarge
      aux-agent-instance-type:
        type: string
        default: m5.large
      max-dynamic-agents:
        type: integer
        default: 1
      slack-mentions:
        type: string
        default: ""
      slack-channel:
        type: string
        default: ""
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - setup-aws-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          extra-tags: test-mark=fs
          det-version: ${CIRCLE_SHA1}
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
          deployment-type: <<parameters.deployment-type>>
      - run:
          name: Set DET_MASTER for further action.
          command: |
            echo "export DET_MASTER=${MASTER_SCHEME:-http}://${MASTER_HOST}:${MASTER_PORT:-8080}" >> "$BASH_ENV"
      - when:
          condition:
            equal: [<<parameters.deployment-type>>, "fsx"]
          steps:
            - run:
                name: Test FSx/Lustre
                command: |
                  DET_USER=admin DET_PASS=${INITIAL_USER_PASSWORD} \
                  det cmd run --config resources.slots=1 'mount | grep lustre && echo MARKER' | grep MARKER
      - when:
          condition:
            equal: [<<parameters.deployment-type>>, "efs"]
          steps:
            - run:
                name: Test EFS
                command: |
                  DET_USER=admin DET_PASS=${INITIAL_USER_PASSWORD} \
                  det cmd run --config resources.slots=1 'mount | grep efs && echo MARKER' | grep MARKER
      - locate-cloudwatch-logs:
          cluster-id: ${CLUSTER_ID}
      - terminate-aws-cluster:
          cluster-id: ${CLUSTER_ID}
      - slack/status:
          fail_only: True
          only_for_branches: main
          failure_message: ':thisisfine: AWS FS \`<<parameters.deployment-type>>\` job on branch \`${CIRCLE_BRANCH}\` has failed! Author Email: \`${AUTHOR_EMAIL}\`'
          mentions: <<parameters.slack-mentions>>
          channel: <<parameters.slack-channel>>
  terminate-vpc-circleci:
    parameters:
        gcloud-service-key:
          default: GCLOUD_SERVICE_KEY
          description: The gcloud service key
          type: env_var_name
        google-compute-zone:
          default: GOOGLE_COMPUTE_ZONE
          description: The Google compute zone to connect with via the gcloud CLI
          type: env_var_name
        google-project-id:
          default: GOOGLE_PROJECT_ID
          description: The Google project ID to connect with via the gcloud CLI
          type: env_var_name
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - gcloud/install:
          version: "412.0.0"
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - set-google-application-credentials
      - run:
          name: Terminate any unnecessary VPC networks and firewall rules from CircleCI jobs
          command: |
            ./.circleci/scripts/terminate-vpc-circleci.sh

  run-shared-cluster-cleanup:
      parameters:
        cluster-id:
          type: string
          default: ${GKE_CLUSTER_NAME}
        region:
          type: string
          default: ${GKE_REGION}
        gcloud-service-key:
          default: GCLOUD_SERVICE_KEY
          description: The gcloud service key
          type: env_var_name
        google-compute-zone:
          default: GOOGLE_COMPUTE_ZONE
          description: The Google compute zone to connect with via the gcloud CLI
          type: env_var_name
        google-project-id:
          default: GOOGLE_PROJECT_ID
          description: The Google project ID to connect with via the gcloud CLI
          type: env_var_name
      circleci_ip_ranges: true
      docker:
        - image: <<pipeline.parameters.docker-image>>
      steps:
        - set-cluster-id:
              cluster-id: <<parameters.cluster-id>>
        - gcloud/install:
              version: "412.0.0"
        - kubernetes/install-kubectl
        - gcloud/initialize:
            gcloud-service-key: <<parameters.gcloud-service-key>>
            google-compute-zone: <<parameters.google-compute-zone>>
            google-project-id: <<parameters.google-project-id>>
        - run:
            command: |
              tries=5
              until gcloud components install gke-gcloud-auth-plugin --quiet; do
                if [[ $((--tries)) -eq 0 ]]; then
                  exit 1
                fi
                sleep 15
              done
              echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $BASH_ENV
            name: Install GKE auth plugin
        - run:
            name: Get Kubeconfig
            command: gcloud container clusters get-credentials ${CLUSTER_ID} --project ${GOOGLE_PROJECT_ID} --region <<parameters.region>>
        - run:
            name: Delete GKE CI Cluster Namespaces
            command: |
              kubectl get namespace | grep -Eo "^test-cpu-[a-z0-9]+-[a-z0-9]+-[0-9]" | xargs -L1 kubectl delete namespace || true
        - run:
            name: Delete GCS CI Buckets
            command: |
              gsutil ls -p ${GOOGLE_PROJECT_ID} | grep -Eo "^gs://test-cpu-[a-z0-9]+-[a-z0-9]+-[0-9]-bucket" | xargs -L1 gsutil -m rm -r || true

workflows:
  lint:
    jobs:
      - build-proto
      - check-py-bindings:
          requires:
            - build-proto
      - check-ts-bindings:
          requires:
            - build-proto
      - lint-docs
      - lint-python
      - lint-go
      - lint-sql
      - lint-react
      - lint-secrets

  previews:
    jobs:
      - build-proto
      - build-helm
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - publish-docs-new:
          requires:
            - build-docs
          context: aws
          filters: *any-upstream

  test-cli:
    jobs:
      - test-cli:
          matrix:
            parameters:
              executor-name:
                - "python-38"
                - "python-39"
                - "win/default"
  test-unit:
    jobs:
      - test-unit-react
      - test-unit-harness-cpu
      - test-unit-harness-tf2
      - test-unit-harness-pytorch2-cpu
      - test-unit-harness-pytorch2-gpu

      - test-unit-harness-gpu:
          filters: *any-upstream
      - test-unit-harness-gpu-deepspeed:
          filters: *any-upstream
      - test-unit-harness-gpu-parallel:
          filters: *any-upstream

      # allows forks to request run approval instead of failing
      - request-gpu-unit-tests:
          type: approval
          filters: *any-fork
      - test-unit-harness-gpu:
          name: f-test-unit-harness-gpu
          filters: *any-fork
      - test-unit-harness-gpu-deepspeed:
          name: f-test-unit-harness-gpu-deepspeed
          filters: *any-fork
      - test-unit-harness-gpu-parallel:
          name: f-test-unit-harness-gpu-parallel
          filters: *any-fork

      - test-unit-model-hub
      - test-unit-storage:
          context: storage-unit-tests
          filters: *any-upstream
      - python-coverage:
          requires:
            - test-unit-harness-cpu
            - test-unit-harness-gpu
            - test-unit-harness-gpu-parallel
            - test-unit-harness-tf2
            - test-unit-harness-pytorch2-cpu
            - test-unit-harness-pytorch2-gpu
            - test-unit-model-hub
            - test-unit-storage

  test-go:
    jobs:
      - test-intg-master:
          context: storage-unit-tests
      - test-intg-agent
      - go-coverage:
          requires:
            - test-intg-master
            - test-intg-agent
      - check-go-coverage-master:
          requires:
            - go-coverage
      - check-go-coverage-agent:
          requires:
            - go-coverage

  test-e2e:
    jobs:
      - build-proto
      - build-helm
      - build-react:
          dev-mode: true
      - test-e2e-react:
          context: playwright
          filters: *any-upstream
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - build-go:
          context: github-read
      - package-and-push-system-local:
          requires:
            - build-react
            - build-docs
          context: github-read
          filters:
            branches:
              only:
                - main

      - package-and-push-system-dev:
          requires:
            - build-react
            - build-docs
          context:
            - determined-ee
            - github-read
          filters:
            branches:
              ignore:
                - /pull\/.*/

      - check-docs-links:
          requires:
            - build-docs
          filters:
            branches:
              only:
                - main

      - test-debian-packaging:
          requires:
            - package-and-push-system-local
          filters:
            branches:
              only:
                - main

      - test-e2e-slurm:
          name: test-e2e-slurm-misconfigured
          slack-mentions: "${SLACK_USER_ID}"
          requires:
            - package-and-push-system-local
          mark: e2e_slurm_misconfigured
          master_config: |
            task_container_defaults:
              slurm:
                sbatch_args:
                  - --time=04:00:00
              environment_variables:
                # Some ports are not working, disable them so distributed jobs work.
                - NCCL_IB_HCA=mlx6_0:0
            checkpoint_storage:
              type: shared_fs
              host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
              storage_path: determined-checkpoint
              save_experiment_best: 0
              save_trial_best: 1
              save_trial_latest: 1
            db:
              user: postgres
              host: localhost
              port: 5432
              name: determined
              password: launcher
            resource_manager:
              type: slurm
              master_host: junkmaster
              master_port: 8080
              host: localhost
              port: 8181
              protocol: http
              slot_type: cuda
              user_name: launcher
              group_name: hpcd
              singularity_image_root: /lustre/hdd/foundation_engineering/images
              job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
              auth_file: /home/launcher/.launcher.$HOSTNAME.token
              path: /opt/singularity/bin:/usr/local/bin:${PATH}
              ld_library_path:

      - test-e2e-slurm:
          name: test-e2e-slurm-gpu
          slack-mentions: "${SLACK_USER_ID}"
          mark: "e2e_slurm_gpu"
          requires:
            - package-and-push-system-local

    # Singularity over SLURM test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
          matrix:
            parameters:
              name: [test-e2e-slurm-singularity-gcp]
              always-run: [true]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go

    # Podman over SLURM test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
          matrix:
            parameters:
              name: [test-e2e-slurm-podman-gcp]
              container-run-type: ["podman"]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go

    # Enroot over SLURM test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
          matrix:
            parameters:
              name: [test-e2e-slurm-enroot-gcp]
              container-run-type: ["enroot"]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go

    # Singularity over PBS test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
          matrix:
            parameters:
              name: [test-e2e-pbs-singularity-gcp]
              workload-manager: ["pbs"]
              always-run: [true]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
          requires:
            - build-go

    # Podman over PBS test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
          matrix:
            parameters:
              name: [test-e2e-pbs-podman-gcp]
              container-run-type: ["podman"]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
              extra-pytest-flags: ["-k 'not test_slurm_verify_home'"]
          requires:
            - build-go

    # Enroot over PBS test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
          matrix:
            parameters:
              name: [test-e2e-pbs-enroot-gcp]
              container-run-type: ["enroot"]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
          requires:
            - build-go

    # Podman over SLURM test using Agent on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
          matrix:
            parameters:
              name: [test-e2e-slurm-agent-podman-gcp]
              agent-use: ["-A"]
              container-run-type: ["podman"]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
              extra-pytest-flags: ["-k 'not test_slurm_verify_home'"]
          requires:
            - build-go

      - test-e2e:
          name: test-e2e-rbac
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_rbac
          devcluster-config: single-rbac.devcluster.yaml

      - test-e2e:
          name: test-e2e-cpu
          requires:
            - build-go
          parallelism: 6
          tf2: true
          mark: e2e_cpu

      - test-e2e:
          name: test-e2e-cpu-double
          requires:
            - build-go
          parallelism: 3
          tf2: true
          mark: e2e_cpu_2a
          target-stage: agent2
          devcluster-config: double-priority.devcluster.yaml

      - test-e2e:
          name: test-e2e-cpu-oauth
          requires:
            - build-go
          parallelism: 1
          devcluster-config: oauth.devcluster.yaml
          mark: test_oauth
          target-stage: agent1

      - test-e2e:
          name: test-e2e-cpu-model-registry-rbac
          requires:
            - build-go
          parallelism: 1
          devcluster-config: rbac-model-registry.yaml
          mark: test_model_registry_rbac
          target-stage: agent1

      - test-e2e:
          name: test-e2e-managed-devcluster
          requires:
            - build-go
          parallelism: 4
          tf2: true
          mark: managed_devcluster
          managed-devcluster: true
          # Managed devcluster restarts the master over the course of the tests,
          # so `compare_stats` cannot get the full logs from `det master logs`.
          extra-pytest-flags: "--no-compare-stats"

      - test-e2e:
          name: test-e2e-multi-k8s
          requires:
            - build-go
          parallelism: 1
          tf2: true
          mark: e2e_multi_k8s
          target-stage: master
          devcluster-config: multi-k8s.devcluster.yaml
          run-minikubes: true

      - test-e2e:
          name: test-e2e-port-registry
          requires:
            - build-go
          parallelism: 1
          devcluster-config: port-registry.devcluster.yaml
          mark: port_registry
          target-stage: agent

      - test-e2e:
          name: test-e2e-cpu-elastic
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_elastic
          devcluster-config: elastic.devcluster.yaml
          target-stage: agent

      - test-e2e:
          name: test-e2e-postgres10-with-ssl
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: "10"

      - test-e2e:
          name: test-e2e-postgres14-with-ssl
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: "14"

      - test-e2e:
          name: test-e2e-old-agent-versions
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: custom-agent-version.devcluster.yaml
          target-stage: agent
          matrix:
            parameters:
              agent-version: ["0.17.10"]

      - test-e2e:
          name: test-e2e-agent-connection-loss
          requires:
            - build-go
          parallelism: 1
          mark: e2e_cpu_agent_connection_loss
          devcluster-config: agent-no-connection.devcluster.yaml
          target-stage: agent
          wait-for-master: false
          extra-pytest-flags: "--no-compare-stats"

      - test-e2e:
          context: okta
          name: test-e2e-saml
          requires:
            - build-go
          parallelism: 1
          devcluster-config: saml.devcluster.yaml
          mark: e2e_saml

      - test-perf:
          name: test-perf
          snapshot-after-migrations: true
          deploy-db: false
          requires:
            - build-go
          context:
            - perf-tests
            - aws
          filters:
            branches:
              only:
                - main

      - deploy-latest-gke:
          name: deploy-latest-gke-master
          context: gcp
          det-version: $CIRCLE_SHA1
          filters:
            branches:
              only:
                - main
          requires:
            - package-and-push-system-dev

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context:
            - aws
            - determined-ee
            - aws-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context:
            - aws
            - determined-ee
            - aws-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context:
            - gcp-ci
            - determined-ee
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context:
            - gcp-ci
            - determined-ee
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              max-slots-per-pod: [4]

      # Legacy tf 2.4 tests.
      - test-e2e-gke:
          name: test-e2e-gke-single-gpu-tfonly
          context:
            - gcp-ci
            - determined-ee
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu-tfonly"]
              mark: ["e2e_gpu and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.10

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context:
            - gcp-ci
            - determined-ee
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              max-slots-per-pod: [1]

      - test-e2e-gke:
          name: test-e2e-gke-k8s-reattach
          context:
            - gcp-ci
            - determined-ee
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-k8s"]
              mark: ["e2e_k8s"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              accel-node-taints: ["accel=truth:NoSchedule"]
              max-slots-per-pod: [1]

      - test-det-deploy:
          name: test-det-deploy-local
          requires:
            - package-and-push-system-local
          matrix:
            parameters:
              parallelism: [2]
              mark: ["det_deploy_local"]
              det-version: [$CIRCLE_SHA1]
    
  test-e2e-gke-shared-cluster:
    unless: << pipeline.parameters.do_nightly_tests >>
    jobs:
      - package-and-push-system-dev-small
      
      - test-e2e-shared-cluster:
          name: test-e2e-shared-cluster-cpu
          context: 
            - gcp-shared-cluster
            - gcp-ci-cluster-default-user-credentials
          requires:
            - package-and-push-system-dev-small
          parallelism: 3
          mark: "e2e_gpu and not gpu_required"
          test-type: cpu

  test-e2e-longrunning:
    jobs:
      # Build and publish artifacts used in tests
      - build-helm
      - build-proto
      - build-react:
          dev-mode: true
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - package-and-push-system-dev:
          requires:
            - build-react
            - build-docs
          context: determined-ee
          filters: *any-upstream

      # Distributed tests
      - request-e2e-cpu-distributed:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-e2e-cpu-distributed
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [16]

      # DeepSpeed tests
      - request-deepspeed-tests:
          type: approval
          filters: *upstream-feature-branch

        # DeepSpeed tests do not work on K80s, so we need V100 instances.
      - test-e2e-aws:
          name: test-e2e-deepspeed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-deepspeed-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              compute-agent-instance-type: ["g4dn.12xlarge"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              max-dynamic-agents: [2]

      # K8s GPU tests
      - request-k8-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context:
            - gcp-ci
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context:
            - gcp-ci
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              max-slots-per-pod: [4]

      # Requestable legacy tf 2.4 tests.
      - test-e2e-gke:
          name: test-e2e-gke-single-gpu-tfonly
          context:
            - gcp-ci
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu-tfonly"]
              mark: ["e2e_gpu and tensorflow2"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              environment-image:
                - determinedai/environments:cuda-11.1-pytorch-1.9-tf-2.4-gpu-0.19.10

      # K8s CPU tests
      - request-k8-tests-cpu:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context:
            - gcp-ci
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cpu
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              max-slots-per-pod: [1]

      - test-e2e-gke:
          name: test-e2e-gke-k8s-reattach
          context:
            - gcp-ci
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cpu
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-k8s"]
              mark: ["e2e_k8s"]
              parallelism: [1]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              accel-node-taints: ["accel=truth:NoSchedule"]
              max-slots-per-pod: [1]

      # Nightly distributed tests
      - request-gpu-distributed-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-distributed-nightly
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - hugging-face
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-distributed-nightly
            - package-and-push-system-dev
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]

      # Perf tests.
      - request-perf-tests:
          type: approval
          filters: *upstream-feature-branch

      - build-go:
          context: github-read
          requires:
            - request-perf-tests

      - test-perf:
          name: test-perf-feature-branch
          snapshot-after-migrations: false
          deploy-db: true
          requires:
            - build-go
            - request-perf-tests
          context:
            - perf-tests
            - aws
          filters: *upstream-feature-branch

      # Nightly tests
      - request-gpu-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-nightly
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]

      # GPU tests
      - request-gpu-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]
              slack-mentions: ["${SLACK_USER_ID}"]
              slack-channel: [<<pipeline.parameters.default-slack-channel>>]

      # Request AWS FS tests
      - request-aws-fs-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-aws-fs:
          name: test-aws-fs-fsx
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-aws-fs-tests
            - package-and-push-system-dev
          cluster-id-prefix: aws-fs-fsx
          deployment-type: fsx
          slack-channel: <<pipeline.parameters.default-slack-channel>>
          slack-mentions: "${SLACK_USER_ID}"

      - test-aws-fs:
          name: test-aws-fs-efs
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-aws-fs-tests
            - package-and-push-system-dev
          cluster-id-prefix: aws-fs-efs
          deployment-type: efs
          slack-channel: <<pipeline.parameters.default-slack-channel>>
          slack-mentions: "${SLACK_USER_ID}"

  nightly:
    when: << pipeline.parameters.do_nightly_tests >>
    jobs:
      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]
      - test-e2e-aws:
          name: test-e2e-gpu-distributed
          context:
            - aws
            - determined-ee
            - hugging-face
            - aws-ci-cluster-default-user-credentials
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-deepspeed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          matrix:
            parameters:
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              compute-agent-instance-type: ["g4dn.12xlarge"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m5.large"]
              max-dynamic-agents: [16]
      - test-aws-fs:
          name: test-aws-fs-fsx
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          cluster-id-prefix: aws-fs-fsx
          deployment-type: fsx
          slack-mentions: "${SLACK_USER_ID}"
          slack-channel: <<pipeline.parameters.default-slack-channel>>

      - test-aws-fs:
          name: test-aws-fs-efs
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          cluster-id-prefix: aws-fs-efs
          deployment-type: efs
          slack-mentions: "${SLACK_USER_ID}"
          slack-channel: <<pipeline.parameters.default-slack-channel>>

      - build-proto
      - build-helm
      - build-docs:
          requires:
            - build-proto
            - build-helm
      - build-react
      - package-and-push-system-local:
          requires:
            - build-docs
            - build-react
          context: github-read
      - test-e2e-slurm:
          name: test-e2e-slurm-restart
          mark: "e2e_slurm_restart"
          slack-mentions: "${SLACK_USER_ID}"
          requires:
            - package-and-push-system-local
          extra-pytest-flags: "--no-compare-stats"
      - test-e2e-slurm:
          name: test-e2e-slurm-preemption
          mark: "e2e_slurm_preemption"
          slack-mentions: "${SLACK_USER_ID}"
          requires:
            - package-and-push-system-local
          extra-pytest-flags: "--no-compare-stats"
      - test-e2e-slurm:
          name: test-e2e-slurm-znode
          slack-mentions: "${SLACK_USER_ID}"
          requires:
            - package-and-push-system-local
          extra-pytest-flags: "--no-compare-stats"
      - test-e2e-slurm:
          name: test-e2e-slurm-enroot-znode
          matrix:
            parameters:
              mark: ["e2e_slurm and not deepspeed"]
          slack-mentions: "${SLACK_USER_ID}"
          requires:
            - package-and-push-system-local
          master_config: |
            task_container_defaults:
              slurm:
                sbatch_args:
                  - --time=04:00:00
              # The current image must be created in the launcher account before running this test
              # cd /lustre/ssd/foundation_engineering/
              # enroot import docker://determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0.sqsh
              # enroot create /lustre/ssd/foundation_engineering/determinedai+environments+cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0.sqsh
              #image: determinedai+environments+cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0
              environment_variables:
                  # Some ports are not working, disable them so distributed jobs work.
                  - NCCL_IB_HCA=mlx6_0:0
                  # Workaround XDG_RUNTIME_DIR not provided by Slurm
                  - ENROOT_RUNTIME_PATH=/tmp/launcher
            checkpoint_storage:
              type: shared_fs
              host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
              storage_path: determined-checkpoint
              save_experiment_best: 0
              save_trial_best: 1
              save_trial_latest: 1
            db:
              user: postgres
              host: localhost
              port: 5432
              name: determined
              password: launcher
            resource_manager:
              type: slurm
              master_host: $HOSTNAME
              master_port: 8080
              host: localhost
              port: 8181
              protocol: http
              slot_type: cuda
              user_name: launcher
              container_run_type: enroot
              group_name: hpcd
              singularity_image_root: /lustre/hdd/foundation_engineering/images
              job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
              auth_file: /home/launcher/.launcher.$HOSTNAME.token
              path: /opt/singularity/bin:/usr/local/bin:${PATH}
              ld_library_path:
      - test-e2e-slurm:
            name: test-e2e-slurm-agent-singularity-znode
            slack-mentions: "${SLACK_USER_ID}"
            requires:
              - package-and-push-system-local
            agent-use: "-A"
            extra-pytest-flags: "-k 'not node_not_available'"
            # For now we must skip node_not_available. There is a inconsistency with the agent where
            # the error is "failed to create experiment: slots requested exceeds cluster capacity" instead of the expected message.
            master_config: |
                task_container_defaults:
                  slurm:
                    sbatch_args:
                      - --time=04:00:00
                  environment_variables:
                    # Some ports are not working, disable them so distributed jobs work.
                    - NCCL_IB_HCA=mlx6_0:0
                checkpoint_storage:
                  type: shared_fs
                  host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
                  storage_path: determined-checkpoint
                  save_experiment_best: 0
                  save_trial_best: 1
                  save_trial_latest: 1
                db:
                  user: postgres
                  host: localhost
                  port: 5432
                  name: determined
                  password: launcher
      - terminate-vpc-circleci:
            context: ["gcp"]

      - run-shared-cluster-cleanup:
          name: gke-cleanup
          context: gcp-shared-cluster

  release:
    jobs:
      - build-helm:
          filters: *release-and-rc-filters
      - build-proto:
          filters: *release-and-rc-filters
      - build-react:
          context: determined-production
          filters: *release-and-rc-filters
      - build-docs:
          context: determined-production
          filters: *release-and-rc-filters
          requires:
            - build-helm
            - build-proto

      - upload-docs-search-index:
          requires:
            - build-docs
          context: determined-production
          filters: *release-and-rc-filters

      - package-and-push-system-rc:
          requires:
            - build-react
            - build-docs
          context:
            - determined-ee
            - github-read
          filters: *rc-filters

      - publish-python-package:
          name: publish-python-package-rc
          matrix:
            parameters:
              path: ["harness"]
          context: determined-ee
          filters: *rc-filters
          requires:
            - package-and-push-system-rc

      - package-and-push-system-release:
          requires:
            - build-react
            - build-docs
          context:
            - determined-ee
          filters: *release-filters

      - publish-python-package:
          name: publish-python-package-release
          matrix:
            parameters:
              path: ["harness"]
          context: determined-ee
          filters: *release-filters
          requires:
            - package-and-push-system-release

      - publish-docs:
          requires:
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-helm:
          requires:
            - build-helm
          context: determined-production
          filters: *release-and-rc-filters

      - publish-helm-gh:
          requires:
            - build-helm
          context: determined-production
          filters: *release-filters
