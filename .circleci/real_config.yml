# Use the latest 2.1 version of CircleCI pipeline process engine. See: https://circleci.com/docs/2.0/configuration-reference
version: 2.1

orbs:
  codecov: codecov/codecov@3.3.0
  gcloud: circleci/gcp-cli@2.1.0
  gh: circleci/github-cli@2.0
  helm: circleci/helm@1.2.0
  kubernetes: circleci/kubernetes@0.11.0
  queue: eddiewebb/queue@2.2.1
  win: circleci/windows@5.0.0

executors:
  python-38:
    docker:
      - image: python:3.8-slim-bookworm
  python-39:
    docker:
      - image: python:3.9-slim-bookworm

parameters:
  det-version:
    type: string
    default: 0.36.1-dev0
  docker-image:
    type: string
    default: determinedai/cimg-base:latest
  machine-image:
    type: string
    default: ubuntu-2004:2024.01.1
  gpu-machine-image:
    type: string
    default: linux-cuda-12:default
  # DEFAULT_PT_GPU_IMAGE: Pytorch training image reference used by the tests
  # Inject here as a parameter so that it is updated by bumpversion, and can
  # be referenced by --ee testing.
  default-pt-gpu-hpc-image:
    type: string
    default: determinedai/pytorch-ngc-dev:e960eae
  default-pt-tf-cpu-hpc-image:
    type: string
    default: determinedai/pytorch-tensorflow-cpu-dev:e960eae
  # Some python, go, and react dependencies are cached by circleci via `save_cache`/`restore_cache`.
  # If the dependencies stay the same, but the circleci code that would produce them is changed,
  # it may be necessary to invalidate the cache by incrementing this value.
  # For example, if you change an env variable affecting a build of a python package with the same version,
  # the old build may be cached, and you may need to invalidate it.
  cache-buster:
    type: string
    default: v1dev25
  gke-version:
    type: string
    default: "1.29"
  master-build-resource-class:
    type: string
    default: xlarge
  do_nightly_tests:
    type: boolean
    default: false
  ee:
    type: boolean
    default: false
  e2e-react:
    type: string
    default: ""

release-and-rc-filters: &release-and-rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+/
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+/

rc-filters: &rc-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /((\d)+(\.(\d)+)+)(-rc)(\d)+/

release-filters: &release-filters
  branches:
    ignore:
      - /.*/
  tags:
    only:
      - /(\d)+(\.(\d)+)+/

upstream-feature-branch: &upstream-feature-branch
  branches:
    ignore:
      - /pull\/.*/
      - /release-.*/
      - main

any-upstream: &any-upstream
  branches:
    ignore:
      - /pull\/.*/

any-fork: &any-fork
  branches:
    only:
      - /pull\/.*/

do-not-run-on-manual-trigger: &do-not-run-on-manual-trigger
  unless:
    equal: [ api, << pipeline.trigger_source >> ]


commands:
  fix-circle-working-directory:
    description: "Fix CIRCLE_WORKING_DIRECTORY"
    steps:
      - run: echo 'CIRCLE_WORKING_DIRECTORY="${CIRCLE_WORKING_DIRECTORY/#\~/$HOME}"' >> $BASH_ENV

  python-report:
    description: Report information about the Python environment.
    steps:
      - run: |
          python --version
          pip --version
          pip list
          sh -c "pip check || true"


  # circleci's checkout does not fetch submodules by default
  # https://circleci.com/docs/2.0/configuration-reference/#checkout
  checkout-with-sm:
    steps:
      - checkout
      - run: git submodule sync
      - run: git submodule update --init

  add-and-fetch-upstream:
    steps:
      - run: git remote add upstream https://github.com/determined-ai/determined
      - run: tools/scripts/retry.sh git fetch upstream

  collect-task-logs:
    parameters:
      master_address:
        type: string
        default: "http://localhost:8080"
      store_path:
        type: string
        default: "/tmp/artifacts/logs"
    description: Collect logs from the cluster tasks.
    steps:
      - run:
          name: "Ensure necessary Python packages are available."
          when: always
          command: |
            pkg_names="fire determined"
            for pkg_name in $pkg_names; do
              if ! python -c "import $pkg_name" 2>/dev/null; then
                pip install $pkg_name
              fi
            done
      - run:
          name: "Collect logs and calculate statistics"
          when: always
          command: |
            target_dir="<< parameters.store_path >>"
            mkdir -p $target_dir
            python .circleci/scripts/collect_logs.py --username determined --password "${INITIAL_USER_PASSWORD}" --mlde-host "<< parameters.master_address >>" save_all_logs $target_dir
            echo "collected logs at $target_dir"
            log_count=$(find "<< parameters.store_path >>" -type f | wc -l)
            total_size=$(du -sh "<< parameters.store_path >>" | cut -f1)
            echo "Number of log files collected: $log_count"
            echo "Total size of collected logs: $total_size"


  skip-if-only-dir:
    parameters:
      dir:
        type: string
    steps:
      - run:
          name: skip if << parameters.dir >>-only changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            DIFF_DIRS="$(git diff-tree --no-commit-id --name-only "$MERGE_BASE" HEAD)"
            if [ "$DIFF_DIRS" = "<< parameters.dir >>" ] ; then
              echo "<< parameters.dir >>-only change detected, halting job now"
              circleci-agent step halt
            else
              echo "detected non-<< parameters.dir >>-only changes: $DIFF_DIRS"
            fi

  skip-if-not-dir:
    parameters:
      dir:
        type: string
    steps:
      - run:
          name: continue if << parameters.dir >> changes
          command: |
            MERGE_BASE="$(git merge-base upstream/main HEAD)"
            if ! git diff --quiet "$MERGE_BASE" HEAD -- "<< parameters.dir >>"; then
              echo "<< parameters.dir >> change detected, running downstream steps"
            else
              echo "<< parameters.dir >> code not affected, halting job now"
              circleci-agent step halt
            fi

  skip-if-only-docs:
    steps:
      - skip-if-only-dir:
          dir: docs

  skip-if-not-docs:
    steps:
      - skip-if-not-dir:
          dir: docs

  skip-if-only-webui:
    steps:
      - skip-if-only-dir:
          dir: webui

  skip-if-only-github:
    steps:
      - skip-if-only-dir:
          dir: .github

  skip-on-dev-branch:
    steps:
      # Install the GitHub CLI.
      - gh/install
      - run:
          name: Check if extra GCP tests specfied
          command: |
            # Check if the 'ci-run-allgcp' label was set on the pull request,
            # which indicates that we want the GCP tests to run on demand.
            if gh pr view --json labels -q '.labels.[] | .name' | grep -w ci-run-allgcp
            then
              echo "The 'ci-run-allgcp' label was found in github pull request. Proceeding to run test-e2e-slurm-*-gcp tests."
            else
              echo "The 'ci-run-allgcp' label was not found in github pull request. Skipping job."
              circleci-agent step halt
            fi

  set-slack-user-id:
    steps:
      - run:
          name: Set Slack variables
          command: |
            if ! [ -x "$(command -v jq)" ]; then
              apt update && apt install -y jq
            fi

            AUTHOR_EMAIL="$(git show -s --format='%ae' $CIRCLE_SHA1)"
            echo "export AUTHOR_EMAIL=\"${AUTHOR_EMAIL}\"" >> $BASH_ENV
            LOOKUP_RESPONSE=$(curl -s "https://slack.com/api/users.lookupByEmail?token=${SLACK_API_TOKEN}&email=${AUTHOR_EMAIL}")
            SUCCESS=$(echo "$LOOKUP_RESPONSE" | jq ".ok")
            if [[ "$SUCCESS" == "true" ]]; then
              SLACK_USER_ID=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.id")
              SLACK_NAME=$(echo "$LOOKUP_RESPONSE" | jq -r ".user.name")
              echo "export SLACK_NAME=\"${SLACK_NAME}\"" >> $BASH_ENV
              echo "export SLACK_USER_ID=\"${SLACK_USER_ID}\"" >> $BASH_ENV
            else
              echo "Unable to find Slack user ID for  \"${AUTHOR_EMAIL}\"."
            fi

  license-gen:
    steps:
      - run: cd .. && git clone https://${GITHUB_TOKEN}@github.com/determined-ai/license-key-gen.git
      - run: cd ../license-key-gen && go run ./licensegen.go
      - run: mv ../license-key-gen/license.txt ./license.txt
      - run: mv ../license-key-gen/public.txt ./public.txt

  pull-task-images:
    parameters:
      tf2:
        type: boolean
        default: false
    steps:
      - when:
          condition: <<parameters.tf2>>
          steps:
            - run: docker pull determinedai/pytorch-ngc-dev:0736b6d

  login-docker:
    parameters:
      repository:
        type: string
        default: ""
      username:
        type: string
      password:
        type: string
    steps:
      - run: echo "<<parameters.password>>" | docker login <<parameters.repository>> -u "<<parameters.username>>" --password-stdin

  reinstall-go:
    steps:
      - run: sudo rm -rf /usr/local/go # Remove system go.
      - run: tools/scripts/retry.sh curl --retry-connrefused --retry 10 https://dl.google.com/go/go1.22.0.linux-amd64.tar.gz -o /tmp/go.linux-amd64.tar.gz
      - run: sudo tar -C /usr/local -xzf /tmp/go.linux-amd64.tar.gz
      - run: echo 'export PATH=$PATH:$HOME/go/bin' >> $BASH_ENV

  install-protoc:
    steps:
      - run: curl --retry-connrefused --retry 10 -o /tmp/protoc.zip -L https://github.com/protocolbuffers/protobuf/releases/download/v24.3/protoc-24.3-linux-x86_64.zip
      - run: unzip -o /tmp/protoc.zip -d $HOME/.local

  install-codecov:
    steps:
      - run: pip install codecov

  upload-junit-datadog:
    parameters:
      service:
        type: string
        default: ""
      env:
        type: string
        default: "ci-cpu"
      path:
        type: string
        default: "/tmp/test-results"
    steps:
      - run:
          name: Upload Tests to DataDog
          when: always
          command: |
              curl -L --fail "https://github.com/DataDog/datadog-ci/releases/latest/download/datadog-ci_linux-x64" --output "./datadog-ci" && chmod +x ./datadog-ci
              ./datadog-ci junit upload --service "determined-ai/<< parameters.service >>" --env "<< parameters.env >>" << parameters.path >>

  setup-go-intg-deps:
    steps:
      - install-protoc # Install newer version of protoc into $HOME/.local/bin, since default is proto2.
      - run: PATH=$HOME/.local/bin:$PATH make -C proto get-deps
      - run: PATH=$HOME/.local/bin:$PATH make -C proto build
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - install-devcluster
      - start-devcluster:
          target-stage: elastic
          devcluster-config: elastic-base.devcluster.yaml

  go-get-deps: # Build Go doesn't use this because it doesn't need it.
    steps:
      - install-protoc
      - restore_cache:
          keys:
            - det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}-{{ checksum  "master/get-deps.sh" }}-{{ checksum  "agent/get-deps.sh" }}-{{ checksum  "proto/get-deps.sh" }}
      - run: make -C proto get-deps
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - save_cache:
          key: det-go-deps-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}-{{ checksum  "master/get-deps.sh" }}-{{ checksum  "agent/get-deps.sh" }}-{{ checksum  "proto/get-deps.sh" }}
          paths:
            - "/home/circleci/go/"
            - "/home/circleci/.cache/go-build/"
  react-get-deps:
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Write cache suffix
          command: |
            echo '<<pipeline.parameters.cache-buster>>' > /tmp/react-get-deps.cachekey
            cat webui/react/package-lock.json >> /tmp/react-get-deps.cachekey
            echo $HOME >> /tmp/react-get-deps.cachekey
            cat /tmp/react-get-deps.cachekey
      - restore_cache:
          keys:
            - det-react-deps-{{ checksum "/tmp/react-get-deps.cachekey" }}
      - run:
          name: Get React dependencies
          command: |
            make -C webui/react node_modules/done.stamp
      - save_cache:
          key: det-react-deps-{{ checksum "/tmp/react-get-deps.cachekey" }}
          paths:
            - "webui/react/node_modules"

  install-wheel:
    parameters:
      package-name:
        type: string
      package-location:
        type: string
    steps:
      - run:
          name: Install <<parameters.package-name>>
          working_directory: <<parameters.package-location>>
          command: |
            make build
            pip install --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>
            pip install --no-deps --force-reinstall --find-links dist <<parameters.package-name>>==<< pipeline.parameters.det-version >>

  setup-python-venv:
    description: Set up and create Python venv.
    parameters:
      determined:
        type: boolean
        default: false
      install-python:
        type: boolean
        default: true
      extras-requires:
        type: string
        default: ""
      extra-requirements-file:
        type: string
        default: ""
      executor:
        type: string
      python-version:
        type: string
        default: "3.8.18"
    steps:
      - run:
          name: Write cache key
          command: |
            echo <<parameters.executor>> > /tmp/cachefile
            pip freeze --all >> /tmp/cachefile
            if [ "<<parameters.determined>>" = "true" ]; then
              cat harness/setup.py >> /tmp/cachefile
            fi
            echo <<parameters.extras-requires>> >> /tmp/cachefile
            if [ -n <<parameters.extra-requirements-file>> ]; then
              for i in <<parameters.extra-requirements-file>>; do
                cat $i >> /tmp/cachefile
              done
            fi
            echo '<<parameters.python-version>>' >> /tmp/cachefile
            echo '<<parameters.install-python>>' >> /tmp/cachefile
            date -u '+%y/%m/%d' >> /tmp/cachefile

      - restore_cache:
          keys:
            - det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
      - when:
          condition: <<parameters.install-python>>
          steps:
            - run:
                name: prepare conda PATH
                command: |
                  echo 'export PATH=/tmp/conda/bin:${PATH}' >> $BASH_ENV

            - run:
                name: prepare conda install_python.sh
                command: |
                  cat \<<'EOF' > /tmp/install_python.sh
                  #!/bin/bash
                  PYTHON_VERSION="${1}"

                  CONDA_DIR="/tmp/conda"
                  CONDA_INSTALLER="Miniconda3-py39_23.5.2-0-Linux-x86_64.sh"
                  CONDA_SHA256="9829d95f639bd0053b2ed06d1204e60644617bf37dd5cc57523732e0e8d64516"
                  CONDA_URL="https://repo.anaconda.com/miniconda"

                  ${CONDA_DIR}/bin/python --version
                  installed_python_version=$(${CONDA_DIR}/bin/python --version 2>&1 | awk '{print $2}')
                  if [ -x "${CONDA_DIR}/bin/python" ] && [ "$installed_python_version" = "$PYTHON_VERSION" ]; then
                      echo "Python already installed, version $PYTHON_VERSION"
                      exit 0
                  fi
                  echo "Python not installed or wrong version, installing conda and Python now"

                  mkdir -p /etc/determined/conda.d
                  mkdir -p "${CONDA_DIR}"

                  cd /tmp
                  curl --retry 3 -fsSL -O "${CONDA_URL}/${CONDA_INSTALLER}"
                  echo "${CONDA_SHA256}  ${CONDA_INSTALLER}" | sha256sum ${CONDA_INSTALLER}
                  bash "./${CONDA_INSTALLER}" -u -b -p "${CONDA_DIR}"
                  rm -f "./${CONDA_INSTALLER}"

                  ${CONDA_DIR}/bin/conda install python=${PYTHON_VERSION}
                  ${CONDA_DIR}/bin/conda update --prefix ${CONDA_DIR} --all -y
                  ${CONDA_DIR}/bin/conda clean --all -y
                  EOF

            - run:
                name: run conda install_python.sh
                command: |
                  sudo bash /tmp/install_python.sh <<parameters.python-version>>

      - run:
          name: Setup venv
          command: |
            python3 -m venv /tmp/venv
            echo 'export PATH=/tmp/venv/bin:$PATH' >> $BASH_ENV
            /tmp/venv/bin/python -m pip install --upgrade pip wheel setuptools

      - when:
          condition: <<parameters.determined>>
          steps:
            - run:
                name: Install pypa builder
                command: python3 -m pip install build
            - install-wheel:
                package-name: determined
                package-location: ./harness
      - run:
          name: Install <<parameters.extras-requires>>
          command: |
            if [ -n "<<parameters.extras-requires>>" ]; then
              tools/scripts/retry.sh pip install --progress-bar off <<parameters.extras-requires>>
            fi
      - run:
          name: Install <<parameters.extra-requirements-file>>
          command: |
            if [ -n "<<parameters.extra-requirements-file>>" ]; then
              for i in <<parameters.extra-requirements-file>>; do
                tools/scripts/retry.sh pip install -r $i
              done
            fi
      - save_cache:
          key: det-python-deps-<<pipeline.parameters.cache-buster>>-{{ checksum "/tmp/cachefile" }}
          paths:
            - "/tmp/venv"
            - "/tmp/conda"
      - python-report
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"

  setup-docker-env:
    description: Set up and create docker env.
    parameters:
      image:
        type: string
      image-name:
        type: string
        default: "test_container"
      bind-mount:
        type: string
        default: "$PWD"
      extras-requires:
        type: string
        default: ""
      gpus:
        type: boolean
        default: true
    steps:
      - when:
          condition: <<parameters.gpus>>
          steps:
            - run: docker run -v <<parameters.bind-mount>>:/mnt/project --gpus=all -it -d --name <<parameters.image-name>> --workdir /mnt/project <<parameters.image>> /bin/bash
      - when:
          condition:
            not: <<parameters.gpus>>
          steps:
            - run: docker run -v <<parameters.bind-mount>>:/mnt/project -it -d --name <<parameters.image-name>> --workdir /mnt/project <<parameters.image>> /bin/bash
      - run:
          name: Install <<parameters.extras-requires>>
          command: |
            if [ -n "<<parameters.extras-requires>>" ]; then
              docker exec <<parameters.image-name>> tools/scripts/retry.sh pip install --progress-bar off <<parameters.extras-requires>>
            fi

  setup-paths:
    steps:
      - run: mkdir -p $HOME/.local/bin
      - run: echo 'export PATH=$PATH:$HOME/.local/bin' >> $BASH_ENV
      - run: echo 'export PATH=$PATH:/usr/local/nvidia/bin' >> $BASH_ENV
      - run: echo 'export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64' >> $BASH_ENV

  run-e2e-tests:
    parameters:
      mark:
        type: string
        default: ""
      junit-path:
        type: string
        default: "/tmp/test-results/e2e/tests.xml"
      master-scheme:
        type: string
        default: "http"
      master-host:
        type: string
        default: "localhost"
      master-port:
        type: string
        default: "8080"
      master-cert:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      managed-devcluster:
        type: boolean
        default: false
      extra-pytest-flags:
        type: string
        default: ""
      wait-for-master:
        type: boolean
        default: true
      collect-det-job-logs:
        type: boolean
        default: true
    steps:
      - when:
          condition:
            and:
              - equal: [<<parameters.master-host>>,'localhost']
              - <<parameters.wait-for-master>>
              - not: <<parameters.managed-devcluster>>
          steps:
            - run:
                name: Install DataDog agent
                command: |
                  if [ "$AIS_DD_ENABLE_MONITORING" == "true" ]; then
                    host_tags="test.mark:<<parameters.mark>>,\
                    ci.pipeline_id:${CIRCLE_PIPELINE_ID},\
                    ci.workflow_id:${CIRCLE_WORKFLOW_ID},\
                    ci.job_num:${CIRCLE_BUILD_NUM},\
                    ci.username:${CIRCLE_USERNAME},\
                    git.tag:${CIRCLE_TAG},\
                    git.commit:${CIRCLE_SHA1},\
                    git.repo:${CIRCLE_PROJECT_REPONAME},\
                    ci.totalNodes:${CIRCLE_NODE_TOTAL},\
                    ci.nodeIdx:${CIRCLE_NODE_INDEX},\
                    git.pr_num:${CIRCLE_PR_NUMBER}"

                    sudo mkdir -p /tmp/artifacts/logs
                    sudo chmod -R a+rw /tmp/artifacts/logs

                    DD_ENV="ci-${CIRCLE_JOB}" \
                    DD_HOST_TAGS="$host_tags" \
                    DD_SERVICE="determined-pytest-<<parameters.mark>>" \
                    bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh)"

                    # config files for the agent have an expected file structure
                    sudo mkdir -p /etc/datadog-agent/conf.d/determined-master.d/
                    sudo chmod a+rw /etc/datadog-agent/datadog.yaml
                    sudo chmod -R a+rw /etc/datadog-agent/conf.d/determined-master.d/
                    sudo cat .circleci/datadog/ci-local-config.yaml >> /etc/datadog-agent/datadog.yaml
                    sudo sed -e "s/<SERVICE_NAME>/determined-pytest-<<parameters.mark>>/g" .circleci/datadog/e2e-log-settings.yaml > /etc/datadog-agent/conf.d/determined-master.d/conf.yaml
                    # restart agent with config
                    sudo usermod -a -G docker dd-agent
                    sudo systemctl stop datadog-agent
                    sudo systemctl start datadog-agent
                    sleep 5
                    sudo datadog-agent status
                  fi
      # Wait for master before splitting tests, since so many splits depend on
      # asking master for its configuration in order to apply skipifs.
      - when:
          condition:
            and:
              - not: <<parameters.managed-devcluster>>
              - <<parameters.wait-for-master>>
          steps:
            - wait-for-master:
                scheme: <<parameters.master-scheme>>
                host: <<parameters.master-host>>
                port: <<parameters.master-port>>

      - run:
          name: Run e2e tests
          working_directory: ./e2e_tests
          no_output_timeout: 30m
          command: |

            # Remove prior test results
            rm -f "<<parameters.junit-path>>"

            pytest --setup-plan -m '<<parameters.mark>>' | egrep '\.py s*$' | sed 's/\.py.*$/.py/' > /tmp/all-relevant-files
            if [ ! -s /tmp/all-relevant-files ]; then
              echo 'No test files found!'
              exit 1
            fi

            if nc -zv <<parameters.master-host>> <<parameters.master-port>> 2>/dev/null; then
              echo "Determined user list from master '<<parameters.master-scheme>>://<<parameters.master-host>>:<<parameters.master-port>>'"
              DET_MASTER_CERT_FILE=noverify DET_USER=admin DET_PASS=${INITIAL_USER_PASSWORD} \
              det --master "<<parameters.master-scheme>>://<<parameters.master-host>>:<<parameters.master-port>>" user list
            else
              echo "No Determined master listening on '<<parameters.master-scheme>>://<<parameters.master-host>>:<<parameters.master-port>>'"
            fi

            tags="test.mark:<<parameters.mark>>,\
            ci.pipeline_id:${CIRCLE_PIPELINE_ID},\
            ci.workflow_id:${CIRCLE_WORKFLOW_ID},\
            ci.job_num:${CIRCLE_BUILD_NUM},\
            ci.username:${CIRCLE_USERNAME},\
            git.tag:${CIRCLE_TAG},\
            git.commit:${CIRCLE_SHA1},\
            ci.totalNodes:${CIRCLE_NODE_TOTAL},\
            ci.nodeIdx:${CIRCLE_NODE_INDEX},\
            git.pr_num:${CIRCLE_PR_NUMBER}"

            CMD="DD_CIVISIBILITY_AGENTLESS_ENABLED=true \
            DD_TAGS='${tags}' \
            DD_ENV='ci-<<parameters.mark>>' \
            DD_SERVICE='determined-pytest-<<parameters.mark>>' \
            DET_MASTER_CERT_FILE=<<parameters.master-cert>> \
            DET_MASTER_CERT_NAME=<<parameters.master-cert-name>> \
            IS_CIRCLECI_JOB=1 XDG_CONFIG_HOME=/tmp \
            xargs pytest --capture=tee-sys -vv \
            -m '<<parameters.mark>>' \
            --durations=0 \
            --ddtrace \
            --master-scheme='<<parameters.master-scheme>>' \
            --master-host='<<parameters.master-host>>' \
            --master-port='<<parameters.master-port>>' \
            -o junit_family=xunit1 \
            --junit-xml='<<parameters.junit-path>>' \
            <<parameters.extra-pytest-flags>>"

            echo "$CMD"
            cat /tmp/all-relevant-files | circleci tests run --command="$CMD" \
            --verbose --split-by=timings
            pytest_status=$?
            echo Pytest exited with $pytest_status
            exit $pytest_status

      - upload-test-job:
          only_on_branch: main
          test_results_path: <<parameters.junit-path>>
      - when:
          condition: <<parameters.collect-det-job-logs>>
          steps:
            - collect-task-logs:
                master_address: "<<parameters.master-scheme>>://<<parameters.master-host>>:<<parameters.master-port>>"
      - store_artifacts:
          path: /tmp/artifacts/logs
      - when:
          condition:
            and:
              - equal: [<<parameters.master-host>>,'localhost']
              - <<parameters.wait-for-master>>
              - not: <<parameters.managed-devcluster>>
          steps:
            - run: # We don't know how long Circle leaves these machines running in the background. Take down the agent for safety.
                name: Stop DataDog agent
                when: always
                command: |
                  if [ "$AIS_DD_ENABLE_MONITORING" == "true" ]; then
                    sudo systemctl stop datadog-agent || true
                  fi


  run-det-deploy-tests:
    parameters:
      mark:
        type: string
        default: ""
      det-version:
        type: string
        default: ""
    steps:
      - run:
          name: Run det-deploy tests
          working_directory: ./e2e_tests
          command: |
            DET_SECURITY_INITIAL_USER_PASSWORD=$INITIAL_USER_PASSWORD \
            pytest -vv -s \
            -m <<parameters.mark>> \
            --junitxml=/tmp/test-results/det-deploy-tests.xml \
            --no-compare-stats \
            --det-version="<<parameters.det-version>>"
      - store_test_results:
          path: /tmp/test-results/

  deploy-aws-cluster:
    parameters:
      cluster-id:
        type: string
      extra-tags:
        type: string
        default: ""
      det-version:
        type: string
      keypair:
        type: string
        default: "integrations-test"
      enable-cors:
        type: boolean
        default: false
      master-tls-cert:
        type: string
        default: ""
      master-tls-key:
        type: string
        default: ""
      master-cert-name:
        type: string
        default: ""
      aux-agent-instance-type:
        type: string
        default: "m6i.large"
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      max-dynamic-agents:
        type: integer
        default: 1
      retain-log-group:
        type: boolean
        default: false
      log-group-prefix:
        type: string
        default: ""
      reattach-enabled:
        type: boolean
        default: false
      deployment-type:
        type: string
        default: simple-rds
      ee:
        type: boolean
        default: false
    steps:
      - run:
          name: Initialize extra arguments
          command: touch /tmp/det-deploy-extra-args
      - when:
          condition:
            equal: [true, << parameters.enable-cors >>]
          steps:
            - run:
                name: Enable CORS
                command: "echo --enable-cors >> /tmp/det-deploy-extra-args"
      - when:
          condition: << parameters.extra-tags >>
          steps:
            - run:
                name: Add extra tags
                command: |
                  echo "<< parameters.extra-tags >>" | \
                  sed 's/ /-/g' | \
                  sed -r 's/([^,=]*=[^,=]*),?/--add-tag \1 /g' >> \
                    /tmp/det-deploy-extra-args

      - run:
          name: Configure TLS arguments
          command: |
            if [ -n "<<parameters.master-tls-cert>>" ]; then echo "--master-tls-cert <<parameters.master-tls-cert>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-tls-key>>" ]; then echo "--master-tls-key <<parameters.master-tls-key>>" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.master-cert-name>>" ]; then echo "--master-cert-name <<parameters.master-cert-name>>" >> /tmp/det-deploy-extra-args; fi
      - run:
          name: Configure log group arguments
          command: |
            if <<parameters.retain-log-group>>; then echo "--retain-log-group" >> /tmp/det-deploy-extra-args; fi
            if [ -n "<<parameters.log-group-prefix>>" ]; then echo "--log-group-prefix <<parameters.log-group-prefix>>" >> /tmp/det-deploy-extra-args; fi
      - when:
          condition:
            equal: [true, <<parameters.reattach-enabled>>]
          steps:
            - run:
                name: set reattach-enabled
                command: echo --agent-reattach-enabled true --agent-reconnect-attempts 24 --no-update-terminate-agents --notebook-timeout 1800 >> /tmp/det-deploy-extra-args
      - run:
          name: Deploy AWS cluster
          no_output_timeout: 20m
          command: |
            echo "-----BEGIN ARGS-----"
            cat /tmp/det-deploy-extra-args
            echo "-----END ARGS-----"
            if <<parameters.ee>>; then
              echo "running enterprise edition"
              MAX_RETRIES=6 DET_DEBUG=1 tools/scripts/retry.sh det deploy aws up \
                $(< /tmp/det-deploy-extra-args) \
                --add-tag owner=determined_ci \
                --add-tag gh_team=machine-users \
                --cluster-id <<parameters.cluster-id>> \
                --det-version <<parameters.det-version>> \
                --aux-agent-instance-type <<parameters.aux-agent-instance-type>> \
                --compute-agent-instance-type <<parameters.compute-agent-instance-type>> \
                --max-dynamic-agents <<parameters.max-dynamic-agents>> \
                --keypair <<parameters.keypair>> \
                --deployment-type <<parameters.deployment-type>> \
                --enterprise-edition \
                --docker-user $DOCKER_USER \
                --docker-pass $DOCKER_PASS \
                --retain-log-group \
                --initial-user-password ${INITIAL_USER_PASSWORD} \
                --yes
            else
              echo "running OSS edition"
              MAX_RETRIES=6 DET_DEBUG=1 tools/scripts/retry.sh det deploy aws up \
                $(< /tmp/det-deploy-extra-args) \
                --add-tag owner=determined_ci \
                --add-tag gh_team=machine-users \
                --cluster-id <<parameters.cluster-id>> \
                --det-version <<parameters.det-version>> \
                --aux-agent-instance-type <<parameters.aux-agent-instance-type>> \
                --compute-agent-instance-type <<parameters.compute-agent-instance-type>> \
                --max-dynamic-agents <<parameters.max-dynamic-agents>> \
                --keypair <<parameters.keypair>> \
                --deployment-type <<parameters.deployment-type>> \
                --docker-user $DOCKER_USER \
                --docker-pass $DOCKER_PASS \
                --retain-log-group \
                --initial-user-password ${INITIAL_USER_PASSWORD} \
                --yes
            fi

  terminate-aws-cluster:
    parameters:
      cluster-id:
        type: string
    steps:
      - run:
          name: Terminate AWS Cluster
          when: always
          no_output_timeout: 20m
          command: |
            MAX_RETRIES=6 DET_DEBUG=1 tools/scripts/retry.sh det deploy aws down \
              --cluster-id <<parameters.cluster-id>> --yes

  setup-aws-cluster:
    parameters:
      cluster-id:
        type: string
      extra-tags:
        type: string
        default: ""
      det-version:
        type: string
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m6i.large"
      max-dynamic-agents:
        type: integer
        default: 1
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
      master-cert-name:
        type: string
      deployment-type:
        type: string
        default: simple-rds
      ee:
        type: boolean
        default: false
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - deploy-aws-cluster:
          cluster-id: ${CLUSTER_ID}
          extra-tags: <<parameters.extra-tags>>
          det-version: <<parameters.det-version>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>
          master-cert-name: <<parameters.master-cert-name>>
          log-group-prefix: determined-ci
          retain-log-group: true
          deployment-type: <<parameters.deployment-type>>
          ee: <<parameters.ee>>
      - set-master-address-aws:
          cluster-id: ${CLUSTER_ID}
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>

  terminate-gke-cluster:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
    steps:
      # Use a run instead of `gke/helm` orbs because circle CI orbs do not support `when`.
      - run:
          name: Delete Helm release
          when: always
          command: helm delete ci
      - run:
          name: Terminate GKE Cluster
          when: always
          command: |
            gcloud container clusters delete <<parameters.cluster-id>> --quiet --region=<<parameters.region>>

  setup-gke-cluster:
    parameters:
      cluster-id:
        type: string
      det-version:
        type: string
      gke-version:
        type: string
      machine-type:
        type: string
      num-machines:
        type: integer
      labels:
        type: string
      gpu-type:
        type: string
      gpus-per-machine:
        type: integer
      slot-type:
        type: string
      max-slots-per-pod:
        type: integer
      slot-resource-requests-cpu:
        type: integer
      region:
        type: string
      node-locations:
        type: string
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
      environment-image:
        type: string
      storage-bucket:
        type: string
        default: "det-ci-373"
      accel-node-taints:
        type: string
        default: ""
      cluster-ipv4-cidr:
        type: string
        description: The IP address range for the pods in this cluster.
        # We are specifying this since the cluster doesn't need to be very large and GKE was intermittently
        # defaulting to an excessively large range. "/19" is the minimum range it will let us allocate and
        # allows the cluster to create at most 16 nodes.
        # Kubernetes services default to 4096 IPs, or a "/20", and the remainder gets split up into a "/24" per node.
        # So with a "/19", there are 8192 total addresses. After 4096 get used for services there are 4096 left.
        # That's 2^12 / 2^8 = 16 nodes for our tests. Thanks @Danny Sauer!
        # If we find we need more nodes in our tests in the future, we can bump this up.
        default: "/19"
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - set-cluster-labels:
          labels: owner=determined_ci,gh_team=machine-users,<<parameters.labels>>
      - gcloud/install:
          version: "412.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: |
            tries=5
            until gcloud components install gke-gcloud-auth-plugin --quiet; do
              if [[ $((--tries)) -eq 0 ]]; then
                exit 1
              fi
              sleep 15
            done

            echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $BASH_ENV
          name: Install GKE auth plugin
      - run:
          command: |
            echo "Console URL for cluster: https://console.cloud.google.com/kubernetes/clusters/details/<<parameters.region>>/${CLUSTER_ID}?project=determined-ai"
            gcloud container clusters create ${CLUSTER_ID} --machine-type=n1-standard-8 --cluster-version=<<parameters.gke-version>> --region=<<parameters.region>> --node-locations=<<parameters.node-locations>> --scopes storage-rw,cloud-platform --num-nodes 1 --labels environment=ci,${LABELS} --no-enable-ip-alias --subnetwork default --cluster-ipv4-cidr=<<parameters.cluster-ipv4-cidr>>
          name: Create GKE cluster
          no_output_timeout: 30m
      - run:
          command: gcloud container clusters get-credentials ${CLUSTER_ID} --project ${GOOGLE_PROJECT_ID} --region <<parameters.region>>
          name: Get Kubeconfig
      - run:
          command: |
            echo 'export HELM_VALUES="initialUserPassword=${INITIAL_USER_PASSWORD},detVersion=<<parameters.det-version>>,maxSlotsPerPod=<<parameters.max-slots-per-pod>>,checkpointStorage.type=gcs,checkpointStorage.bucket=<<parameters.storage-bucket>>,taskContainerDefaults.startupHook=echo hello from master tcd startup hook"' >> "$BASH_ENV"
          name: Prepare helm overrides
      - run:
          command: |
            echo 'export HELM_VALUES="${HELM_VALUES},enterpriseEdition=true,imagePullSecretName=docker-cred"' >> "$BASH_ENV"
            kubectl create secret docker-registry docker-cred --docker-username=$DOCKER_USER --docker-password=$DOCKER_PASS
          name: Set EE helm overrides and create docker secret
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
                name: Install NVIDIA drivers
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},slotType=<<parameters.slot-type>>,slotResourceRequests.cpu=<<parameters.slot-resource-requests-cpu>>,resourcePools[0].agent_reattach_enabled=true,resourcePools[0].pool_name=default,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].effect=NoSchedule,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].effect=NoSchedule"' >> "$BASH_ENV"
                name: CPU setup helm overrides
      - when:
          condition: <<parameters.environment-image>>
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},taskContainerDefaults.cpuImage=<<parameters.environment-image>>,taskContainerDefaults.gpuImage=<<parameters.environment-image>>"' >> "$BASH_ENV"
                name: env image helm overrides
      - helm/install-helm-chart:
          chart: helm/charts/determined
          helm-version: v3.2.4
          namespace: "default"
          wait: true
          release-name: "ci"
          values-to-override: ${HELM_VALUES}
      - set-master-address-gke:
          release-name: "ci"
          namespace: "default"
      - when:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                no_output_timeout: 30m
                command: |
                  (sleep 29.5m && pkill python3) &
                  gcloud container node-pools create accel \
                    --cluster ${CLUSTER_ID} \
                    --region '<<parameters.region>>' \
                    --num-nodes '<<parameters.num-machines>>' \
                    --accelerator 'type=<<parameters.gpu-type>>,count=<<parameters.gpus-per-machine>>' \
                    --machine-type='<<parameters.machine-type>>' \
                    --scopes cloud-platform \
                    --node-taints='<<parameters.accel-node-taints>>' \
                    || (
                      curl "$SLACK_WEBHOOK" \
                        -H 'Content-Type: application/json' \
                        -d "{\"text\":\"GKE node pool creation failed on branch \`$CIRCLE_BRANCH\`! $CIRCLE_BUILD_URL\"}"
                      echo "================"
                      echo "GKE node pool creation failed, but we are marking the job as successful because this is not directly our problem. Ask the infrastructure engineering team about increasing the quota if you want."
                      echo "================"
                      circleci-agent step halt
                    )
                name: Create GPU node pool
      - unless:
          condition: <<parameters.gpus-per-machine>>
          steps:
            - run:
                command: gcloud container node-pools create accel --cluster ${CLUSTER_ID} --region <<parameters.region>> --num-nodes <<parameters.num-machines>> --machine-type=<<parameters.machine-type>> --scopes cloud-platform --node-taints=<<parameters.accel-node-taints>>
                name: Create CPU node pool

  setup-shared-cluster:
    parameters:
      cluster-id:
        type: string
        default: ${GKE_CLUSTER_NAME}
      labels:
        type: string
        default: ""
      det-version:
        type: string
      region:
        type: string
        default: ${GKE_REGION}
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
      gpus-per-machine:
        type: integer
        default: 20
      slot-type:
        type: string
        default: "cpu"
      slot-resource-requests-cpu:
        type: integer
        default: 1
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
      master-cert-name:
        type: string
    steps:
      - set-cluster-id:
          cluster-id: <<parameters.cluster-id>>
      - set-cluster-labels:
          labels: <<parameters.labels>>
      - gcloud/install:
          version: "412.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: |
            echo 'export HELM_VALUES="detVersion=<<parameters.det-version>>,maxSlotsPerPod=<<parameters.gpus-per-machine>>,checkpointStorage.type=gcs,checkpointStorage.bucket=${GENERATED_NAMESPACE}-bucket,createNonNamespacedObjects=false"' >> "$BASH_ENV"
          name: Prepare helm overrides
      - when:
          condition:
            and:
            - <<parameters.gpus-per-machine>>
            - equal: [ "gpu", <<parameters.slot-type>> ]
          steps:
            - run:
                command: kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
                name: Install NVIDIA drivers
      - unless:
          condition:
            equal: [ "gpu", <<parameters.slot-type>> ]
          steps:
            - run:
                command: |
                  echo 'export HELM_VALUES="${HELM_VALUES},slotType=<<parameters.slot-type>>,slotResourceRequests.cpu=<<parameters.slot-resource-requests-cpu>>,resourcePools[0].agent_reattach_enabled=true,resourcePools[0].pool_name=default,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.gpuPodSpec.spec.tolerations[0].effect=NoSchedule,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].key=accel,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].operator=Equal,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].value=truth,taskContainerDefaults.cpuPodSpec.spec.tolerations[0].effect=NoSchedule"' >> "$BASH_ENV"
                name: CPU setup helm overrides
      - run:
          command: |
            echo 'export HELM_VALUES="${HELM_VALUES},security.tls.cert=\"${MASTER_TLS_CERT}\",security.tls.key=\"${MASTER_TLS_KEY}\",masterPort=8443,tlsSecret=<<parameters.master-cert-name>>"' >> "$BASH_ENV"
          name: Setup TLS Helm Values
      - run:
          command: |
            ip_addresses=($(echo "${CI_RANGES}" | tr -d '" ' | tr ',' ' '))
            formattedRange="export HELM_VALUES=${HELM_VALUES},'loadBalancerSourceRanges={"
            for i in ${ip_addresses[@]}; do
              formattedRange+="${i},"
            done
            formattedRange=${formattedRange::-1}
            formattedRange+="}'"
            echo ${formattedRange} >> "$BASH_ENV"
          name: Setup Firewall Config
      - run:
          command: |
            echo 'export HELM_VALUES="${HELM_VALUES},initialUserPassword=${INITIAL_USER_PASSWORD}"' >> "$BASH_ENV"
      - run:
          command: |
            tries=5
            until gcloud components install gke-gcloud-auth-plugin --quiet; do
              if [[ $((--tries)) -eq 0 ]]; then
                exit 1
              fi
              sleep 15
            done

            echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $BASH_ENV
          name: Install GKE auth plugin
      - run:
          command: gcloud container clusters get-credentials <<parameters.cluster-id>> --project ${<<parameters.google-project-id>>} --region <<parameters.region>>
          name: Get Kubeconfig
      - run:
          command: kubectl create namespace ${GENERATED_NAMESPACE}
          name: Create namespace
      - run:
          command: kubectl config set-context --current --namespace=${GENERATED_NAMESPACE}
          name: Set context to the created namespace
      - run:
          command: kubectl create secret tls <<parameters.master-cert-name>> --cert <<parameters.master-tls-cert>> --key <<parameters.master-tls-key>> --namespace ${GENERATED_NAMESPACE} # Create tls secret in namespace w/secret name
      - run:
          name: Pull docker images onto nodes
          command: |
            kubectl apply -f .circleci/scripts/pull_image_daemonset.yaml
      - run:
          command: gsutil mb -p ${<<parameters.google-project-id>>} gs://${GENERATED_NAMESPACE}-bucket
      - helm/install-helm-client:
          version: v3.12.3
      - run:
          command: |
            helm install ${GENERATED_NAMESPACE} helm/charts/determined  --set "${HELM_VALUES}" --namespace="${GENERATED_NAMESPACE}" --wait --timeout 20m0s
          name: Helm Install
      - run:
          command: |
            helm get values ${GENERATED_NAMESPACE} --namespace="${GENERATED_NAMESPACE}"
          name: Get Helm Values
      - set-master-address-gke:
          release-name: ${GENERATED_NAMESPACE}
          namespace: ${GENERATED_NAMESPACE}
          master-tls-cert: <<parameters.master-tls-cert>>
          master-tls-key: <<parameters.master-tls-key>>

  generate-tls-cert:
    steps:
      - run: |
          .circleci/scripts/generate_cert.sh /tmp/master
          echo 'export MASTER_TLS_CERT=/tmp/master.crt MASTER_TLS_KEY=/tmp/master.key MASTER_CERT_NAME=determined-master-ci' >> $BASH_ENV

  set-master-address-aws:
    parameters:
      cluster-id:
        type: string
      master-tls-cert:
        type: string
      master-tls-key:
        type: string
    steps:
      - run: |
          MASTER_HOST=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> DeterminedAddress)
          echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV
          if [ -n "<<parameters.master-tls-cert>>" ] && [ -n "<<parameters.master-tls-key>>" ]; then
            echo "export MASTER_PORT=8443" >> $BASH_ENV
            echo "export MASTER_SCHEME=https" >> $BASH_ENV
          fi

  set-master-address-gke:
    parameters:
      release-name:
        type: string
      namespace:
        type: string
      master-tls-cert:
        type: string
        default: ""
      master-tls-key:
        type: string
        default: ""
    steps:
      - run:
          name: Set Master Address
          command: |
            MASTER_HOST=$(kubectl get -n <<parameters.namespace>>  service determined-master-service-<<parameters.release-name>> \
            --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
            echo "export MASTER_HOST=\"${MASTER_HOST}\"" >> $BASH_ENV
            echo "${MASTER_HOST}"
            if [ -n "<<parameters.master-tls-cert>>" ] && [ -n "<<parameters.master-tls-key>>" ]; then
              echo "export MASTER_PORT=8443" >> $BASH_ENV
              echo "export MASTER_SCHEME=https" >> $BASH_ENV
            fi

  set-google-application-credentials:
    steps:
      - run:
          name: Set Google Application Credentials
          command: |
            GOOGLE_APPLICATION_CREDENTIALS=${HOME}/gcloud-service-key.json
            echo "export GOOGLE_APPLICATION_CREDENTIALS=\"${GOOGLE_APPLICATION_CREDENTIALS}\"" >> $BASH_ENV

  set-cluster-id:
    parameters:
      cluster-id:
        type: string
    steps:
      - run: echo "export CLUSTER_ID=\"<<parameters.cluster-id>>\"" >> $BASH_ENV

  set-cluster-labels:
    parameters:
      labels:
        type: string
    steps:
      - run: echo "export LABELS=\"$(echo '<<parameters.labels>>' | sed 's/ /-/g')\"" >> $BASH_ENV

  wait-for-master:
    parameters:
      scheme:
        type: string
        default: "http"
      host:
        type: string
      port:
        type: string
        default: "8080"
    steps:
      - run: python .circleci/scripts/wait_for_master.py <<parameters.scheme>>://<<parameters.host>>:<<parameters.port>>

  upload-test-job:
    parameters:
      only_on_branch:
        type: string
        default: ""
      test_results_path:
        type: string
        default: ""
    steps:
      - when:
          condition:
            equal: [<<parameters.only_on_branch>>, << pipeline.git.branch >>]
          steps:
            - run:
                name: Test run success -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'success' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_success
            - run:
                name: Test run failed -> uploading results to Determined CI
                command: |
                  python .circleci/scripts/upload_test_results.py \
                  'failed' \
                  ${CIRCLE_NODE_INDEX} \
                  <<parameters.test_results_path>> \
                  ${CIRCLE_BUILD_NUM} \
                  ${DET_CI_ACCESS_KEY} \
                  ${DET_CI_URL}
                when: on_fail

  locate-cloudwatch-logs:
    parameters:
      cluster-id:
        type: string
      region:
        type: string
        default: us-west-2
    steps:
      - run:
          name: Locate CloudWatch logs
          when: always
          command: |
            LOG_GROUP=$(python .circleci/scripts/get_output_from_stack.py <<parameters.cluster-id>> LogGroup)
            echo "Cluster logs can be found in CloudWatch under the log group $LOG_GROUP"
            echo "or the URL below (if the log group is in your default region):"
            ENC_LOG_GROUP=$(echo "$LOG_GROUP" | sed 's|/|$252F|g')
            echo "https://console.aws.amazon.com/cloudwatch/home#logsV2:log-groups/log-group/$ENC_LOG_GROUP"

  pre-package-and-push-system:
    parameters:
      check:
        type: boolean
        default: true
    steps:
      - go-get-deps
      - run: make -C proto build
      - when:
          condition: <<parameters.check>>
          steps:
            - run: make -C master check
            - run: make -C agent check

  make-package:
    steps:
      - attach_workspace:
          at: .
      - run:
          no_output_timeout: 30m
          command: make package

  make-package-ee:
    steps:
      - attach_workspace:
          at: .
      - run:
          no_output_timeout: 30m
          command: make package-ee

  install-devcluster:
    steps:
      - run: pip install git+https://github.com/determined-ai/devcluster.git@v1.1.0#egg=devcluster
      - run:
          command: |
            if ! [ -x "$(command -v socat)" ]; then
              apt update && apt install -y socat
            fi

  start-devcluster:
    parameters:
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
    steps:
      - run:
          command: devcluster --oneshot -c .circleci/devcluster/<<parameters.devcluster-config>> --target-stage <<parameters.target-stage>> || circleci-agent step halt
          background: true

jobs:
  build-helm:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - attach_workspace:
          at: .
      - run: make -C helm build
      - persist_to_workspace:
          root: .
          paths:
            - helm/build
      - store_artifacts:
          path: helm/build

  build-docs:
    parameters:
      ee:
        type: boolean
        default: false
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - helm/install-helm-client
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C examples build
      - run: make -C helm build
      - attach_workspace:
          at: .
      - run: |
          if <<parameters.ee>>; then
            make -C docs attributions.rst rebrand build
          else
            make -C docs build
          fi
      - persist_to_workspace:
          root: .
          paths:
            - examples/build
            - harness/dist
            - docs/build
            - docs/site
      - run: tar czf docs.tgz docs/site/html
      - store_artifacts:
          path: docs.tgz

  upload-docs-search-index:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C docs upload-search-index

  check-docs-links:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run:
          command: MAX_RETRIES=2 tools/scripts/retry.sh make -C docs check-links

  publish-docs:
    parameters:
      ee:
        type: boolean
        default: false
      dry-run:
        type: boolean
        default: false
    docker:
      - image: hashicorp/terraform:latest
    steps:
      - checkout
      - attach_workspace:
          at: .
      - run: apk add make curl py3-pip
      - run: pip install --break-system-packages awscli  # used in the terraform apply
      - run: |
          if <<parameters.dry-run>>; then
            echo 'Doing a dry run!'
            if <<parameters.ee>>; then
              export DET_VARIANT=EE
            else
              export DET_VARIANT=OSS
            fi
            make -C docs pre-publish publish-check
            make -C docs/deploy plan
          else
            if <<parameters.ee>>; then
              make -C docs publish-ee
            else
              make -C docs publish-oss
            fi
          fi

  # TODO(danh): eventually replace 'publish-docs' with this new workflow after
  # all dependent scripts are updated.
  # Related work: https://hpe-aiatscale.atlassian.net/browse/INFENG-183
  publish-docs-new:
    parameters:
      preview:
        type: boolean
        default: true
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-not-docs
      - attach_workspace:
          at: .
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - when:
          condition: <<parameters.preview>>
          steps:
            - run: make -C docs preview

  package-and-push-system-local:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: large
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - reinstall-go
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: previous
      - pre-package-and-push-system:
          check: false
      - make-package
      - run: mkdir -p build/
      - run: docker tag determinedai/determined-master:${CIRCLE_SHA1}-amd64 determinedai/determined-master:${CIRCLE_SHA1}
      - run: docker save -o build/master.image determinedai/determined-master:${CIRCLE_SHA1}
      - run: docker save -o build/master-oss.image determinedai/determined-master:${CIRCLE_SHA1}
      - run: docker tag determinedai/determined-agent:${CIRCLE_SHA1}-amd64 determinedai/determined-agent:${CIRCLE_SHA1}
      - run: docker save -o build/agent.image determinedai/determined-agent:${CIRCLE_SHA1}
      - run: docker save -o build/agent-oss.image determinedai/determined-agent:${CIRCLE_SHA1}
      - persist_to_workspace:
          root: .
          paths:
            - "master/dist/*linux_amd64.deb"
            - "master/dist/*linux_amd64.rpm"
            - "agent/dist/*linux_amd64.deb"
            - "agent/dist/*linux_amd64.rpm"
            - "build/*.image"

  package-and-push-system-local-ee:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - reinstall-go
      - license-gen
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: previous
      - pre-package-and-push-system:
          check: false
      - make-package-ee
      - run: mkdir -p build/
      - run: docker tag determinedai/hpe-mlde-master:${CIRCLE_SHA1}-amd64 determinedai/hpe-mlde-master:${CIRCLE_SHA1}
      - run: docker save -o build/master.image determinedai/hpe-mlde-master:${CIRCLE_SHA1}
      - run: docker save -o build/master-ee.image determinedai/hpe-mlde-master:${CIRCLE_SHA1}
      - run: docker tag determinedai/hpe-mlde-agent:${CIRCLE_SHA1}-amd64 determinedai/hpe-mlde-agent:${CIRCLE_SHA1}
      - run: docker save -o build/agent.image determinedai/hpe-mlde-agent:${CIRCLE_SHA1}
      - run: docker save -o build/agent-ee.image determinedai/hpe-mlde-agent:${CIRCLE_SHA1}
      - persist_to_workspace:
          root: .
          paths:
            - "master/dist/*linux_amd64.deb"
            - "master/dist/*linux_amd64.rpm"
            - "agent/dist/*linux_amd64.deb"
            - "agent/dist/*linux_amd64.rpm"
            - "agent/dist/determined-agent_linux_amd64_v1/determined-agent"
            - "build/*.image"

  package-and-push-system-dev:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system:
          check: false
      - make-package
      - run: tools/scripts/retry.sh make -C master publish-dev
      - run: tools/scripts/retry.sh make -C agent publish-dev
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  package-and-push-system-dev-ee:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - license-gen
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system:
          check: false
      - make-package-ee
      - run: tools/scripts/retry.sh make -C master publish-dev-ee
      - run: tools/scripts/retry.sh make -C agent publish-dev-ee
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs

  package-and-push-system-rc:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - login-docker:
          repository: nvcr.io
          username: ${NGC_API_USERNAME}
          password: ${NGC_API_KEY}
      - pre-package-and-push-system
      - make-package
      - run: make -C master publish
      - run: make -C agent publish
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  package-and-push-system-rc-ee:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - license-gen
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system
      - make-package-ee
      - run: make -C master publish-ee
      - run: make -C agent publish-ee
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs

  package-and-push-system-release:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - login-docker:
          repository: nvcr.io
          username: ${NGC_API_USERNAME}
          password: ${NGC_API_KEY}
      - pre-package-and-push-system
      - run:
          no_output_timeout: 30m
          command: make -C master release
      - run:
          no_output_timeout: 30m
          command: make -C agent release
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs
      - store_artifacts:
          path: /tmp/pkgs

  package-and-push-system-release-ee:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: <<pipeline.parameters.master-build-resource-class>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - license-gen
      - setup_remote_docker:
          version: previous
      - login-docker:
          username: ${DOCKER_USER}
          password: ${DOCKER_PASS}
      - pre-package-and-push-system
      - run:
          no_output_timeout: 30m
          command: make -C master release-ee
      - run:
          no_output_timeout: 30m
          command: make -C agent release-ee
      - run: mkdir /tmp/pkgs && cp -v */dist/*.{rpm,deb,tar.gz} /tmp/pkgs

  publish-helm-gh:
    parameters:
      ee:
        type: boolean
        default: false
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - attach_workspace:
          at: .
      - reinstall-go
      - run: make -C helm release-gh
      - run: |
          if <<parameters.ee>>; then
            make -C helm release-gh-ee
          else
            make -C helm release-gh
          fi

  publish-python-package:
    parameters:
      path:
        type: string
      ee:
        type: boolean
        default: false
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "build twine"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C <<parameters.path>> build
      - run: |
          if <<parameters.ee>>; then
            make -C <<parameters.path>> publish-ee
          else
            make -C <<parameters.path>> publish
          fi

  check-ts-bindings:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "bindings/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C bindings force-gen
      - run: make -C bindings check/typescript

  check-py-bindings:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "bindings/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: make -C bindings force-gen
      - run: make -C bindings check/python

  upload-try-now-template:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extras-requires: "awscli"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C harness upload-try-now-template

  test-debian-packaging:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - run: sudo apt-get install -y $(pwd)/master/dist/hpe-mlde-master*.deb
      - run: sudo apt-get install -y $(pwd)/agent/dist/hpe-mlde-agent*.deb
      - run: sudo cp .circleci/packaging/master.yaml /etc/determined/master.yaml
      - run: sudo cp .circleci/packaging/agent.yaml /etc/determined/agent.yaml
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - install-devcluster
      - start-devcluster:
          target-stage: db
      - run: |
          sudo mkdir -p /etc/systemd/system/determined-master.service.d
          echo "[Service]" | sudo tee /etc/systemd/system/determined-master.service.d/password.override.conf >/dev/null
          echo "Environment=\"DET_SECURITY_INITIAL_USER_PASSWORD=${INITIAL_USER_PASSWORD}\"" | sudo tee -a /etc/systemd/system/determined-master.service.d/password.override.conf >/dev/null
          sudo systemctl daemon-reload
      - run: python3 .circleci/scripts/wait_for_server.py localhost 5432
      - run: sudo systemctl restart determined-master
      - run: python3 .circleci/scripts/wait_for_server.py localhost 8080 || { journalctl --no-pager -u determined-master; exit 1; }
      - run: sudo systemctl restart determined-agent
      - run: ./.circleci/scripts/sanity.sh

  lint-react:
    docker:
      - image: cimg/node:20.9.0
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - react-get-deps
      - run: make -C webui/react check
      - store_artifacts:
          path: /home/circleci/.npm/_logs/

  build-react:
    parameters:
      dev-mode:
        type: boolean
        default: false
      ee:
        type: boolean
        default: false
    docker:
      - image: cimg/node:20.9.0
    resource_class: large
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - react-get-deps
      - run: |
          if <<parameters.dev-mode>>; then
            echo 'Setting development mode...'
            export DET_NODE_ENV=development
          fi
          if <<parameters.ee>>; then
            echo 'building Enterprise index.html'
            export DET_BUILD_EE=true
          fi
          make -C webui/react build
      - persist_to_workspace:
          root: .
          paths:
            - webui/react/build

  test-e2e-react:
    parameters:
      ee:
        type: boolean
        default: false
      playwright-options:
        type: string
        default: ""
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - attach_workspace:
          at: .
      - react-get-deps
      - setup-python-venv:
          determined: true
          install-python: true
          executor: <<pipeline.parameters.machine-image>>
      - install-protoc
      - when:
          condition: << parameters.ee >>
          steps:
            - license-gen
      - run: PATH=$HOME/.local/bin:$PATH make -C proto get-deps
      - run: PATH=$HOME/.local/bin:$PATH make -C proto build
      - run: make -C master get-deps
      - run: make -C agent get-deps
      - install-devcluster
      - start-devcluster:
          devcluster-config: react.devcluster.yaml
      - run: make -C webui/react get-playwright-ci
      - run: SERVER_ADDRESS=${PW_SERVER_ADDRESS} npm run build --prefix webui/react
      - wait-for-master:
          host: "localhost"
          port: "8082"
      - run: npm install --save-dev dd-trace # DataDog integration
      - run:
          environment:
            PW_EE: << parameters.ee >>
            NODE_OPTIONS: "-r dd-trace/ci/init"
          command: |
              if [[ "$PW_EE" -eq 1 ]]; then env="ee"; else env="oss"; fi
              DD_ENV="ci-devcluster-$env" \
              DD_SERVICE=determined-ui-e2e \
              PW_PASSWORD=${INITIAL_USER_PASSWORD} \
              npm run e2e --prefix webui/react -- << parameters.playwright-options >>
      - store_artifacts:
          path: webui/react/src/e2e/playwright-report
      - store_artifacts:
          path: webui/react/src/e2e/test-results
      - store_artifacts:
          path: /home/circleci/.npm/_logs/
      - store_test_results:
          path: webui/react/src/e2e/junit-results.xml

  test-unit-react:
    docker:
      - image: cimg/node:20.9.0
        environment:
          CI: "true"
    resource_class: xlarge
    steps:
      - checkout-with-sm
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - react-get-deps
      - run: make -C webui/react test-ci
      - codecov/upload:
          flags: "web"
          xtra_args: "-v"
      - upload-junit-datadog:
          service:
          env: "ci-cpu"
      - store_test_results:
          path: webui/react/junit.xml
      - store_artifacts:
          path: webui/react/coverage

  lint-sql:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          extras-requires: "sqlfluff"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C master check-sql

  lint-go:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    resource_class: large
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - reinstall-go
      - go-get-deps
      - run: sudo apt-get update && sudo apt-get install -y clang-format
      - run: make -C proto build
      - run: make -C proto check
      - run: make -C master check
      - run: make -C agent check

  build-go:
    parameters:
      ee:
        type: boolean
        default: true
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - reinstall-go
      - when:
          condition: << parameters.ee >>
          steps:
            - license-gen
      - restore_cache:
          keys:
            - det-go-build-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}
      - run: make -C master build
      - run: make -C agent build
      - persist_to_workspace:
          root: .
          paths:
            - "master/build"
            - "agent/build"
      # Save cache after we build master and agent so we can have our build cache there.
      - save_cache:
          key: det-go-build-<<pipeline.parameters.cache-buster>>-{{ checksum  "go.sum" }}
          paths:
            - "/home/circleci/go/"
            - "/home/circleci/.cache/go-build/"

  build-proto:
    docker:
      - image: <<pipeline.parameters.docker-image>>
        environment:
          GO111MODULE: "on"
    steps:
      - checkout
      - reinstall-go
      - go-get-deps
      - run: make -C proto build
      - persist_to_workspace:
          root: .
          paths:
            - "proto/build/**/*"

  test-intg-master:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && sudo install minikube-linux-amd64 /usr/local/bin/minikube
      - run: minikube start
      - license-gen
      - run: make -C master test-intg
      - codecov/upload:
          flags: "backend"
          xtra_args: "-v -X fixes"
      - upload-junit-datadog:
          service: master/test-intg
          env: ci-cpu
          path: master/test-intg.junit.xml
      - store_test_results:
          path: master/test-intg.junit.xml
      - persist_to_workspace:
          root: .
          paths:
            - master/coverage.out
            - master/internal/mocks

  test-intg-agent:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - setup-python-venv:
          executor: <<pipeline.parameters.machine-image>>
      - setup-go-intg-deps
      - run: make -C agent test-intg
      - codecov/upload:
          flags: "backend"
          xtra_args: "-v -X fixes"
      - upload-junit-datadog:
          service: agent/test-intg
          env: ci-cpu
          path: agent/test-intg.junit.xml
      - store_test_results:
          path: agent/test-intg.junit.xml
      - persist_to_workspace:
          root: .
          paths:
            - agent/coverage.out

  go-coverage:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - run: cd master && go tool cover -func coverage.out
      - run: cd master && go tool cover -html coverage.out -o coverage.html
      - run: cd agent && go tool cover -func coverage.out
      - run: cd agent && go tool cover -html coverage.out -o coverage.html
      - run: mkdir go-coverage
      - run: mv master/coverage.html go-coverage/master-coverage.html
      - run: mv agent/coverage.html go-coverage/agent-coverage.html
      - store_artifacts:
          path: go-coverage
          destination: go-coverage

  check-go-coverage-master:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - run: go run master/cmd/cover-regex/cover_regex.go master/coverage.out || true

  check-go-coverage-agent:
    machine:
      image: <<pipeline.parameters.machine-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - reinstall-go
      - run: go run master/cmd/cover-regex/cover_regex.go agent/coverage.out || true

  lint-docs:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - setup-python-venv:
          install-python: false
          extra-requirements-file: "docs/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C docs check

  lint-secrets:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - run: git clone https://github.com/awslabs/git-secrets /tmp/git-secrets
      - run: cd /tmp/git-secrets && sudo make install
      - checkout
      - run: git secrets --install
      - run: git secrets --register-aws
      - run: git secrets --add '"private_key":\s"-----BEGIN\sPRIVATE\sKEY-----'
      - run: git secrets --scan-history

  lint-python:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: make -C harness check
      - run: make -C e2e_tests check
      - run: make -C tools check
      - run: make -C schemas check

  test-unit-harness-cpu:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-harness-cpu-pycov make -C harness test-cpu
      - run: coverage xml -i --data-file=./test-unit-harness-cpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-harness-cpu
          env: ci-cpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-cpu-pycov
      - store_test_results:
          path: /tmp/test-results

  test-unit-harness-gpu-tf:
    docker:
      - image: determinedai/tensorflow-ngc-dev:0736b6d
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-tf-pycov make -C harness test-gpu-tf
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-tf-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-harness-gpu
          env: ci-gpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-tf-pycov
      - store_test_results:
          path: /tmp/test-results

  test-unit-harness-pytorch2-gpu:
    docker:
      - image: determinedai/pytorch-ngc-dev:0736b6d
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-pytorch2-gpu-pycov make -C harness test-pytorch-gpu
      - run: coverage xml -i --data-file=./test-unit-harness-pytorch2-gpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-harness-pytorch2-gpu
          env: ci-gpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-pytorch2-gpu-pycov
      - store_test_results:
          path: /tmp/test-results

  test-unit-harness-pytorch2-cpu:
    docker:
      - image: determinedai/pytorch-ngc-dev:0736b6d
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - run: echo 'export PATH=$PATH:$HOME/.local/bin' >> $BASH_ENV
      - run: COVERAGE_FILE=/root/project/test-unit-harness-pytorch2-cpu-pycov make -C harness test-pytorch-cpu
      - run: coverage xml -i --data-file=./test-unit-harness-pytorch2-cpu-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-harness-pytorch2-cpu
          env: ci-cpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-pytorch2-cpu-pycov
      - store_test_results:
          path: /tmp/test-results

  test-unit-harness-gpu-parallel:
    docker:
      - image: determinedai/pytorch-ngc-dev:0736b6d
    resource_class: determined-ai/container-runner-multi-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-parallel-pycov make -C harness test-gpu-parallel
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-parallel-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-harness-gpu-parallel
          env: ci-gpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-parallel-pycov
      - store_test_results:
          path: /tmp/test-results

  test-unit-harness-gpu-deepspeed:
    docker:
      - image: determinedai/pytorch-ngc-dev:0736b6d
    resource_class: determined-ai/container-runner-gpu
    steps:
      - run: mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=/root/project/test-unit-harness-gpu-deepspeed-pycov make -C harness test-gpu-deepspeed
      - run: coverage xml -i --data-file=./test-unit-harness-gpu-deepspeed-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-harness-gpu-deepseed
          env: ci-gpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-gpu-deepspeed-pycov
      - store_test_results:
          path: /tmp/test-results

  test-unit-harness-tf2:
    docker:
      - image: determinedai/tensorflow-ngc-dev:f17151a
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - install-codecov
      - run: pip install mypy pytest coverage
      - install-codecov
      - setup-paths
      - run: COVERAGE_FILE=$PWD/test-unit-harness-tf2-pycov make -C harness test-tf2
      - run: coverage xml -i --data-file=./test-unit-harness-tf2-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-harness-tf2
          env: ci-cpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-harness-tf2-pycov
      - store_test_results:
          path: /tmp/test-results

  test-unit-storage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - install-codecov
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "harness/tests/requirements/requirements-harness.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run: COVERAGE_FILE=$PWD/test-unit-storage-pycov coverage run -m pytest -v --durations=0 --junitxml=/tmp/test-results/test-unit-storage.xml --require-secrets -m cloud harness/tests
      - run: coverage xml -i --data-file=./test-unit-storage-pycov
      - run: codecov -v -t $CODECOV_TOKEN -F harness
      - upload-junit-datadog:
          service: test-unit-storage
          env: ci-cpu
      - persist_to_workspace:
          root: .
          paths:
            - test-unit-storage-pycov
      - store_test_results:
          path: /tmp/test-results

  python-coverage:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - setup-python-venv:
          install-python: false
          determined: false
          extras-requires: "coverage"
          executor: <<pipeline.parameters.docker-image>>
      - attach_workspace:
          at: .
      - run: coverage combine *-pycov
      - run: coverage report --include 'harness/determined/*' --skip-covered
      - run: coverage html --include 'harness/determined/*' --skip-covered -d cov-html/harness
      - store_artifacts:
          path: cov-html
          destination: cov-html

  test-cli:
    parameters:
      executor-name:
        type: string
    executor: << parameters.executor-name >>
    steps:
      - checkout
      - python-report
      # Running the pip executable causes an error with the win/default executor for some reason.
      - run: python -m pip install --upgrade --user pip
      - run: pip install wheel setuptools
      - run: cd harness; python setup.py bdist_wheel -d ../build
      - run: pip install --find-links build determined==<< pipeline.parameters.det-version >>
      - run: pip freeze --all
      # Allow this to fail, but it is useful for debugging.
      - run: sh -c "pip check || true"
      # Ensure Determined cli can run without installing cli test requirements
      - run: det --help
      - run: pip install setuptools_scm setuptools
      - run: pip install -r harness/tests/requirements/requirements-cli.txt
      - run: pip freeze --all
      - python-report
      - run: pytest --junitxml=/tmp/test-results/test-unit-storage.xml harness/tests/cli
      - store_test_results:
          path: /tmp/test-results

  test-e2e-slurm-disabled:
    parameters:
      master_config:
        type: string
        default:
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - run: echo "Test suite disabled."

  # By default, this job only runs on the main branch unless otherwise
  # specified. To invoke this job on a developer branch, add the 'ci-run-allgcp'
  # label to your pull request on github.
  test-e2e-hpc-gcp:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      extra-pytest-flags:
        type: string
        default: ""
      container-run-type:
        type: string
        default: singularity
      workload-manager:
        type: string
        default: slurm
      master-scheme:
        type: string
        default: "http"
      master-host:
        type: string
        default: "localhost"
      master-port:
        type: string
        default: "8080"
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id

      - run:
          name: Create instance name hash and set OPT_DEVBOX_PREFIX env var
          command: |
            # This check is for the scenario when two pushes to main are running concurrently (we don't want this).
            # In this case, we need more specificity to the hash information to make two separate VMs.
            if [[ $CIRCLE_BRANCH == "main" ]]; then
              echo "export OPT_DEVBOX_PREFIX=circleci-job-$(echo -n "${CIRCLE_USERNAME}-${CIRCLE_BRANCH}-${CIRCLE_JOB}-${CIRCLE_WORKFLOW_JOB_ID}" | md5sum | awk '{print $1}')" >> "$BASH_ENV"
            else
              echo "export OPT_DEVBOX_PREFIX=circleci-job-$(echo -n "${CIRCLE_USERNAME}-${CIRCLE_BRANCH}-${CIRCLE_JOB}" | md5sum | awk '{print $1}')" >> "$BASH_ENV"
            fi

      - run:
          name: Set initial user password
          command: |
            echo "export INITIAL_USER_PASSWORD=${INITIAL_USER_PASSWORD}" >> "$BASH_ENV"

      - attach_workspace:
          at: .

      - reinstall-go
      - go-get-deps

      - setup-python-venv:
          install-python: true
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - install-devcluster
      - license-gen

      - gcloud/install:
          version: "412.0.0"
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>

      - run:
          name: Install terraform
          command: |
            wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
            echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
            sudo apt update && sudo apt install terraform
      - run: terraform --version

      - set-google-application-credentials

      - run:
          name: Delete existing resources (if they exist)
          command: |
            # This grabs the zone.default from tools/slurm/terraform/variables.tf used to specify the zone (as this
            # is not set at this point since no terraform initiation has happened. See workaround section in README)
            GCLOUD_ZONE=$(grep -A2 'variable "zone"' ~/project/tools/slurm/terraform/variables.tf | grep 'default' \
            | awk -F' = ' '{ print $2 }' | tr -d '\"')
            gcloud compute instances delete ${OPT_DEVBOX_PREFIX}-dev-box --zone=$GCLOUD_ZONE --quiet || true
            gcloud compute firewall-rules delete ${OPT_DEVBOX_PREFIX}-dev-box --quiet || true
            gcloud compute networks delete ${OPT_DEVBOX_PREFIX}-dev-box --quiet || true

      - run:
          name: Make slurmcluster
          # Breaks without apt-get installs for some reason
          # NOTE: TF_LOCK is set to 'false' to catch the case where a developer triggered a workflow and did not
          # let it run completely. This should not be an issue since we delete the instance already in the previous
          # step (this is precautionary). See a similar thing in the 'make unslurmcluster' step.
          command: |
            sudo apt-get update
            sudo apt-get install gettext
            sudo apt-get install iproute2
            yes yes | PATH=$HOME/.local/bin:$PATH make slurmcluster \
              FLAGS="-c <<parameters.container-run-type>> -w <<parameters.workload-manager>> -l '{owner=\\\"determined_ci\\\", gh_team=\\\"machine-user\\\"}'" \
              TF_LOCK=false | tee output.log || circleci-agent step halt
          background: true

      - run:
          name: Wait for VM creation
          command: |
            sleep 5
            for i in {1..500}; do
              ELAPSED_TIME=$((i * 5))
              echo "Waiting for VM creation [${ELAPSED_TIME}s]..."
              if grep -q "^Running cluster..." output.log; then
                echo "VM creation has finished."
                break
              fi
              if [[ $i -eq 500 ]]; then
                echo "Timeout waiting for VM creation."
                exit 1
              fi
              sleep 5
            done

      - run-e2e-tests:
          mark: <<parameters.mark>>
          wait-for-master: true
          master-scheme: <<parameters.master-scheme>>
          master-host: <<parameters.master-host>>
          master-port: <<parameters.master-port>>
          extra-pytest-flags: <<parameters.extra-pytest-flags>>

      - run:
          name: Make unslurmcluster
          when: always
          command: |
            # These scripts always run with `set -e`. Be careful that a failure doesn't stop
            # make unslurmcluster from running, otherwise resources are left around.
            pkill determined-mast || echo "failed to kill master (exit code $?)"
            (yes yes || true) | make unslurmcluster TF_LOCK=false
            # For some reason, even when `make unslurmcluster` is successful, CircleCI
            # receives exit code 141, so ignore that.
            EXIT_STATUS=$?
            echo $EXIT_STATUS
            if [[ $EXIT_STATUS -eq 141 ]]; then
              echo "Ignoring exit code 141"
              exit 0
            else
              exit $EXIT_STATUS
            fi
      - store_test_results:
          path: /tmp/test-results/

  test-e2e-slurm:
    parameters:
      mark:
        type: string
        default: e2e_slurm
      runner_class:
        type: string
        default: determined-ai/znode-cluster
      master_config:
        type: string
        default: |
          task_container_defaults:
            slurm:
              sbatch_args:
                - --time=04:00:00
            environment_variables:
              # Some ports are not working, disable them so distributed jobs work.
              - NCCL_IB_HCA=mlx6_0:0
          checkpoint_storage:
            type: shared_fs
            host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
            storage_path: determined-checkpoint
            save_experiment_best: 0
            save_trial_best: 1
            save_trial_latest: 1
          db:
            user: postgres
            host: localhost
            port: 5432
            name: determined
            password: ${HPC_DB_PASSWD}
          resource_manager:
            type: slurm
            master_host: $HOSTNAME
            master_port: 8080
            host: localhost
            port: 8181
            protocol: http
            slot_type: cuda
            user_name: launcher
            group_name: hpcd
            singularity_image_root: /lustre/hdd/foundation_engineering/images
            job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
            auth_file: /home/launcher/.launcher.$HOSTNAME.token
            path: /opt/singularity/bin:/usr/local/bin:${PATH}
            ld_library_path:
          security:
            initial_user_password: ${INITIAL_USER_PASSWORD}
      reserved_ports_znode50:
        type: string
        default: |
          reserved_ports:
            - 12350
            - 12351
            - 12360
            - 12361
            - 12365
            - 12366
            - 29400
      determined_master_host:
        type: string
        default: localhost:8080
      cluster_unix_user:
        type: string
        default: launcher
      cluster_determined_user:
        type: string
        default: determined
      determined_admin_username:
        type: string
        default: admin
      database_username:
        type: string
        default: postgres
      database_password:
        type: string
        default: launcher
      extra-pytest-flags:
        type: string
        default: ""
      collect-det-job-logs:
        type: boolean
        default: true
    # Following https://circleci.com/docs/2.0/runner-installation-linux/index.html#start-the-service
    machine: true
    resource_class: <<parameters.runner_class>>
    environment:
      SHARED_CLUSTER: True
    steps:
      - checkout
      - attach_workspace:
          at: .

      - set-slack-user-id
      - run: sudo yum install -y xmlsec1
      - run:
          name: Remove previous HPE MLDE Master RPM
          # FE-35: the slow exit of a container, or NFS caching can cause incorrect response
          # and failure executing command "rm -rf ${dir}". Wait for 5 seconds and retry the command.
          # Ignore the failure on retry, cause the failure from "rm -rf" should not stop the test suite to run.
          command: |
            export DET_PKG_NAME=$(rpm -qp --queryformat "%{NAME}" master/dist/hpe-mlde-master_*-ee_linux_amd64.rpm)
            if rpm -q $DET_PKG_NAME; then
              sudo rpm -e $DET_PKG_NAME
            fi
            echo "Cleanup state from prior runs on HOSTNAME=$HOSTNAME"

            dir="/scratch/launcher/.launcher.$HOSTNAME/checkpoints/determined-checkpoint"
            if sudo rm -rf ${dir}; then
              echo "Removed ${dir}"
            else
              sleep 5
              echo "Retry cleanup ${dir}"
              sudo rm -rf ${dir} || true
            fi

            dir="/scratch/launcher/.launcher.$HOSTNAME/archiveVolumes/"
            if sudo rm -rf ${dir}; then
              echo "Removed ${dir}"
            else
              sleep 5
              echo "Retry cleanup ${dir}"
              sudo rm -rf ${dir} || true
            fi

            dir="/scratch/launcher/.launcher.$HOSTNAME/jobs"
            if sudo rm -rf ${dir}; then
              echo "Removed ${dir}"
            else
              sleep 5
              echo "Retry cleanup ${dir}"
              sudo rm -rf ${dir} || true
            fi

      - setup-python-venv:
          determined: True
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          install-python: false
          executor: <<parameters.runner_class>>

      - run:
          name: Recreate Fresh Database
          command: |
            if systemctl is-active --quiet determined-master; then
              sudo systemctl stop determined-master
            fi
            PGPASSWORD=<<parameters.database_password>> dropdb --host=localhost --port=5432 --username=<<parameters.database_username>> --if-exists determined
            PGPASSWORD=<<parameters.database_password>> createdb --host=localhost --port=5432 --username=<<parameters.database_username>> determined

      - run:
          name: Install/Configure HPE MLDE Master
          command: |
            sudo rpm -i master/dist/hpe-mlde-master_*-ee_linux_amd64.rpm
            cat \<< EOF > ./master.yaml
            <<parameters.master_config>>
            EOF
            cat \<< EOF > ./reserved.yaml
            <<parameters.reserved_ports_znode50>>
            EOF
            echo "hostname is $(hostname)"
            # Disallow certain ports on znode50 for fewer conflicts with znode51
            if [[ "$(hostname)" == "znode50" ]] ; then cat ./reserved.yaml >> ./master.yaml ; fi
            cat ./master.yaml
            sudo cp ./master.yaml /etc/determined/master.yaml
            sudo systemctl daemon-reload
            sudo systemctl start determined-master
            # Show if there are any drained nodes
            sinfo -R
            # Resume any drained nodes due to problems killing podman processes.
            # This will return an error if all nodes do not require a resume (ignore status)
            sudo scontrol update nodename=znode5[0-1,3-4] state=resume || true
            sudo su - launcher -c "pdsh -R ssh -w znode50,znode51,znode53,znode54 mkdir -p /tmp/launcher_podman" || true
            # Cleanup podman state from any issues in prior runs
            #sudo su - launcher -c "pdsh -R ssh -w znode50,znode51,znode53,znode54 XDG_RUNTIME_DIR=/tmp/launcher_podman podman system migrate" || true
            sinfo -R

      - run:
          name: Download Apptainer/Enroot image
          command: |
            # rocm images are not required at present
            if sudo su - launcher -c "/etc/launcher/scripts/manage-singularity-cache <<pipeline.parameters.default-pt-gpu-hpc-image>>"; then
                echo "Downloaded Singularity image <<pipeline.parameters.default-pt-gpu-hpc-image>> "
            else
                EXIT_STATUS=$?
                echo "Failed downloading Singularity image. Received exit code $EXIT_STATUS"
                #Ignore the other failures except for IMAGE_REF_NOT_FOUND_IN_DOC=18 or DOC_FILE_NOT_FOUND=11
                if [[ $EXIT_STATUS -eq 18 || $EXIT_STATUS -eq 11 ]]; then
                    exit $EXIT_STATUS
                else
                    exit 0
                fi
            fi
            if sudo su - launcher -c "ENROOT_RUNTIME_PATH=/tmp/launcher /etc/launcher/scripts/manage-enroot-cache -s /lustre/ssd/foundation_engineering/ <<pipeline.parameters.default-pt-tf-cpu-hpc-image>>"; then
                echo "Downloaded Enroot image <<pipeline.parameters.default-pt-tf-cpu-hpc-image>> "
            else
                EXIT_STATUS=$?
                echo "Failed downloading Enroot image. Received exit code $EXIT_STATUS"
                #Ignore the other failures except for IMAGE_REF_NOT_FOUND_IN_DOC=18 or DOC_FILE_NOT_FOUND=11
                if [[ $EXIT_STATUS -eq 18 || $EXIT_STATUS -eq 11 ]]; then
                    exit $EXIT_STATUS
                else
                    exit 0
                fi
            fi

      - wait-for-master:
          host: localhost

      - run:
          name: Populate determined user agent values
          command: |
            id <<parameters.cluster_unix_user>> || sudo useradd <<parameters.cluster_unix_user>>
            TOKEN=$(
              curl "<<parameters.determined_master_host>>/api/v1/auth/login" \
                -f \
                -X POST \
                --data-binary @- \<< EOF | jq -r '.token'
              {
                  "username": "<<parameters.determined_admin_username>>",
                  "password": "$INITIAL_USER_PASSWORD"
              }
            EOF
            )
            curl "<<parameters.determined_master_host>>/api/v1/users/2" \
              -f \
              -X PATCH \
              -H "Authorization: Bearer ${TOKEN}" \
              --data-binary @- \<< EOF
              {"agentUserGroup": {
                "agentUid": $(id -u <<parameters.cluster_unix_user>>),
                "agentUser": "<<parameters.cluster_unix_user>>",
                "agentGid": $(id -g <<parameters.cluster_unix_user>>),
                "agentGroup": "$(id -gn <<parameters.cluster_unix_user>>)"
                }
              }
            EOF

      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: localhost
          managed-devcluster: false
          extra-pytest-flags: <<parameters.extra-pytest-flags>>
          collect-det-job-logs: <<parameters.collect-det-job-logs>>

      - store_test_results:
          path: /tmp/test-results/

  test-e2e:
    parameters:
      tf2:
        type: boolean
        default: false
      mark:
        type: string
      parallelism:
        type: integer
      devcluster-config:
        type: string
        default: double.devcluster.yaml
      target-stage:
        type: string
        default: agent1
      managed-devcluster:
        type: boolean
        default: false
      postgres-version:
        type: string
        default: "10"
      agent-version:
        type: string
        default: ""
      extra-pytest-flags:
        type: string
        default: ""
      wait-for-master:
        type: boolean
        default: true
      run-minikubes:
        type: boolean
        default: false
      multi-k8s:
        type: boolean
        default: false
      resource-class:
        type: string
        default: xlarge
      collect-det-job-logs:
        type: boolean
        default: true
      k8s-version:
        type: string
        default: "1.29.5"
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: <<parameters.resource-class>>
    parallelism: <<parameters.parallelism>>
    environment:
      DET_POSTGRES_VERSION: <<parameters.postgres-version>>
      DET_AGENT_VERSION: <<parameters.agent-version>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .

      - setup-python-venv:
          determined: True
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - when:
          condition: <<parameters.run-minikubes>>
          steps:
            - kubernetes/install-kubectl
            - run:
                name: Install minikube
                command: |
                  curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && sudo install minikube-linux-amd64 /usr/local/bin/minikube
            - run:
                name: Start defaultrm minikube
                command: |
                  K8S_VERSION=<<parameters.k8s-version>> source tools/k8s/launch-minikube-with-gateway.sh defaultrm
                  echo "export GATEWAY_IP=\"${GATEWAY_IP}\"" >> $BASH_ENV
            - when:
                condition: <<parameters.multi-k8s>>
                steps:
                  - run:
                      name: Start additionalrm minikube
                      command: minikube start --profile additionalrm --kubernetes-version <<parameters.k8s-version>>
      - install-devcluster
      - unless:
          condition: <<parameters.managed-devcluster>>
          steps:
            - start-devcluster:
                devcluster-config: <<parameters.devcluster-config>>
                target-stage: <<parameters.target-stage>>

      - pull-task-images:
          tf2: <<parameters.tf2>>

      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: localhost
          managed-devcluster: <<parameters.managed-devcluster>>
          extra-pytest-flags: <<parameters.extra-pytest-flags>>
          wait-for-master: <<parameters.wait-for-master>>
          collect-det-job-logs: <<parameters.collect-det-job-logs>>

      - store_test_results:
          path: /tmp/test-results/

      - when:
          condition: <<parameters.managed-devcluster>>
          steps:
            - store_artifacts:
                path: /tmp/devcluster/
                destination: devcluster-logs
            - store_artifacts:
                path: /tmp/priority_scheduler
                destination: devcluster-priority_scheduler-logs

  send-job-level-alerts:
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: small
    steps:
      - checkout
      - add-and-fetch-upstream
      - attach_workspace:
          at: .
      - run:
          name: Install alert deps
          command: tools/scripts/retry.sh pip install requests slack_sdk
      - run:
          name: Listen for failures and send slack alerts
          command: python .circleci/scripts/job_level_alerts.py

  test-perf:
    parameters:
      snapshot-after-migrations:
        type: boolean
        default: false
      deploy-db:
        type: boolean
        default: false
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: xlarge
    steps:
      - queue/until_front_of_line:
          only-on-branch: main
          time: "120" # Wait two hours at most. Adjust this over time.
      - checkout
      - add-and-fetch-upstream
      - attach_workspace:
          at: .
      - setup-python-venv:
          determined: True
          executor: <<pipeline.parameters.machine-image>>
      - install-devcluster
      - run:
          name: Install upload deps
          command: tools/scripts/retry.sh pip install requests psycopg2-binary

      - when:
          condition: <<parameters.deploy-db>>
          steps:
            - run:
                name: Select snapshot to use
                command: |
                  echo 'export PERF_SNAPSHOT_TO_USE="perf-test-base-snapshot"' >> "$BASH_ENV"

                  SNAPSHOT_COMMITS=$(aws rds describe-db-snapshots \
                    --region="us-west-2" \
                    --query "DBSnapshots[?TagList[?Key=='ci-snapshot']].DBSnapshotIdentifier" \
                    --output json | jq -r '.[] | split("-")[3]')
                  echo "Snapshot commits (${SNAPSHOT_COMMITS})"

                  for ((n=0; n<=1000; n++)); do
                    COMMIT=$(git log --format="%H" -n 1 --skip=$n)

                    if [[ " $SNAPSHOT_COMMITS " =~ .*"$COMMIT".* ]]; then
                      echo "export PERF_SNAPSHOT_TO_USE=\"ci-snapshot-commit-${COMMIT}\"" >> "$BASH_ENV"
                      break
                    fi
                  done

                  source $BASH_ENV
                  echo "Deciding to use $PERF_SNAPSHOT_TO_USE"
            - run:
                name: Wait for snapshot to be available
                command: |
                  aws rds wait db-snapshot-available \
                    --region "us-west-2" \
                    --db-snapshot-identifier "${PERF_SNAPSHOT_TO_USE}"
            - run:
                name: Deploy database
                command: |
                  aws rds restore-db-instance-from-db-snapshot \
                    --region="us-west-2" \
                    --db-snapshot-identifier="${PERF_SNAPSHOT_TO_USE}" \
                    --db-instance-identifier="ci-perf-db-${CIRCLE_BUILD_NUM}" \
                    --no-multi-az \
                    --no-publicly-accessible \
                    --no-auto-minor-version-upgrade \
                    --db-parameter-group-name="logquerieslong" \
                    --tags "Key=ci-snapshot" \
                    --vpc-security-group-ids="${PERF_DB_SECURITY_GROUP_ID}" \
                no_output_timeout: 30m
            - run:
                name: Wait for database to be ready
                command: |
                  aws rds wait db-instance-available \
                    --region="us-west-2" \
                    --db-instance-identifier="ci-perf-db-${CIRCLE_BUILD_NUM}"
                no_output_timeout: 30m
            - run:
                name: Get db instance host
                command: |
                  echo "export PERF_DB_HOST=$(aws rds describe-db-instances \
                    --region us-west-2 \
                    --db-instance-identifier "ci-perf-db-${CIRCLE_BUILD_NUM}" \
                    --query "DBInstances[0].Endpoint.Address" \
                    --output text)" >> "$BASH_ENV"
                  source $BASH_ENV
                  echo "perf db host ${PERF_DB_HOST}"

      - run:
          name: Add SSH key
          command: echo "${PERF_DB_BASTION_SSH_KEY}" | base64 --decode | ssh-add -
      - run:
          name: Port forward to bastion instance
          command: ssh -L 5432:${PERF_DB_HOST}:5432 -N -f ubuntu@$PERF_DB_BASTION_HOST
      - start-devcluster:
          target-stage: master
          devcluster-config: perftest.devcluster.yaml
      - run:
          name: Wait and record any migrations ran
          command: python .circleci/scripts/wait_for_perf_migration_upload_results.py

      - when:
          condition: <<parameters.snapshot-after-migrations>>
          steps:
            - run:
                name: Take and wait for RDS snapshot, only on main and when migrations were applied
                command: |
                  if [ -f /tmp/no-migrations-needed ]; then
                    echo "/tmp/no-migrations-needed exists, no need to take a snapshot"
                    exit 0
                  fi

                  COMMIT=$(git log -1 --pretty=format:%H)
                  echo "Taking snapshot"
                  aws rds create-db-snapshot \
                    --region="us-west-2" \
                    --db-instance-identifier="${PERF_DB_AWS_NAME}" \
                    --db-snapshot-identifier="ci-snapshot-commit-${COMMIT}" \
                    --tags "Key=ci-snapshot"

                  echo "Snapshot taken now waiting for it to become completed"
                  aws rds wait db-snapshot-completed \
                    --region="us-west-2" \
                    --db-snapshot-identifier="ci-snapshot-commit-${COMMIT}"
                  echo "Snapshot completed"
      - run:
          name: Run REST API read-only test
          command: ./performance/daist/run.py daist.rest_api.test_locust.TestRO

      - store_artifacts:
          path: ./performance/daist/daist/results

      - run:
          name: Delete RDS instance
          when: always
          command: |
            if [ -n "$PERF_SNAPSHOT_TO_USE" ]; then
              aws rds delete-db-instance \
                --region="us-west-2" \
                --db-instance-identifier="ci-perf-db-${CIRCLE_BUILD_NUM}" \
                --skip-final-snapshot
            else
              echo "PERF_SNAPSHOT_TO_USE is not set. Skipping deletion."
            fi

  deploy:
    parameters:
      compute-agent-instance-type:
        type: string
        default: "g4dn.xlarge"
      aux-agent-instance-type:
        type: string
        default: "m6i.large"
      cluster-id:
        type: string
        default: determined-${CIRCLE_BRANCH////--}
      max-dynamic-agents:
        type: integer
        default: 1
      enable-cors:
        type: boolean
        default: false
      reattach-enabled:
        type: boolean
        default: false
      extra-tags:
        type: string
        default: ""
      ee:
        type: boolean
        default: false
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - queue/until_front_of_line:
          only-on-branch: main
          # Basically wait forever -- we would prefer not to fail deploys, and
          # we'll likely never be this backed up.
          time: "10000"
      - checkout
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          executor: <<pipeline.parameters.docker-image>>
      - deploy-aws-cluster:
          cluster-id: <<parameters.cluster-id>>
          det-version: ${CIRCLE_SHA1}
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          enable-cors: <<parameters.enable-cors>>
          reattach-enabled: <<parameters.reattach-enabled>>
          deployment-type: simple
          extra-tags: <<parameters.extra-tags>>
          ee: <<parameters.ee>>

  deploy-latest-gke:
    parameters:
      det-version:
        type: string
      cluster-id:
        type: string
        default: latest-ee-gke
      region:
        type: string
        default: us-west1
      gcloud-service-key:
        default: GCLOUD_SERVICE_KEY
        description: The gcloud service key
        type: env_var_name
      google-compute-zone:
        default: GOOGLE_COMPUTE_ZONE
        description: The Google compute zone to connect with via the gcloud CLI
        type: env_var_name
      google-project-id:
        default: GOOGLE_PROJECT_ID
        description: The Google project ID to connect with via the gcloud CLI
        type: env_var_name
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - set-slack-user-id
      - gcloud/install:
          version: "319.0.0"
      - kubernetes/install-kubectl
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - run:
          command: gcloud container clusters get-credentials <<parameters.cluster-id>> --project determined-ai --region <<parameters.region>>
          name: Get Kubeconfig
      - helm/upgrade-helm-chart:
          chart: helm/charts/determined
          helm-version: v3.2.4
          namespace: "default"
          wait: true
          release-name: <<parameters.cluster-id>>
          values-to-override: |
            detVersion=<<parameters.det-version>>,\
            maxSlotsPerPod=4,\
            checkpointStorage.type=gcs,\
            checkpointStorage.bucket=det-ci,\
            imagePullSecretName=release-cred,\
            masterPort=80,\
            security.authz.type=rbac,\
            enterpriseEdition=true

  test-e2e-aws:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      compute-agent-instance-type:
        type: string
        default: g4dn.xlarge
      aux-agent-instance-type:
        type: string
        default: m6i.large
      max-dynamic-agents:
        type: integer
        default: 1
      parallelism:
        type: integer
        default: 1
      enable-tls:
        type: boolean
        default: false
      environment-gpu-enabled:
        type: string
        default: "1"
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - when:
          condition: <<parameters.enable-tls>>
          steps:
            - generate-tls-cert
      - setup-aws-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          extra-tags: test-mark=<<parameters.mark>>
          det-version: ${CIRCLE_SHA1}
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
          ee: true
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
          master-scheme: ${MASTER_SCHEME:-http}
          master-port: ${MASTER_PORT:-8080}
          master-cert: ${MASTER_TLS_CERT}
          master-cert-name: ${MASTER_CERT_NAME}
          wait-for-master: false
      - locate-cloudwatch-logs:
          cluster-id: ${CLUSTER_ID}
      - terminate-aws-cluster:
          cluster-id: ${CLUSTER_ID}
      - store_test_results:
          path: /tmp/test-results/

  test-e2e-gke:
    parameters:
      cluster-id-prefix:
        type: string
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      gke-version:
        type: string
        default: <<pipeline.parameters.gke-version>>
      machine-type:
        type: string
        default: "n1-standard-8"
      num-machines:
        type: integer
        default: 1
      gpu-type:
        type: string
        default: "nvidia-tesla-t4"
      gpus-per-machine:
        type: integer
        default: 1
      slot-type:
        type: string
        default: gpu
      slot-resource-requests-cpu:
        type: integer
        default: 0
      max-slots-per-pod:
        type: integer
        default: 1
      region:
        type: string
        default: "us-west1"
      node-locations:
        type: string
        default: "us-west1-b"
      environment-gpu-enabled:
        type: string
        default: "1"
      environment-image:
        default: determinedai/pytorch-ngc-dev:0736b6d
        type: string
      accel-node-taints:
        type: string
        default: ""
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - queue/until_front_of_line:
          only-on-branch: main
          # Basically wait forever -- we would prefer not to fail tests, and
          # we'll likely never be this backed up.
          time: "10000"
      - setup-gke-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          labels: test-mark=<<parameters.mark>>
          det-version: ${CIRCLE_SHA1}
          gke-version: <<parameters.gke-version>>
          machine-type: <<parameters.machine-type>>
          num-machines: <<parameters.num-machines>>
          gpu-type: <<parameters.gpu-type>>
          gpus-per-machine: <<parameters.gpus-per-machine>>
          slot-type: <<parameters.slot-type>>
          slot-resource-requests-cpu: <<parameters.slot-resource-requests-cpu>>
          max-slots-per-pod: <<parameters.max-slots-per-pod>>
          region: <<parameters.region>>
          node-locations: <<parameters.node-locations>>
          environment-image: <<parameters.environment-image>>
          accel-node-taints: <<parameters.accel-node-taints>>
      - set-google-application-credentials
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
      - store_test_results:
          path: /tmp/test-results/
      - terminate-gke-cluster:
          cluster-id: ${CLUSTER_ID}
          region: <<parameters.region>>

  test-e2e-shared-cluster:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      environment-gpu-enabled:
        type: string
        default: "0"
      test-type:
        type: string
    circleci_ip_ranges: true
    environment:
      DET_TEST_GPU_ENABLED: <<parameters.environment-gpu-enabled>>
      SHARED_CLUSTER: true
    docker:
      - image: <<pipeline.parameters.docker-image>>
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - attach_workspace:
          at: .
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - run:
          name: Create Namespace & Cert Name
          command: |
            TIMESTAMP=$(date +"%Y%m%d%H%M%S")
            uuid=$(cat /proc/sys/kernel/random/uuid)
            uuid=${uuid:0:8}
            echo "GENERATED_NAMESPACE=test-<<parameters.test-type>>-${TIMESTAMP}-${uuid}-${CIRCLE_NODE_INDEX}" >> $BASH_ENV
      - generate-tls-cert
      - setup-shared-cluster:
          det-version: ${CIRCLE_SHA1}-shared-cluster
          labels: test-mark=<<parameters.mark>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
      - set-google-application-credentials
      - run:
          name: Set initial user password
          command: |
            echo "export INITIAL_USER_PASSWORD=${INITIAL_USER_PASSWORD}" >> $BASH_ENV
      - run-e2e-tests:
          mark: <<parameters.mark>>
          master-host: ${MASTER_HOST}
          master-scheme: ${MASTER_SCHEME:-http}
          master-port: ${MASTER_PORT:-8080}
          master-cert: ${MASTER_TLS_CERT}
          master-cert-name: ${MASTER_CERT_NAME}
          wait-for-master: true
      - store_test_results:
          path: /tmp/test-results/

  test-det-deploy:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      # because of name collisions, we have to split and specify ee and oss images.
      - run: docker load --input build/master-oss.image
      - run: docker load --input build/agent-oss.image
      - run: docker load --input build/master-ee.image
      - run: docker load --input build/agent-ee.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  test-stress:
    parameters:
      mark:
        type: string
      parallelism:
        type: integer
        default: 1
      det-version:
        type: string
    machine:
      image: <<pipeline.parameters.machine-image>>
    resource_class: large
    parallelism: <<parameters.parallelism>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - attach_workspace:
          at: .
      - run: docker load --input build/master.image
      - run: docker load --input build/agent.image

      - setup-python-venv:
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.machine-image>>

      - run-det-deploy-tests:
          mark: <<parameters.mark>>
          det-version: <<parameters.det-version>>

  test-aws-fs:
    parameters:
      cluster-id-prefix:
        type: string
      deployment-type:
        type: string
      compute-agent-instance-type:
        type: string
        default: g4dn.xlarge
      aux-agent-instance-type:
        type: string
        default: m6i.large
      max-dynamic-agents:
        type: integer
        default: 1
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - add-and-fetch-upstream
      - skip-if-only-docs
      - skip-if-only-github
      - skip-if-only-webui
      - set-slack-user-id
      - setup-python-venv:
          install-python: false
          determined: true
          extra-requirements-file: "e2e_tests/tests/requirements.txt"
          executor: <<pipeline.parameters.docker-image>>
      - setup-aws-cluster:
          cluster-id: <<parameters.cluster-id-prefix>>-$(git rev-parse --short HEAD)-${CIRCLE_BUILD_NUM}-${CIRCLE_NODE_INDEX}
          extra-tags: test-mark=fs
          det-version: ${CIRCLE_SHA1}
          aux-agent-instance-type: <<parameters.aux-agent-instance-type>>
          compute-agent-instance-type: <<parameters.compute-agent-instance-type>>
          max-dynamic-agents: <<parameters.max-dynamic-agents>>
          master-tls-cert: ${MASTER_TLS_CERT}
          master-tls-key: ${MASTER_TLS_KEY}
          master-cert-name: ${MASTER_CERT_NAME}
          deployment-type: <<parameters.deployment-type>>
          ee: true
      - run:
          name: Set DET_MASTER for further action.
          command: |
            echo "export DET_MASTER=${MASTER_SCHEME:-http}://${MASTER_HOST}:${MASTER_PORT:-8080}" >> "$BASH_ENV"
      - when:
          condition:
            equal: [<<parameters.deployment-type>>, "fsx"]
          steps:
            - run:
                name: Test FSx/Lustre
                command: |
                  DET_USER=admin DET_PASS=${INITIAL_USER_PASSWORD} \
                  det cmd run --config resources.slots=1 'mount | grep lustre && echo MARKER' | grep MARKER
      - when:
          condition:
            equal: [<<parameters.deployment-type>>, "efs"]
          steps:
            - run:
                name: Test EFS
                command: |
                  DET_USER=admin DET_PASS=${INITIAL_USER_PASSWORD} \
                  det cmd run --config resources.slots=1 'mount | grep nfs4 && echo MARKER' | grep MARKER
      - locate-cloudwatch-logs:
          cluster-id: ${CLUSTER_ID}
      - terminate-aws-cluster:
          cluster-id: ${CLUSTER_ID}

  terminate-vpc-circleci:
    parameters:
        gcloud-service-key:
          default: GCLOUD_SERVICE_KEY
          description: The gcloud service key
          type: env_var_name
        google-compute-zone:
          default: GOOGLE_COMPUTE_ZONE
          description: The Google compute zone to connect with via the gcloud CLI
          type: env_var_name
        google-project-id:
          default: GOOGLE_PROJECT_ID
          description: The Google project ID to connect with via the gcloud CLI
          type: env_var_name
    docker:
      - image: <<pipeline.parameters.docker-image>>
    steps:
      - checkout
      - attach_workspace:
          at: .
      - gcloud/install:
          version: "412.0.0"
      - gcloud/initialize:
          gcloud-service-key: <<parameters.gcloud-service-key>>
          google-compute-zone: <<parameters.google-compute-zone>>
          google-project-id: <<parameters.google-project-id>>
      - set-google-application-credentials
      - run:
          name: Terminate any unnecessary VPC networks and firewall rules from CircleCI jobs
          command: |
            ./.circleci/scripts/terminate-vpc-circleci.sh

  run-shared-cluster-cleanup:
      parameters:
        cluster-id:
          type: string
          default: ${GKE_CLUSTER_NAME}
        region:
          type: string
          default: ${GKE_REGION}
        gcloud-service-key:
          default: GCLOUD_SERVICE_KEY
          description: The gcloud service key
          type: env_var_name
        google-compute-zone:
          default: GOOGLE_COMPUTE_ZONE
          description: The Google compute zone to connect with via the gcloud CLI
          type: env_var_name
        google-project-id:
          default: GOOGLE_PROJECT_ID
          description: The Google project ID to connect with via the gcloud CLI
          type: env_var_name
      circleci_ip_ranges: true
      docker:
        - image: <<pipeline.parameters.docker-image>>
      steps:
        - set-cluster-id:
              cluster-id: <<parameters.cluster-id>>
        - gcloud/install:
              version: "412.0.0"
        - kubernetes/install-kubectl
        - gcloud/initialize:
            gcloud-service-key: <<parameters.gcloud-service-key>>
            google-compute-zone: <<parameters.google-compute-zone>>
            google-project-id: <<parameters.google-project-id>>
        - run:
            command: |
              tries=5
              until gcloud components install gke-gcloud-auth-plugin --quiet; do
                if [[ $((--tries)) -eq 0 ]]; then
                  exit 1
                fi
                sleep 15
              done
              echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $BASH_ENV
            name: Install GKE auth plugin
        - run:
            name: Get Kubeconfig
            command: gcloud container clusters get-credentials ${CLUSTER_ID} --project ${GOOGLE_PROJECT_ID} --region <<parameters.region>>
        - run:
            name: Delete GKE CI Cluster Namespaces
            command: |
              kubectl get namespace | grep -Eo "^test-cpu-[a-z0-9]+-[a-z0-9]+-[0-9]" | xargs -L1 kubectl delete namespace || true
        - run:
            name: Delete GCS CI Buckets
            command: |
              gsutil ls -p ${GOOGLE_PROJECT_ID} | grep -Eo "^gs://test-cpu-[a-z0-9]+-[a-z0-9]+-[0-9]-bucket" | xargs -L1 gsutil -m rm -r || true

workflows:
  lint:
    <<: *do-not-run-on-manual-trigger
    jobs:
      - build-proto
      - check-py-bindings:
          requires:
            - build-proto
      - check-ts-bindings:
          requires:
            - build-proto
      - lint-docs
      - lint-python
      - lint-go
      - lint-sql
      - lint-react
      - lint-secrets

  previews:
    <<: *do-not-run-on-manual-trigger
    jobs:
      - build-proto
      - build-helm
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - publish-docs-new:
          requires:
            - build-docs
          context: aws
          filters: *any-upstream

  test-cli:
    <<: *do-not-run-on-manual-trigger
    jobs:
      - test-cli:
          matrix:
            parameters:
              executor-name:
                - "python-38"
                - "python-39"
                - "win/default"
  test-unit:
    <<: *do-not-run-on-manual-trigger
    jobs:
      - test-unit-react
      - test-unit-harness-cpu
      - test-unit-harness-tf2
      - test-unit-harness-pytorch2-cpu
      - test-unit-harness-pytorch2-gpu

      - test-unit-harness-gpu-tf:
          filters: *any-upstream
      - test-unit-harness-gpu-deepspeed:
          filters: *any-upstream
      - test-unit-harness-gpu-parallel:
          filters: *any-upstream

      # allows forks to request run approval instead of failing
      - request-gpu-unit-tests:
          type: approval
          filters: *any-fork
      - test-unit-harness-gpu-tf:
          name: f-test-unit-harness-gpu-tf
          filters: *any-fork
      - test-unit-harness-gpu-deepspeed:
          name: f-test-unit-harness-gpu-deepspeed
          filters: *any-fork
      - test-unit-harness-gpu-parallel:
          name: f-test-unit-harness-gpu-parallel
          filters: *any-fork

      - test-unit-storage:
          context: storage-unit-tests
          filters: *any-upstream
      - python-coverage:
          requires:
            - test-unit-harness-cpu
            - test-unit-harness-gpu-tf
            - test-unit-harness-gpu-parallel
            - test-unit-harness-tf2
            - test-unit-harness-pytorch2-cpu
            - test-unit-harness-pytorch2-gpu
            - test-unit-storage

  send-alerts:
    <<: *do-not-run-on-manual-trigger
    jobs:
      - send-job-level-alerts:
          filters:
            branches:
              only:
                - main

  test-go:
    <<: *do-not-run-on-manual-trigger
    jobs:
      - test-intg-master:
          context:
            - storage-unit-tests
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
      - test-intg-agent
      - go-coverage:
          requires:
            - test-intg-master
            - test-intg-agent
      - check-go-coverage-master:
          requires:
            - go-coverage
      - check-go-coverage-agent:
          requires:
            - go-coverage

  package-and-deploy-oss:
    <<: *do-not-run-on-manual-trigger
    jobs:
      # These jobs are important for docs and latest main and have been moved
      # here from test-e2e. These jobs are OSS focused while test-e2e now only
      # uses EE images (with the exception of test-det-deploy, which requires
      # both).
      - build-proto
      - build-helm
      - build-react:
          name: build-react-oss
          dev-mode: true
      - test-e2e-react:
          name: test-e2e-react-oss
          requires:
            - build-go-oss
          context:
            - playwright
            - github-read
            - dev-ci-cluster-default-user-credentials
          filters: *any-upstream
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - build-go:
          name: build-go-oss
          ee: false
          context: github-read

      - package-and-push-system-local:
          requires:
            - build-react-oss
            - build-docs
          filters:
            branches:
              only:
                - main

      - package-and-push-system-dev:
          requires:
            - build-react-oss
            - build-docs

      - upload-docs-search-index:
          requires:
            - build-docs
          context: determined-production
          filters:
            branches:
              only:
                - main

      - check-docs-links:
          requires:
            - build-docs
          filters:
            branches:
              only:
                - main

      - deploy:
          name: deploy-latest-main-cluster
          enable-cors: true
          reattach-enabled: true
          extra-tags: "long_running=determined_preview_cluster"
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
          filters:
            branches:
              only:
                - main
          requires:
            - package-and-push-system-dev
          max-dynamic-agents: 2

      - deploy:
          name: deploy-preview-cluster
          enable-cors: true
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
          filters:
            branches:
              only:
                - main
          requires:
            - package-and-push-system-dev
          max-dynamic-agents: 1
          cluster-id: determined-preview
          compute-agent-instance-type: t2.medium

  test-e2e:
    <<: *do-not-run-on-manual-trigger
    jobs:
      - build-proto
      - build-helm
      - build-react:
          dev-mode: true
      - test-e2e-react:
          name: test-e2e-react-ee
          ee: true
          requires:
            - build-go-ee
          context:
            - playwright
            - github-read
            - dev-ci-cluster-default-user-credentials
          filters: *any-upstream
      - build-docs:
          requires:
            - build-helm
            - build-proto
      - build-go:
          name: build-go-ee
          context: github-read

      - package-and-push-system-local:
          requires:
            - build-react
            - build-docs
          filters:
            branches:
              only:
                - main

      - package-and-push-system-local-ee:
          requires:
            - build-react
            - build-docs
            - package-and-push-system-local
          context: github-read
          filters:
            branches:
              only:
                - main

      - package-and-push-system-dev-ee:
          requires:
            - build-react
            - build-docs
          context:
            - determined-ee
            - github-read
          filters:
            branches:
              ignore:
                - /pull\/.*/

      - test-debian-packaging:
          requires:
            - package-and-push-system-local-ee
          context:
            - dev-ci-cluster-default-user-credentials
          filters:
            branches:
              only:
                - main

      - test-e2e-slurm:
          name: test-e2e-slurm-misconfigured
          requires:
            - package-and-push-system-local-ee
          context:
            - dev-ci-cluster-default-user-credentials
          filters:
            branches:
              only:
                - main
          mark: e2e_slurm_misconfigured
          master_config: |
            task_container_defaults:
              slurm:
                sbatch_args:
                  - --time=04:00:00
              environment_variables:
                # Some ports are not working, disable them so distributed jobs work.
                - NCCL_IB_HCA=mlx6_0:0
            checkpoint_storage:
              type: shared_fs
              host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
              storage_path: determined-checkpoint
              save_experiment_best: 0
              save_trial_best: 1
              save_trial_latest: 1
            db:
              user: postgres
              host: localhost
              port: 5432
              name: determined
              password: ${HPC_DB_PASSWD}
            resource_manager:
              type: slurm
              master_host: junkmaster
              master_port: 8080
              host: localhost
              port: 8181
              protocol: http
              slot_type: cuda
              user_name: launcher
              group_name: hpcd
              singularity_image_root: /lustre/hdd/foundation_engineering/images
              job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
              auth_file: /home/launcher/.launcher.$HOSTNAME.token
              path: /opt/singularity/bin:/usr/local/bin:${PATH}
              ld_library_path:
            security:
              initial_user_password: ${INITIAL_USER_PASSWORD}

      - test-e2e-slurm:
          name: test-e2e-slurm-gpu
          mark: "e2e_slurm_gpu"
          requires:
            - package-and-push-system-local-ee
          context:
            - dev-ci-cluster-default-user-credentials
          filters:
            branches:
              only:
                - main

      # Singularity over SLURM test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-slurm-singularity-gcp]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go-ee
          filters:
            branches:
              only:
                - main

      # Podman over SLURM test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-slurm-podman-gcp]
              container-run-type: ["podman"]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go-ee
          filters:
            branches:
              only:
                - main

      # Enroot over SLURM test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-slurm-enroot-gcp]
              container-run-type: ["enroot"]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go-ee
          filters:
            branches:
              only:
                - main

      # Singularity over PBS test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-pbs-singularity-gcp]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
          requires:
            - build-go-ee
          filters:
            branches:
              only:
                - main

      # Podman over PBS test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-pbs-podman-gcp]
              container-run-type: ["podman"]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
              extra-pytest-flags: ["-k 'not test_slurm_verify_home'"]
          requires:
            - build-go-ee
          filters:
            branches:
              only:
                - main

      # Enroot over PBS test on GCP
      - test-e2e-hpc-gcp:
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-pbs-enroot-gcp]
              container-run-type: ["enroot"]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
          requires:
            - build-go-ee
          filters:
            branches:
              only:
                - main

      - test-e2e:
          name: test-e2e-rbac
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          mark: e2e_cpu_rbac
          devcluster-config: single-rbac.devcluster.yaml

      - test-e2e:
          name: test-e2e-cpu
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 6
          resource-class: xlarge
          tf2: true
          mark: e2e_cpu

      - test-e2e:
          name: test-e2e-cpu-double
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 5
          resource-class: large
          tf2: true
          mark: e2e_cpu_2a
          target-stage: agent2
          devcluster-config: double-priority.devcluster.yaml

      - test-e2e:
          name: test-e2e-cpu-oauth
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          devcluster-config: oauth.devcluster.yaml
          mark: test_oauth
          target-stage: agent1

      - test-e2e:
          name: test-e2e-cpu-model-registry-rbac
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          devcluster-config: rbac-model-registry.yaml
          mark: test_model_registry_rbac
          target-stage: agent1

      - test-e2e:
          name: test-e2e-managed-devcluster
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 4
          resource-class: large
          tf2: true
          mark: managed_devcluster
          managed-devcluster: true
          # Managed devcluster restarts the master over the course of the tests,
          # so `compare_stats` cannot get the full logs from `det master logs`.
          extra-pytest-flags: "--no-compare-stats"
          collect-det-job-logs: false

      - test-e2e:
          name: test-e2e-managed-devcluster-resource-pools
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 4
          resource-class: large
          tf2: true
          mark: managed_devcluster_resource_pools
          managed-devcluster: true
          extra-pytest-flags: "--no-compare-stats"
          collect-det-job-logs: false

      - test-e2e:
          name: test-e2e-multi-k8s
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          tf2: true
          mark: e2e_multi_k8s
          target-stage: master
          devcluster-config: multi-k8s.devcluster.yaml
          run-minikubes: true
          multi-k8s: true

      - test-e2e:
          name: test-e2e-single-k8s
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          mark: e2e_single_k8s
          target-stage: master
          devcluster-config: single-k8s.devcluster.yaml
          run-minikubes: true

      - test-e2e:
          name: test-e2e-port-registry
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          devcluster-config: port-registry.devcluster.yaml
          mark: port_registry
          target-stage: agent

      - test-e2e:
          name: test-e2e-cpu-elastic
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          resource-class: large
          mark: e2e_cpu_elastic
          devcluster-config: elastic.devcluster.yaml
          target-stage: agent

      - test-e2e:
          name: test-e2e-postgres10-with-ssl
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: "10"

      - test-e2e:
          name: test-e2e-postgres14-with-ssl
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: postgres-with-ssl.devcluster.yaml
          target-stage: agent
          postgres-version: "14"

      - test-e2e:
          name: test-e2e-old-agent-versions
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          mark: e2e_cpu_postgres
          devcluster-config: custom-agent-version.devcluster.yaml
          target-stage: agent
          matrix:
            parameters:
              agent-version: ["0.17.10"]

      - test-e2e:
          name: test-e2e-agent-connection-loss
          requires:
            - build-go-ee
          parallelism: 1
          mark: e2e_cpu_agent_connection_loss
          devcluster-config: agent-no-connection.devcluster.yaml
          target-stage: agent
          wait-for-master: false
          extra-pytest-flags: "--no-compare-stats"
          collect-det-job-logs: false

      - test-e2e:
          name: test-e2e-saml
          context:
            - okta
            - dev-ci-cluster-default-user-credentials
          requires:
            - build-go-ee
          parallelism: 1
          devcluster-config: saml.devcluster.yaml
          mark: e2e_saml

      - test-perf:
          name: test-perf
          snapshot-after-migrations: true
          deploy-db: false
          requires:
            - build-go-ee
          context:
            - perf-tests
            - aws
          filters:
            branches:
              only:
                - main

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context:
            - aws
            - determined-ee
            - aws-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m6i.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context:
            - aws
            - determined-ee
            - aws-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              max-slots-per-pod: [4]

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              max-slots-per-pod: [1]

      - test-e2e-gke:
          name: test-e2e-gke-k8s-reattach
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters:
            branches:
              only: main
          requires:
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-k8s"]
              mark: ["e2e_k8s"]
              parallelism: [1]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              accel-node-taints: ["accel=truth:NoSchedule"]
              max-slots-per-pod: [1]

      - test-det-deploy:
          name: test-det-deploy-local
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - package-and-push-system-local
            - package-and-push-system-local-ee
          matrix:
            parameters:
              parallelism: [2]
              mark: ["det_deploy_local"]
              det-version: [$CIRCLE_SHA1]

      - deploy-latest-gke:
          name: deploy-latest-gke-master
          context: gcp
          det-version: $CIRCLE_SHA1
          filters:
            branches:
              only:
                - main
          requires:
            - package-and-push-system-dev-ee

  test-e2e-longrunning:
    <<: *do-not-run-on-manual-trigger
    jobs:
      # Build and publish artifacts used in tests
      - build-helm:
          filters: *upstream-feature-branch
      - build-proto:
          filters: *upstream-feature-branch
      - build-react:
          dev-mode: true
          filters: *upstream-feature-branch
      - build-docs:
          filters: *upstream-feature-branch
          requires:
            - build-helm
            - build-proto
      - build-go:
          filters: *upstream-feature-branch
          context: github-read
      - package-and-push-system-dev-ee:
          requires:
            - build-react
            - build-docs
          context:
            - determined-ee
            - github-read
          filters: *upstream-feature-branch

      - package-and-push-system-local:
          context: github-read
          filters: *upstream-feature-branch
          requires:
            - build-react
            - build-docs

      - package-and-push-system-local-ee:
          context: github-read
          filters: *upstream-feature-branch
          requires:
            - build-react
            - build-docs
            - package-and-push-system-local

      # Distributed tests
      - request-e2e-cpu-distributed:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-e2e-cpu-distributed
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m6i.large"]
              max-dynamic-agents: [16]

      # DeepSpeed tests
      - request-deepspeed-tests:
          type: approval
          filters: *upstream-feature-branch

        # DeepSpeed tests do not work on K80s, so we need V100 instances.
      - test-e2e-aws:
          name: test-e2e-deepspeed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-deepspeed-tests
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              compute-agent-instance-type: ["g4dn.12xlarge"]
              aux-agent-instance-type: ["m6i.large"]
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              max-dynamic-agents: [2]

      # K8s GPU tests
      - request-k8-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-single-gpu
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-gpu"]
              mark: ["e2e_gpu"]
              parallelism: [1]

      - test-e2e-gke:
          name: test-e2e-gke-parallel
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]
              parallelism: [1]
              machine-type: ["n1-standard-32"]
              gpus-per-machine: [4]
              num-machines: [2]
              max-slots-per-pod: [4]

      # K8s CPU tests
      - request-k8-tests-cpu:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-gke:
          name: test-e2e-gke-single-cpu
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cpu
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-cpu"]
              mark: ["e2e_gpu and not gpu_required"]
              parallelism: [1]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              max-slots-per-pod: [1]

      - test-e2e-gke:
          name: test-e2e-gke-k8s-reattach
          context:
            - gcp-ci
            - determined-ee
            - gcp-ci-cluster-default-user-credentials
          filters: *upstream-feature-branch
          requires:
            - request-k8-tests-cpu
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["e2e-k8s"]
              mark: ["e2e_k8s"]
              parallelism: [1]
              machine-type: ["n1-standard-8"]
              gpus-per-machine: [0]
              environment-gpu-enabled: ["0"]
              slot-type: ["cpu"]
              slot-resource-requests-cpu: [7]
              accel-node-taints: ["accel=truth:NoSchedule"]
              max-slots-per-pod: [1]

      # Nightly distributed tests
      - request-gpu-distributed-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-distributed-nightly
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - hugging-face
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-distributed-nightly
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m6i.large"]
              max-dynamic-agents: [2]

      # Perf tests.
      - request-perf-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-perf:
          name: test-perf-feature-branch
          snapshot-after-migrations: false
          deploy-db: true
          requires:
            - build-go
            - request-perf-tests
          context:
            - perf-tests
            - aws
          filters: *upstream-feature-branch

      # Nightly tests
      - request-gpu-nightly:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-nightly
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]

      # GPU tests
      - request-gpu-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-aws:
          name: test-e2e-gpu-parallel
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              parallelism: [2]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m6i.large"]
              cluster-id-prefix: ["parallel"]
              mark: ["parallel"]

      - test-e2e-aws:
          name: test-e2e-gpu-single
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-gpu-tests
            - package-and-push-system-dev-ee
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["e2e-gpu"]
              enable-tls: [true]
              mark: ["e2e_gpu"]

      # packaging tests
      - request-packaging-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-debian-packaging:
          filters: *upstream-feature-branch
          requires:
            - package-and-push-system-local-ee
            - request-packaging-tests
          context:
            - dev-ci-cluster-default-user-credentials

      # Local deployment
      - test-det-deploy:
          name: test-det-deploy-local
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - package-and-push-system-local
            - package-and-push-system-local-ee
            - request-packaging-tests
          matrix:
            parameters:
              parallelism: [2]
              mark: ["det_deploy_local"]
              det-version: [$CIRCLE_SHA1]

      # Request AWS FS tests
      - request-aws-fs-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-aws-fs:
          name: test-aws-fs-fsx
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-aws-fs-tests
            - package-and-push-system-dev-ee
          cluster-id-prefix: aws-fs-fsx
          deployment-type: fsx

      - test-aws-fs:
          name: test-aws-fs-efs
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          filters: *upstream-feature-branch
          requires:
            - request-aws-fs-tests
            - package-and-push-system-dev-ee
          cluster-id-prefix: aws-fs-efs
          deployment-type: efs

      # Request HPC tests
      - request-hpc-tests:
          type: approval
          filters: *upstream-feature-branch

      - test-e2e-slurm:
          name: test-e2e-slurm-misconfigured
          context:
            - dev-ci-cluster-default-user-credentials
          filters: *upstream-feature-branch
          requires:
            - package-and-push-system-local-ee
            - request-hpc-tests
          mark: e2e_slurm_misconfigured
          master_config: |
            task_container_defaults:
              slurm:
                sbatch_args:
                  - --time=04:00:00
              environment_variables:
                # Some ports are not working, disable them so distributed jobs work.
                - NCCL_IB_HCA=mlx6_0:0
            checkpoint_storage:
              type: shared_fs
              host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
              storage_path: determined-checkpoint
              save_experiment_best: 0
              save_trial_best: 1
              save_trial_latest: 1
            db:
              user: postgres
              host: localhost
              port: 5432
              name: determined
              password: ${HPC_DB_PASSWD}
            resource_manager:
              type: slurm
              master_host: junkmaster
              master_port: 8080
              host: localhost
              port: 8181
              protocol: http
              slot_type: cuda
              user_name: launcher
              group_name: hpcd
              singularity_image_root: /lustre/hdd/foundation_engineering/images
              job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
              auth_file: /home/launcher/.launcher.$HOSTNAME.token
              path: /opt/singularity/bin:/usr/local/bin:${PATH}
              ld_library_path:
            security:
              initial_user_password: ${INITIAL_USER_PASSWORD}

      - test-e2e-slurm:
          name: test-e2e-slurm-restart
          context:
            - dev-ci-cluster-default-user-credentials
          mark: "e2e_slurm_restart"
          filters: *upstream-feature-branch
          requires:
            - package-and-push-system-local-ee
            - request-hpc-tests
          extra-pytest-flags: "--no-compare-stats"
          collect-det-job-logs: false

      - test-e2e-slurm:
          name: test-e2e-slurm-gpu
          context:
            - dev-ci-cluster-default-user-credentials
          filters: *upstream-feature-branch
          mark: "e2e_slurm_gpu"
          requires:
            - package-and-push-system-local-ee
            - request-hpc-tests

      - test-e2e-slurm:
          name: test-e2e-slurm-enroot-znode
          context:
            - dev-ci-cluster-default-user-credentials
          matrix:
            parameters:
              mark: ["e2e_slurm and not deepspeed"]
          requires:
            - package-and-push-system-local-ee
            - request-hpc-tests
          master_config: |
            task_container_defaults:
              slurm:
                sbatch_args:
                  - --time=04:00:00
              # The current image must be created in the launcher account before running this test
              # cd /lustre/ssd/foundation_engineering/
              # enroot import docker://determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0.sqsh
              # enroot create /lustre/ssd/foundation_engineering/determinedai+environments+cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0.sqsh
              # image: determinedai+environments+cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0
              image:
                cpu: <<pipeline.parameters.default-pt-tf-cpu-hpc-image>>
                cuda: <<pipeline.parameters.default-pt-gpu-hpc-image>>
              environment_variables:
                  # Some ports are not working, disable them so distributed jobs work.
                  - NCCL_IB_HCA=mlx6_0:0
                  # Workaround XDG_RUNTIME_DIR not provided by Slurm
                  - ENROOT_RUNTIME_PATH=/tmp/launcher
            checkpoint_storage:
              type: shared_fs
              host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
              storage_path: determined-checkpoint
              save_experiment_best: 0
              save_trial_best: 1
              save_trial_latest: 1
            db:
              user: postgres
              host: localhost
              port: 5432
              name: determined
              password: ${HPC_DB_PASSWD}
            resource_manager:
              type: slurm
              master_host: $HOSTNAME
              master_port: 8080
              host: localhost
              port: 8181
              protocol: http
              slot_type: cuda
              user_name: launcher
              container_run_type: enroot
              group_name: hpcd
              singularity_image_root: /lustre/hdd/foundation_engineering/images
              job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
              auth_file: /home/launcher/.launcher.$HOSTNAME.token
              path: /opt/singularity/bin:/usr/local/bin:${PATH}
              ld_library_path:
            security:
              initial_user_password: ${INITIAL_USER_PASSWORD}

      # Singularity over SLURM test on GCP
      - test-e2e-hpc-gcp:
          filters: *upstream-feature-branch
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-slurm-singularity-gcp]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go
            - request-hpc-tests

      # Podman over SLURM test on GCP
      - test-e2e-hpc-gcp:
          filters: *upstream-feature-branch
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-slurm-podman-gcp]
              container-run-type: ["podman"]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go
            - request-hpc-tests

      # Enroot over SLURM test on GCP
      - test-e2e-hpc-gcp:
          filters: *upstream-feature-branch
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-slurm-enroot-gcp]
              container-run-type: ["enroot"]
              mark: ["e2e_slurm and not parallel and not gpu_required"]
          requires:
            - build-go
            - request-hpc-tests

      # Singularity over PBS test on GCP
      - test-e2e-hpc-gcp:
          filters: *upstream-feature-branch
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-pbs-singularity-gcp]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
          requires:
            - build-go
            - request-hpc-tests

      # Podman over PBS test on GCP
      - test-e2e-hpc-gcp:
          filters: *upstream-feature-branch
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-pbs-podman-gcp]
              container-run-type: ["podman"]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
              extra-pytest-flags: ["-k 'not test_slurm_verify_home'"]
          requires:
            - build-go
            - request-hpc-tests

      # Enroot over PBS test on GCP
      - test-e2e-hpc-gcp:
          filters: *upstream-feature-branch
          context:
            # Provides the GITHUB_USERNAME and GITHUB_TOKEN enviroment variable
            # that's required by the "gh" command for authentication.
            - github-read
            - gcp
            - gcp-ci-cluster-default-user-credentials
          matrix:
            parameters:
              name: [test-e2e-pbs-enroot-gcp]
              container-run-type: ["enroot"]
              workload-manager: ["pbs"]
              mark: ["e2e_pbs and not parallel and not gpu_required"]
          requires:
            - build-go
            - request-hpc-tests

  nightly:
    when: << pipeline.parameters.do_nightly_tests >>
    jobs:
      - test-e2e-aws:
          name: test-e2e-gpu-nightly
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          matrix:
            parameters:
              parallelism: [2]
              cluster-id-prefix: ["nightly"]
              mark: ["nightly"]
      - test-e2e-aws:
          name: test-e2e-gpu-distributed
          context:
            - aws
            - determined-ee
            - hugging-face
            - aws-ci-cluster-default-user-credentials
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed"]
              compute-agent-instance-type: ["g4dn.metal"]
              aux-agent-instance-type: ["m6i.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-gpu-deepspeed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          matrix:
            parameters:
              cluster-id-prefix: ["deepspeed"]
              mark: ["deepspeed"]
              compute-agent-instance-type: ["g4dn.12xlarge"]
              aux-agent-instance-type: ["m6i.large"]
              max-dynamic-agents: [2]
      - test-e2e-aws:
          name: test-e2e-cpu-distributed
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          matrix:
            parameters:
              cluster-id-prefix: ["distributed"]
              mark: ["distributed and not gpu_required"]
              compute-agent-instance-type: ["c5.2xlarge"]
              environment-gpu-enabled: ["0"]
              aux-agent-instance-type: ["m6i.large"]
              max-dynamic-agents: [16]
      - test-aws-fs:
          name: test-aws-fs-fsx
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          cluster-id-prefix: aws-fs-fsx
          deployment-type: fsx

      - test-aws-fs:
          name: test-aws-fs-efs
          context:
            - aws
            - aws-ci-cluster-default-user-credentials
            - determined-ee
          cluster-id-prefix: aws-fs-efs
          deployment-type: efs

      - run-shared-cluster-cleanup:
          name: gke-cleanup
          context: gcp-shared-cluster

      - build-proto
      - build-helm
      - build-docs:
          requires:
            - build-proto
            - build-helm
      - build-react:
          name: build-react-ee
          ee: true
      - package-and-push-system-local-ee:
          requires:
            - build-docs
            - build-react-ee
          context: github-read
      - test-e2e-slurm:
          name: test-e2e-slurm-restart
          context:
            - dev-ci-cluster-default-user-credentials
          mark: "e2e_slurm_restart"
          requires:
            - package-and-push-system-local-ee
          extra-pytest-flags: "--no-compare-stats"
          collect-det-job-logs: false
      - test-e2e-slurm:
          name: test-e2e-slurm-preemption
          context:
            - dev-ci-cluster-default-user-credentials
          mark: "e2e_slurm_preemption"
          requires:
            - package-and-push-system-local-ee
          extra-pytest-flags: "--no-compare-stats"
      - test-e2e-slurm:
          name: test-e2e-slurm-znode
          context:
            - dev-ci-cluster-default-user-credentials
          requires:
            - package-and-push-system-local-ee
          extra-pytest-flags: "--no-compare-stats"
      - test-e2e-slurm:
          name: test-e2e-slurm-enroot-znode
          context:
            - dev-ci-cluster-default-user-credentials
          matrix:
            parameters:
              mark: ["e2e_slurm and not deepspeed"]
          requires:
            - package-and-push-system-local-ee
          master_config: |
            task_container_defaults:
              slurm:
                sbatch_args:
                  - --time=04:00:00
              # The current image must be created in the launcher account before running this test
              # cd /lustre/ssd/foundation_engineering/
              # enroot import docker://determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0.sqsh
              # enroot create /lustre/ssd/foundation_engineering/determinedai+environments+cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0.sqsh
              # image: determinedai+environments+cuda-11.3-pytorch-1.10-tf-2.8-gpu-24586f0
              image:
                cpu: <<pipeline.parameters.default-pt-tf-cpu-hpc-image>>
                cuda: <<pipeline.parameters.default-pt-gpu-hpc-image>>
              environment_variables:
                  # Some ports are not working, disable them so distributed jobs work.
                  - NCCL_IB_HCA=mlx6_0:0
                  # Workaround XDG_RUNTIME_DIR not provided by Slurm
                  - ENROOT_RUNTIME_PATH=/tmp/launcher
            checkpoint_storage:
              type: shared_fs
              host_path: /scratch/launcher/.launcher.$HOSTNAME/checkpoints
              storage_path: determined-checkpoint
              save_experiment_best: 0
              save_trial_best: 1
              save_trial_latest: 1
            db:
              user: postgres
              host: localhost
              port: 5432
              name: determined
              password: ${HPC_DB_PASSWD}
            resource_manager:
              type: slurm
              master_host: $HOSTNAME
              master_port: 8080
              host: localhost
              port: 8181
              protocol: http
              slot_type: cuda
              user_name: launcher
              container_run_type: enroot
              group_name: hpcd
              singularity_image_root: /lustre/hdd/foundation_engineering/images
              job_storage_root: /scratch/launcher/.launcher.$HOSTNAME
              auth_file: /home/launcher/.launcher.$HOSTNAME.token
              path: /opt/singularity/bin:/usr/local/bin:${PATH}
              ld_library_path:
            security:
              initial_user_password: ${INITIAL_USER_PASSWORD}
      - terminate-vpc-circleci:
            context: ["gcp"]

  manual-e2e-react:
    when: << pipeline.parameters.e2e-react >>
    jobs:
      - build-go:
          name: build-go
          ee: << pipeline.parameters.ee >>
          context: github-read
      - test-e2e-react:
          ee: << pipeline.parameters.ee >>
          playwright-options: << pipeline.parameters.e2e-react >>
          requires:
            - build-go
          context:
            - playwright
            - github-read
            - dev-ci-cluster-default-user-credentials

  release:
    jobs:
      - build-helm:
          filters: *release-and-rc-filters
      - build-proto:
          filters: *release-and-rc-filters
      - build-react:
          context: determined-production
          filters: *release-and-rc-filters
      - build-docs:
          context: determined-production
          filters: *release-and-rc-filters
          requires:
            - build-helm
            - build-proto

      - upload-docs-search-index:
          requires:
            - build-docs
          context: determined-production
          filters: *release-and-rc-filters

      - package-and-push-system-rc:
          requires:
            - build-react
            - build-docs
          context: determined-production
          filters: *rc-filters

      - publish-python-package:
          name: publish-python-package-rc
          matrix:
            parameters:
              path:
                - "harness"
          context: determined-production
          filters: *rc-filters
          requires:
            - package-and-push-system-rc

      - package-and-push-system-release:
          requires:
            - build-react
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-python-package:
          name: publish-python-package-release
          matrix:
            parameters:
              path:
                - "harness"
          context: determined-production
          filters: *release-filters
          requires:
            - package-and-push-system-release

      - publish-docs:
          name: publish-docs-rc
          matrix:
            parameters:
              dry-run: [true]
          requires:
            - build-docs
          context: determined-production
          filters: *rc-filters

      - publish-docs:
          requires:
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-helm-gh:
          requires:
            - build-helm
          context: determined-production
          filters: *release-filters

      - upload-try-now-template:
          context: determined-production
          filters: *release-filters

  release-ee:
    jobs:
      - build-helm:
          filters: *release-and-rc-filters
      - build-proto:
          filters: *release-and-rc-filters
      - build-react:
          context: determined-production
          matrix:
            parameters:
              ee: [true]
          filters: *release-and-rc-filters
      - build-docs:
          matrix:
            parameters:
              ee: [true]
          context: determined-production
          filters: *release-and-rc-filters
          requires:
            - build-helm
            - build-proto
      - upload-docs-search-index:
          requires:
            - build-docs
          context: determined-production
          filters: *release-and-rc-filters

      - package-and-push-system-rc-ee:
          requires:
            - build-react
            - build-docs
          context:
            - determined-ee
            - github-read
          filters: *rc-filters

      - publish-python-package:
          name: publish-python-package-rc-ee
          matrix:
            parameters:
              path: ["harness"]
              ee: [true]
          context:
            - determined-production
            - determined-ee # to override TWINE envs for fury.io
          filters: *rc-filters
          requires:
            - package-and-push-system-rc-ee

      - package-and-push-system-release-ee:
          requires:
            - build-react
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-python-package:
          name: publish-python-package-release-ee
          matrix:
            parameters:
              path: ["harness"]
              ee: [true]
          context:
            - determined-production
            - determined-ee # to override TWINE envs for fury.io
          filters: *release-filters
          requires:
            - package-and-push-system-release-ee

      - publish-docs:
          matrix:
            parameters:
              ee: [true]
          requires:
            - build-docs
          context: determined-production
          filters: *release-filters

      - publish-helm-gh:
          matrix:
            parameters:
              ee: [true]
          requires:
            - build-helm
          context: determined-production
          filters: *release-filters
