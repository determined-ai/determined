name: ffn_example
workspace: DS_AT_PROTOTYPING
project: ffn_example
max_restarts: 0
resources:
  slots_per_trial: 2
  shm_size: 17179869184 # 16 GiB. Was hitting "Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)."
  max_slots: 8
  # resource_pool: young # Grenoble specific; remove later.
searcher:
  name: custom
  metric: throughput
  smaller_is_better: False
hyperparameters:
  dim: 1024
  layers: 4 # Total params will be layers * 1024 ** 2
  # NOTE: dsat code expects usual DS config dict to appear as in the below.
  ds_config:
    train_micro_batch_size_per_gpu: 128
    gradient_accumulation_steps: 1
    optimizer:
      TYPE: Adam
      params:
        lr: 1e-3
    fp16:
      enabled: True
    autotuning:
      enabled: True
      fast: False
      tuner_type: random
      tuner_num_trials: 50
      num_tuning_micro_batch_sizes: 5
      tuner_early_stopping: 5
      metric: throughput
    # Using torch_distributed's launcher for simplicity. TODO: Use DS launcher instead (?).
entrypoint: python3 -m determined.launch.torch_distributed python3 script.py
