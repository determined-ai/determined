name: ffn_example_no_autotune
workspace: DS_AT_PROTOTYPING
project: ffn_example
max_restarts: 0
environment:
  image:
    gpu: determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-deepspeed-0.7.0-gpu-0.20.1
    cpu: determinedai/environments:cuda-11.3-pytorch-1.10-tf-2.8-deepspeed-0.7.0-gpu-0.20.1
resources:
  slots_per_trial: 2
  shm_size: 17179869184 # 16 GiB. Was hitting "Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)."
  max_slots: 8
  # resource_pool: young # Grenoble specific; remove later.
searcher:
  name: single
  max_length: 1e3
  metric: placeholder
hyperparameters:
  dim: 16
  layers: 2 # Total params will be layers * dim ** 2
  # NOTE: dsat code expects usual DS config dict to appear as in the below.
  ds_config:
    train_micro_batch_size_per_gpu: 128
    gradient_accumulation_steps: 1
    optimizer:
      TYPE: Adam
      params:
        lr: 1e-3
    fp16:
      enabled: True
    # Using torch_distributed's launcher for simplicity. TODO: Use DS launcher instead (?).
entrypoint: python3 -m determined.launch.torch_distributed python3 script.py
